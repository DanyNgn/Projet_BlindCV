{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - Test modèle - Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in c:\\python39\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.3 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.3.2 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\python39\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python39\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python39\\lib\\site-packages (from matplotlib) (1.23.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python39\\lib\\site-packages (from matplotlib) (4.34.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python39\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python39\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip \n",
    "pip.main([\"install\",\"matplotlib\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in c:\\python39\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.3.2 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.3 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.23.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des librairies nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # to avoid deprecation warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la dataset\n",
    "dataset = pd.read_csv(\"dataset_CV_labelise.csv\", delimiter = \";\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4049, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 4049\n",
      "\n",
      "Display of dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_Sentences</th>\n",
       "      <th>Sentences_CV_clean</th>\n",
       "      <th>CV_Number</th>\n",
       "      <th>Sentence_line</th>\n",
       "      <th>Nb_tokens</th>\n",
       "      <th>%texte_lu</th>\n",
       "      <th>%texte_lu_fin_ligne</th>\n",
       "      <th>Is_alpha</th>\n",
       "      <th>Grammar</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE</td>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>97.20</td>\n",
       "      <td>[True, True, True, False, True, True]</td>\n",
       "      <td>['NOUN', 'PROPN', 'VERB', 'NUM', 'PROPN', 'PRO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PROFIL</td>\n",
       "      <td>PROFIL</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.27</td>\n",
       "      <td>96.73</td>\n",
       "      <td>[True]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PERSONNEL Je suis Ã©tudiante au lycÃ©e Condorcet.</td>\n",
       "      <td>PERSONNEL Je suis Ã©tudiante au lycÃ©e Condorcet.</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.01</td>\n",
       "      <td>92.99</td>\n",
       "      <td>[True, True, True, True, True, True, True, False]</td>\n",
       "      <td>['NOUN', 'PRON', 'AUX', 'ADJ', 'ADP', 'NOUN', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je code depuis l'Ã¢ge de 13 ans et j'aime crÃ©...</td>\n",
       "      <td>Je code depuis l'Ã¢ge de 13 ans et j'aime crÃ©...</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.22</td>\n",
       "      <td>81.78</td>\n",
       "      <td>[True, True, True, False, True, True, False, T...</td>\n",
       "      <td>['PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RÃALISATIONS</td>\n",
       "      <td>RÃALISATIONS</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.69</td>\n",
       "      <td>81.31</td>\n",
       "      <td>[True]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        CV_Sentences  \\\n",
       "0         SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE   \n",
       "1                                             PROFIL   \n",
       "2  PERSONNEL Je suis Ã©tudiante au lycÃ©e Condorcet.   \n",
       "3  Je code depuis l'Ã¢ge de 13 ans et j'aime crÃ©...   \n",
       "4                                      RÃALISATIONS   \n",
       "\n",
       "                                  Sentences_CV_clean CV_Number  Sentence_line  \\\n",
       "0         SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE      CV_1            0.0   \n",
       "1                                             PROFIL      CV_1            1.0   \n",
       "2  PERSONNEL Je suis Ã©tudiante au lycÃ©e Condorcet.      CV_1            2.0   \n",
       "3  Je code depuis l'Ã¢ge de 13 ans et j'aime crÃ©...      CV_1            3.0   \n",
       "4                                      RÃALISATIONS      CV_1            4.0   \n",
       "\n",
       "   Nb_tokens  %texte_lu  %texte_lu_fin_ligne  \\\n",
       "0        6.0       2.80                97.20   \n",
       "1        1.0       3.27                96.73   \n",
       "2        8.0       7.01                92.99   \n",
       "3       24.0      18.22                81.78   \n",
       "4        1.0      18.69                81.31   \n",
       "\n",
       "                                            Is_alpha  \\\n",
       "0              [True, True, True, False, True, True]   \n",
       "1                                             [True]   \n",
       "2  [True, True, True, True, True, True, True, False]   \n",
       "3  [True, True, True, False, True, True, False, T...   \n",
       "4                                             [True]   \n",
       "\n",
       "                                             Grammar  Label  \n",
       "0  ['NOUN', 'PROPN', 'VERB', 'NUM', 'PROPN', 'PRO...      1  \n",
       "1                                           ['NOUN']      0  \n",
       "2  ['NOUN', 'PRON', 'AUX', 'ADJ', 'ADP', 'NOUN', ...      0  \n",
       "3  ['PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', ...      0  \n",
       "4                                           ['NOUN']      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basics statistics: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_Sentences</th>\n",
       "      <th>Sentences_CV_clean</th>\n",
       "      <th>CV_Number</th>\n",
       "      <th>Sentence_line</th>\n",
       "      <th>Nb_tokens</th>\n",
       "      <th>%texte_lu</th>\n",
       "      <th>%texte_lu_fin_ligne</th>\n",
       "      <th>Is_alpha</th>\n",
       "      <th>Grammar</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4049</td>\n",
       "      <td>4045</td>\n",
       "      <td>4047</td>\n",
       "      <td>4047.000000</td>\n",
       "      <td>4047.000000</td>\n",
       "      <td>4047.000000</td>\n",
       "      <td>4047.000000</td>\n",
       "      <td>4047</td>\n",
       "      <td>4047</td>\n",
       "      <td>4049.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2829</td>\n",
       "      <td>2825</td>\n",
       "      <td>145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1251</td>\n",
       "      <td>2058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>#NOM?</td>\n",
       "      <td>#NOM?</td>\n",
       "      <td>CV_10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>463</td>\n",
       "      <td>249</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.822090</td>\n",
       "      <td>9.097850</td>\n",
       "      <td>50.400032</td>\n",
       "      <td>49.599970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.040257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.372538</td>\n",
       "      <td>10.625067</td>\n",
       "      <td>30.183407</td>\n",
       "      <td>30.183408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.196585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>24.310000</td>\n",
       "      <td>23.635000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>49.760000</td>\n",
       "      <td>50.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>76.365000</td>\n",
       "      <td>75.690000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.760000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CV_Sentences Sentences_CV_clean CV_Number  Sentence_line    Nb_tokens  \\\n",
       "count          4049               4045      4047    4047.000000  4047.000000   \n",
       "unique         2829               2825       145            NaN          NaN   \n",
       "top           #NOM?              #NOM?     CV_10            NaN          NaN   \n",
       "freq            131                131        66            NaN          NaN   \n",
       "mean            NaN                NaN       NaN      15.822090     9.097850   \n",
       "std             NaN                NaN       NaN      11.372538    10.625067   \n",
       "min             NaN                NaN       NaN       0.000000     1.000000   \n",
       "25%             NaN                NaN       NaN       7.000000     2.000000   \n",
       "50%             NaN                NaN       NaN      14.000000     5.000000   \n",
       "75%             NaN                NaN       NaN      23.000000    12.000000   \n",
       "max             NaN                NaN       NaN      65.000000   109.000000   \n",
       "\n",
       "          %texte_lu  %texte_lu_fin_ligne Is_alpha   Grammar        Label  \n",
       "count   4047.000000          4047.000000     4047      4047  4049.000000  \n",
       "unique          NaN                  NaN     1251      2058          NaN  \n",
       "top             NaN                  NaN  [False]  ['NOUN']          NaN  \n",
       "freq            NaN                  NaN      463       249          NaN  \n",
       "mean      50.400032            49.599970      NaN       NaN     0.040257  \n",
       "std       30.183407            30.183408      NaN       NaN     0.196585  \n",
       "min        0.240000             0.000000      NaN       NaN     0.000000  \n",
       "25%       24.310000            23.635000      NaN       NaN     0.000000  \n",
       "50%       49.760000            50.240000      NaN       NaN     0.000000  \n",
       "75%       76.365000            75.690000      NaN       NaN     0.000000  \n",
       "max      100.000000            99.760000      NaN       NaN     1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CV_Sentences           0.000000\n",
       "Sentences_CV_clean     0.098790\n",
       "CV_Number              0.049395\n",
       "Sentence_line          0.049395\n",
       "Nb_tokens              0.049395\n",
       "%texte_lu              0.049395\n",
       "%texte_lu_fin_ligne    0.049395\n",
       "Is_alpha               0.049395\n",
       "Grammar                0.049395\n",
       "Label                  0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic stats\n",
    "print(\"Number of rows : {}\".format(dataset.shape[0]))\n",
    "print()\n",
    "\n",
    "print(\"Display of dataset: \")\n",
    "display(dataset.head())\n",
    "print()\n",
    "\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = dataset.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "print(\"Percentage of missing values: \")\n",
    "display(100*dataset.isnull().sum()/dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3918, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Label\"].unique()\n",
    "dataset = dataset.loc[(dataset['CV_Sentences'] != \"#NOM?\"),:]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3914, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.loc[(dataset['CV_Sentences'] != \":\"),:]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3910, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna(axis =0, how = 'any')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV_Sentences           0.0\n",
       "Sentences_CV_clean     0.0\n",
       "CV_Number              0.0\n",
       "Sentence_line          0.0\n",
       "Nb_tokens              0.0\n",
       "%texte_lu              0.0\n",
       "%texte_lu_fin_ligne    0.0\n",
       "Is_alpha               0.0\n",
       "Grammar                0.0\n",
       "Label                  0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(100*dataset.isnull().sum()/dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating labels from features...\n",
      "...Done.\n",
      "\n",
      "Y : \n",
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "X :\n",
      "   Sentence_line  Nb_tokens  %texte_lu  %texte_lu_fin_ligne\n",
      "0            0.0        6.0       2.80                97.20\n",
      "1            1.0        1.0       3.27                96.73\n",
      "2            2.0        8.0       7.01                92.99\n",
      "3            3.0       24.0      18.22                81.78\n",
      "4            4.0        1.0      18.69                81.31\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable Y from features X\n",
    "print(\"Separating labels from features...\")\n",
    "features_list = [\"CV_Sentences\",\"Sentences_CV_clean\",\"CV_Number\", \"Is_alpha\", \"Grammar\", \"Label\"]\n",
    "target_variable = \"Label\"\n",
    "\n",
    "X = dataset.drop(features_list, axis = 1)\n",
    "Y = dataset.loc[:,target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print('Y : ')\n",
    "print(Y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numeric features  ['Sentence_line', 'Nb_tokens', '%texte_lu', '%texte_lu_fin_ligne']  at positions  [0, 1, 2, 3]\n",
      "Found categorical features  []  at positions  []\n"
     ]
    }
   ],
   "source": [
    "# Automatically detect positions of numeric/categorical features\n",
    "idx = 0\n",
    "numeric_features = []\n",
    "numeric_indices = []\n",
    "categorical_features = []\n",
    "categorical_indices = []\n",
    "for i,t in X.dtypes.iteritems():\n",
    "    if ('float' in str(t)) or ('int' in str(t)) :\n",
    "        numeric_features.append(i)\n",
    "        numeric_indices.append(idx)\n",
    "    else :\n",
    "        categorical_features.append(i)\n",
    "        categorical_indices.append(idx)\n",
    "\n",
    "    idx = idx + 1\n",
    "\n",
    "print('Found numeric features ', numeric_features,' at positions ', numeric_indices)\n",
    "print('Found categorical features ', categorical_features,' at positions ', categorical_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing into train and test sets...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "print(\"Dividing into train and test sets...\")\n",
    "# WARNING : don't forget stratify=Y for classification problems\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify = Y)\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert pandas DataFrames to numpy arrays...\n",
      "...Done\n",
      "[[ 7.    6.   67.72 32.28]\n",
      " [ 6.    1.   17.58 82.42]\n",
      " [12.   34.   95.47  4.53]\n",
      " [11.   28.   50.62 49.38]\n",
      " [ 1.    1.    0.49 99.51]]\n",
      "[[15.   13.   82.58 17.42]\n",
      " [ 1.   33.   13.55 86.45]]\n",
      "\n",
      "[0, 0, 0, 0, 1]\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrames to numpy arrays before using scikit-learn\n",
    "print(\"Convert pandas DataFrames to numpy arrays...\")\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "Y_train = Y_train.tolist()\n",
    "Y_test = Y_test.tolist()\n",
    "print(\"...Done\")\n",
    "\n",
    "print(X_train[0:5,:])\n",
    "print(X_test[0:2,:])\n",
    "print()\n",
    "print(Y_train[0:5])\n",
    "print(Y_test[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "[[ 7.    6.   67.72 32.28]\n",
      " [ 6.    1.   17.58 82.42]\n",
      " [12.   34.   95.47  4.53]\n",
      " [11.   28.   50.62 49.38]\n",
      " [ 1.    1.    0.49 99.51]]\n",
      "\n",
      "...Done!\n",
      "[[ 7.    6.   67.72 32.28]\n",
      " [ 6.    1.   17.58 82.42]\n",
      " [12.   34.   95.47  4.53]\n",
      " [11.   28.   50.62 49.38]\n",
      " [ 1.    1.    0.49 99.51]]\n",
      "\n",
      "Encoding categorical features and standardizing numerical features...\n",
      "\n",
      "[[ 7.    6.   67.72 32.28]\n",
      " [ 6.    1.   17.58 82.42]\n",
      " [12.   34.   95.47  4.53]\n",
      " [11.   28.   50.62 49.38]\n",
      " [ 1.    1.    0.49 99.51]]\n",
      "...Done\n",
      "[[-0.77628051 -0.30248416  0.58084389 -0.58084396]\n",
      " [-0.86533341 -0.76821091 -1.08793027  1.08793009]\n",
      " [-0.33101601  2.30558561  1.50442752 -1.50442752]\n",
      " [-0.42006891  1.74671352  0.01171669 -0.01171679]\n",
      " [-1.31059791 -0.76821091 -1.65672466  1.65672443]]\n",
      "Encoding labels...\n",
      "[0, 0, 0, 0, 1]\n",
      "...Done\n",
      "[0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "print(\"Imputing missing values...\")\n",
    "print(X_train[0:5,:])\n",
    "print()\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train[:,numeric_indices] = imputer.fit_transform(X_train[:,numeric_indices])\n",
    "print(\"...Done!\")\n",
    "print(X_train[0:5,:]) \n",
    "print() \n",
    "\n",
    "# Encoding categorical features and standardizing numerical features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print()\n",
    "print(X_train[0:5,:])\n",
    "\n",
    "# Normalization\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# OHE / dummyfication\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "featureencoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_indices),    \n",
    "        ('num', numeric_transformer, numeric_indices)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "X_train = featureencoder.fit_transform(X_train)\n",
    "print(\"...Done\")\n",
    "print(X_train[0:5,:])\n",
    "\n",
    "# Label encoding\n",
    "print(\"Encoding labels...\")\n",
    "print(Y_train[0:5])\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"...Done\")\n",
    "print(Y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "...Done.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Train model...\")\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "print(\"...Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on training set...\n",
      "...Done.\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on training set\n",
    "print(\"Predictions on training set...\")\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print(Y_train_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "[[15.   13.   82.58 17.42]\n",
      " [ 1.   33.   13.55 86.45]\n",
      " [ 2.    8.   19.3  80.7 ]\n",
      " [22.    1.   87.91 12.09]\n",
      " [ 9.    2.   37.16 62.84]]\n",
      "...Done!\n",
      "[[15.   13.   82.58 17.42]\n",
      " [ 1.   33.   13.55 86.45]\n",
      " [ 2.    8.   19.3  80.7 ]\n",
      " [22.    1.   87.91 12.09]\n",
      " [ 9.    2.   37.16 62.84]]\n",
      "\n",
      "Encoding categorical features and standardizing numerical features...\n",
      "[[15.   13.   82.58 17.42]\n",
      " [ 1.   33.   13.55 86.45]\n",
      " [ 2.    8.   19.3  80.7 ]\n",
      " [22.    1.   87.91 12.09]\n",
      " [ 9.    2.   37.16 62.84]]\n",
      "...Done\n",
      "[[-0.06385731  0.34953328  1.07541877 -1.0754188 ]\n",
      " [-1.31059791  2.21244026 -1.22205791  1.22205772]\n",
      " [-1.22154501 -0.11619346 -1.03068473  1.03068455]\n",
      " [ 0.55951299 -0.76821091  1.25281339 -1.25281341]\n",
      " [-0.59817471 -0.67506556 -0.43626298  0.43626284]]\n",
      "Encoding labels...\n",
      "[0, 0, 1, 0, 0]\n",
      "...Done\n",
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "print(\"Imputing missing values...\")\n",
    "print(X_test[0:5,:])\n",
    "X_test[:,numeric_indices] = imputer.transform(X_test[:,numeric_indices])\n",
    "print(\"...Done!\")\n",
    "print(X_test[0:5,:]) \n",
    "print() \n",
    "\n",
    "# Encoding categorical features and standardizing numerical features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print(X_test[0:5,:])\n",
    "X_test = featureencoder.transform(X_test)\n",
    "print(\"...Done\")\n",
    "print(X_test[0:5,:])\n",
    "\n",
    "# Label encoding\n",
    "print(\"Encoding labels...\")\n",
    "print(Y_test[0:5])\n",
    "Y_test = encoder.transform(Y_test)\n",
    "print(\"...Done\")\n",
    "print(Y_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set...\n",
      "...Done.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on test set\n",
    "print(\"Predictions on test set...\")\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print(Y_test_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set :  0.9584398976982097\n",
      "accuracy on test set :  0.9578005115089514\n",
      "\n",
      "f1-score on training set :  0.0\n",
      "f1-score on test set :  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print scores\n",
    "print(\"accuracy on training set : \", accuracy_score(Y_train, Y_train_pred))\n",
    "print(\"accuracy on test set : \", accuracy_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "print(\"f1-score on training set : \", f1_score(Y_train, Y_train_pred))                                            ## ATTENTION REVOIR F1-SCORE\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "train",
         "type": "scatter",
         "x": [
          0,
          0.007692307692307693,
          0.007692307692307693,
          0.015384615384615385,
          0.015384615384615385,
          0.03076923076923077,
          0.03076923076923077,
          0.038461538461538464,
          0.038461538461538464,
          0.038461538461538464,
          0.05384615384615385,
          0.06153846153846154,
          0.06153846153846154,
          0.06923076923076923,
          0.13076923076923078,
          0.13076923076923078,
          0.14615384615384616,
          0.16923076923076924,
          0.18461538461538463,
          0.2,
          0.23076923076923078,
          0.23846153846153847,
          0.23846153846153847,
          0.25384615384615383,
          0.25384615384615383,
          0.26153846153846155,
          0.2692307692307692,
          0.2692307692307692,
          0.2846153846153846,
          0.2846153846153846,
          0.3230769230769231,
          0.3230769230769231,
          0.36923076923076925,
          0.36923076923076925,
          0.4,
          0.4,
          0.4076923076923077,
          0.4230769230769231,
          0.4230769230769231,
          0.43846153846153846,
          0.4846153846153846,
          0.4846153846153846,
          0.5,
          0.5,
          0.5076923076923077,
          0.5230769230769231,
          0.5307692307692308,
          0.5307692307692308,
          0.5384615384615384,
          0.5384615384615384,
          0.5538461538461539,
          0.5538461538461539,
          0.5615384615384615,
          0.5615384615384615,
          0.5692307692307692,
          0.5692307692307692,
          0.5769230769230769,
          0.5769230769230769,
          0.5846153846153846,
          0.5846153846153846,
          0.5923076923076923,
          0.5923076923076923,
          0.5923076923076923,
          0.5923076923076923,
          0.6,
          0.6,
          0.6076923076923076,
          0.6076923076923076,
          0.6076923076923076,
          0.6076923076923076,
          0.6230769230769231,
          0.6230769230769231,
          0.6307692307692307,
          0.6307692307692307,
          0.6461538461538462,
          0.6461538461538462,
          0.6461538461538462,
          0.6538461538461539,
          0.6538461538461539,
          0.6615384615384615,
          0.6615384615384615,
          0.6615384615384615,
          0.6692307692307692,
          0.6692307692307692,
          0.676923076923077,
          0.676923076923077,
          0.6846153846153846,
          0.6846153846153846,
          0.6846153846153846,
          0.6846153846153846,
          0.6923076923076923,
          0.6923076923076923,
          0.7,
          0.7,
          0.7,
          0.7,
          0.7076923076923077,
          0.7076923076923077,
          0.7076923076923077,
          0.7076923076923077,
          0.7076923076923077,
          0.7076923076923077,
          0.7076923076923077,
          0.7076923076923077,
          0.7153846153846154,
          0.7153846153846154,
          0.7230769230769231,
          0.7230769230769231,
          0.7230769230769231,
          0.7230769230769231,
          0.7230769230769231,
          0.7230769230769231,
          0.7307692307692307,
          0.7307692307692307,
          0.7307692307692307,
          0.7307692307692307,
          0.7384615384615385,
          0.7384615384615385,
          0.7461538461538462,
          0.7461538461538462,
          0.7538461538461538,
          0.7538461538461538,
          0.7615384615384615,
          0.7615384615384615,
          0.7692307692307693,
          0.7692307692307693,
          0.7769230769230769,
          0.7769230769230769,
          0.7769230769230769,
          0.7769230769230769,
          0.7769230769230769,
          0.7769230769230769,
          0.7769230769230769,
          0.7846153846153846,
          0.7846153846153846,
          0.7923076923076923,
          0.7923076923076923,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8076923076923077,
          0.8076923076923077,
          0.8076923076923077,
          0.8076923076923077,
          0.8076923076923077,
          0.8153846153846154,
          0.8153846153846154,
          0.8153846153846154,
          0.8153846153846154,
          0.823076923076923,
          0.823076923076923,
          0.823076923076923,
          0.823076923076923,
          0.823076923076923,
          0.823076923076923,
          0.823076923076923,
          0.823076923076923,
          0.8307692307692308,
          0.8307692307692308,
          0.8384615384615385,
          0.8384615384615385,
          0.8384615384615385,
          0.8384615384615385,
          0.8461538461538461,
          0.8461538461538461,
          0.8538461538461538,
          0.8538461538461538,
          0.8538461538461538,
          0.8538461538461538,
          0.8538461538461538,
          0.8538461538461538,
          0.8615384615384616,
          0.8615384615384616,
          0.8615384615384616,
          0.8615384615384616,
          0.8615384615384616,
          0.8615384615384616,
          0.8692307692307693,
          0.8692307692307693,
          0.8769230769230769,
          0.8769230769230769,
          0.8769230769230769,
          0.8769230769230769,
          0.8769230769230769,
          0.8769230769230769,
          0.8846153846153846,
          0.8846153846153846,
          0.8846153846153846,
          0.8846153846153846,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.8923076923076924,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9076923076923077,
          0.9076923076923077,
          0.9076923076923077,
          0.9076923076923077,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9153846153846154,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9230769230769231,
          0.9384615384615385,
          0.9384615384615385,
          0.9384615384615385,
          0.9384615384615385,
          0.9384615384615385,
          0.9384615384615385,
          0.9461538461538461,
          0.9461538461538461,
          0.9461538461538461,
          0.9461538461538461,
          0.9461538461538461,
          0.9461538461538461,
          0.9461538461538461,
          0.9461538461538461,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9538461538461539,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9615384615384616,
          0.9692307692307692,
          0.9692307692307692,
          0.9692307692307692,
          0.9692307692307692,
          0.9692307692307692,
          0.9692307692307692,
          0.9692307692307692,
          0.9692307692307692,
          0.9692307692307692,
          0.9692307692307692,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9769230769230769,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9846153846153847,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          0.9923076923076923,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "y": [
          0,
          0,
          0.000333555703802535,
          0.001667778519012675,
          0.0023348899266177454,
          0.0023348899266177454,
          0.00266844563042028,
          0.00266844563042028,
          0.00333555703802535,
          0.004336224149432955,
          0.004336224149432955,
          0.005003335557038025,
          0.005670446964643095,
          0.00600400266844563,
          0.00600400266844563,
          0.0066711140760507,
          0.0066711140760507,
          0.0066711140760507,
          0.0066711140760507,
          0.0066711140760507,
          0.0066711140760507,
          0.0066711140760507,
          0.00733822548365577,
          0.00733822548365577,
          0.008005336891260841,
          0.008005336891260841,
          0.008338892595063376,
          0.009006004002668445,
          0.009006004002668445,
          0.009339559706470981,
          0.009339559706470981,
          0.009673115410273516,
          0.009673115410273516,
          0.01000667111407605,
          0.01000667111407605,
          0.010340226817878585,
          0.010340226817878585,
          0.010340226817878585,
          0.01067378252168112,
          0.01067378252168112,
          0.01067378252168112,
          0.01134089392928619,
          0.01134089392928619,
          0.011674449633088725,
          0.011674449633088725,
          0.011674449633088725,
          0.011674449633088725,
          0.0133422281521014,
          0.0133422281521014,
          0.014342895263509006,
          0.014342895263509006,
          0.01467645096731154,
          0.01467645096731154,
          0.015343562374916611,
          0.015343562374916611,
          0.016344229486324215,
          0.016344229486324215,
          0.017678452301534357,
          0.017678452301534357,
          0.019346230820547032,
          0.019346230820547032,
          0.0200133422281521,
          0.02068045363575717,
          0.021014009339559707,
          0.021014009339559707,
          0.02134756504336224,
          0.02134756504336224,
          0.022014676450967312,
          0.02268178785857238,
          0.024683122081387593,
          0.024683122081387593,
          0.02601734489659773,
          0.02601734489659773,
          0.0266844563042028,
          0.0266844563042028,
          0.027685123415610406,
          0.028352234823215475,
          0.028352234823215475,
          0.03135423615743829,
          0.03135423615743829,
          0.032021347565043365,
          0.03302201467645097,
          0.03302201467645097,
          0.034356237491661105,
          0.034356237491661105,
          0.03669112741827885,
          0.03669112741827885,
          0.037358238825883926,
          0.03835890593729153,
          0.0390260173448966,
          0.0390260173448966,
          0.039693128752501666,
          0.039693128752501666,
          0.0400266844563042,
          0.040693795863909275,
          0.041027351567711805,
          0.041027351567711805,
          0.04236157438292195,
          0.043028685790527016,
          0.04369579719813209,
          0.04436290860573716,
          0.04503002001334223,
          0.0456971314209473,
          0.046364242828552366,
          0.046364242828552366,
          0.0513675783855904,
          0.0513675783855904,
          0.052368245496998,
          0.05303535690460307,
          0.054036024016010674,
          0.05470313542361575,
          0.056037358238825885,
          0.056037358238825885,
          0.05870580386924616,
          0.059372915276851235,
          0.06037358238825884,
          0.06037358238825884,
          0.06170780520346898,
          0.06170780520346898,
          0.06270847231487658,
          0.06270847231487658,
          0.06470980653769179,
          0.06470980653769179,
          0.06537691794529686,
          0.06537691794529686,
          0.06737825216811208,
          0.06737825216811208,
          0.06804536357571714,
          0.07171447631754503,
          0.0723815877251501,
          0.08505670446964643,
          0.0857238158772515,
          0.08739159439626418,
          0.08739159439626418,
          0.08805870580386925,
          0.08805870580386925,
          0.09006004002668445,
          0.09006004002668445,
          0.09172781854569713,
          0.0923949299533022,
          0.09973315543695797,
          0.10040026684456305,
          0.10640426951300867,
          0.10707138092061375,
          0.10807204803202135,
          0.1094062708472315,
          0.11607738492328219,
          0.11674449633088725,
          0.1180787191460974,
          0.11874583055370247,
          0.13475650433622416,
          0.13542361574382922,
          0.13575717144763175,
          0.13642428285523683,
          0.13742494996664442,
          0.13742494996664442,
          0.1380920613742495,
          0.1417611741160774,
          0.14242828552368245,
          0.1467645096731154,
          0.1467645096731154,
          0.14743162108072047,
          0.14809873248832556,
          0.14876584389593062,
          0.14876584389593062,
          0.15510340226817879,
          0.15577051367578384,
          0.1561040693795864,
          0.15677118078719146,
          0.15777184789859908,
          0.15843895930620414,
          0.1627751834556371,
          0.1627751834556371,
          0.16544362908605736,
          0.16544362908605736,
          0.16611074049366245,
          0.1667778519012675,
          0.16911274182788524,
          0.16911274182788524,
          0.17178118745830553,
          0.17178118745830553,
          0.17244829886591062,
          0.17311541027351568,
          0.17444963308872583,
          0.17511674449633088,
          0.1754503002001334,
          0.1754503002001334,
          0.17878585723815876,
          0.17945296864576385,
          0.18112074716477652,
          0.18178785857238158,
          0.1847898599066044,
          0.1847898599066044,
          0.20513675783855903,
          0.20513675783855903,
          0.209472981987992,
          0.21014009339559706,
          0.21247498332221482,
          0.21314209472981988,
          0.2161440960640427,
          0.2161440960640427,
          0.22148098732488325,
          0.22214809873248834,
          0.22348232154769845,
          0.22348232154769845,
          0.23282188125416944,
          0.2334889926617745,
          0.2391594396264176,
          0.23982655103402267,
          0.24216144096064043,
          0.2428285523682455,
          0.24449633088725817,
          0.24516344229486325,
          0.2458305537024683,
          0.24649766511007337,
          0.2501667778519013,
          0.25083388925950634,
          0.2608405603735824,
          0.26150767178118745,
          0.2621747831887925,
          0.2628418945963976,
          0.2715143428952635,
          0.27218145430286855,
          0.2901934623082055,
          0.29086057371581053,
          0.29286190793862577,
          0.2935290193462308,
          0.29753168779186123,
          0.2988659106070714,
          0.3072048032021348,
          0.30787191460973984,
          0.3162108072048032,
          0.31687791861240827,
          0.32354903268845897,
          0.32421614409606403,
          0.3245496997998666,
          0.32521681120747165,
          0.3258839226150767,
          0.3258839226150767,
          0.3298865910607071,
          0.3305537024683122,
          0.3325550366911274,
          0.33322214809873246,
          0.33455637091394264,
          0.33455637091394264,
          0.3352234823215477,
          0.33555703802535025,
          0.3362241494329553,
          0.3362241494329553,
          0.35156771180787194,
          0.352234823215477,
          0.3525683789192795,
          0.3532354903268846,
          0.35957304869913276,
          0.3602401601067378,
          0.36924616410940625,
          0.3705803869246164,
          0.3709139426284189,
          0.37158105403602404,
          0.37825216811207474,
          0.37825216811207474,
          0.3789192795196798,
          0.37958639092728486,
          0.3815877251501001,
          0.38225483655770515,
          0.3839226150767178,
          0.3845897264843229,
          0.3912608405603736,
          0.39192795196797864,
          0.39392928619079387,
          0.39459639759839893,
          0.39826551034022684,
          0.39826551034022684,
          0.40226817878585724,
          0.4029352901934623,
          0.4069379586390927,
          0.4076050700466978,
          0.4086057371581054,
          0.4086057371581054,
          0.4152768512341561,
          0.4159439626417612,
          0.41694462975316876,
          0.4176117411607739,
          0.41961307538358905,
          0.4202801867911941,
          0.4226150767178119,
          0.4226150767178119,
          0.42361574382921946,
          0.4242828552368246,
          0.43929286190793865,
          0.4399599733155437,
          0.44629753168779185,
          0.4469646430953969,
          0.44996664442961976,
          0.4506337558372248,
          0.45263509006004005,
          0.4533022014676451,
          0.4573048699132755,
          0.4579719813208806,
          0.45830553702468313,
          0.45830553702468313,
          0.4589726484322882,
          0.4626417611741161,
          0.46330887258172115,
          0.46364242828552366,
          0.46430953969312877,
          0.47131420947298197,
          0.4719813208805871,
          0.47631754503002,
          0.4769846564376251,
          0.4786524349566378,
          0.47931954636424284,
          0.4809873248832555,
          0.4816544362908606,
          0.4829886591060707,
          0.4836557705136758,
          0.48732488325550366,
          0.4879919946631087,
          0.4903268845897265,
          0.49099399599733157,
          0.4919946631087392,
          0.49266177451634424,
          0.5016677785190127,
          0.5030020013342228,
          0.5083388925950634,
          0.5083388925950634,
          0.5093395597064709,
          0.510006671114076,
          0.5146764509673115,
          0.5153435623749166,
          0.5266844563042028,
          0.5280186791194129,
          0.5290193462308206,
          0.5296864576384256,
          0.5330220146764509,
          0.5330220146764509,
          0.533689126084056,
          0.5343562374916611,
          0.5356904603068713,
          0.5363575717144763,
          0.5440293529019347,
          0.5446964643095397,
          0.5483655770513676,
          0.5490326884589727,
          0.5527018012008005,
          0.5533689126084056,
          0.5563709139426284,
          0.5570380253502335,
          0.5577051367578385,
          0.5583722481654436,
          0.5643762508338893,
          0.5650433622414943,
          0.5713809206137425,
          0.5720480320213476,
          0.5750500333555704,
          0.5757171447631755,
          0.5763842561707805,
          0.5763842561707805,
          0.5830553702468312,
          0.5837224816544363,
          0.5847231487658439,
          0.585390260173449,
          0.5897264843228819,
          0.590393595730487,
          0.5907271514342896,
          0.5913942628418946,
          0.5960640426951301,
          0.5967311541027351,
          0.5983989326217478,
          0.5990660440293529,
          0.6024016010673783,
          0.6030687124749833,
          0.6054036024016011,
          0.6060707138092062,
          0.6087391594396264,
          0.6094062708472315,
          0.614076050700467,
          0.614076050700467,
          0.6194129419613076,
          0.6200800533689126,
          0.6224149432955304,
          0.6230820547031354,
          0.6237491661107405,
          0.6244162775183456,
          0.6304202801867912,
          0.6317545030020013,
          0.6324216144096064,
          0.6330887258172114,
          0.642761841227485,
          0.64342895263509,
          0.6444296197464977,
          0.6450967311541027,
          0.6577718478985991,
          0.6584389593062041,
          0.6597731821214142,
          0.6604402935290193,
          0.6614409606404269,
          0.662108072048032,
          0.6654436290860574,
          0.6661107404936625,
          0.6721147431621081,
          0.6727818545697132,
          0.6731154102735156,
          0.6737825216811207,
          0.6801200800533689,
          0.680787191460974,
          0.6821214142761841,
          0.6834556370913942,
          0.6941294196130754,
          0.6947965310206805,
          0.6954636424282855,
          0.6961307538358906,
          0.6964643095396932,
          0.6971314209472982,
          0.6981320880587059,
          0.6987991994663109,
          0.7004669779853235,
          0.7011340893929286,
          0.709472981987992,
          0.710140093395597,
          0.7131420947298199,
          0.7138092061374249,
          0.7158105403602402,
          0.7164776517678453,
          0.7211474316210807,
          0.7218145430286858,
          0.7308205470313542,
          0.7308205470313542,
          0.7404936624416277,
          0.7411607738492328,
          0.7414943295530354,
          0.7421614409606404,
          0.7451634422948632,
          0.7458305537024683,
          0.7498332221480988,
          0.7505003335557038,
          0.752501667778519,
          0.7531687791861241,
          0.7541694462975317,
          0.7555036691127418,
          0.7591727818545697,
          0.7598398932621748,
          0.7621747831887925,
          0.7628418945963976,
          0.7641761174116077,
          0.7648432288192129,
          0.766844563042028,
          0.7675116744496331,
          0.7695130086724483,
          0.7701801200800533,
          0.7708472314876584,
          0.7715143428952635,
          0.7758505670446965,
          0.7765176784523016,
          0.776851234156104,
          0.7775183455637091,
          0.7841894596397598,
          0.784856571047365,
          0.7911941294196131,
          0.7918612408272181,
          0.7935290193462308,
          0.7941961307538359,
          0.8028685790527018,
          0.8035356904603068,
          0.8088725817211474,
          0.8095396931287525,
          0.8122081387591727,
          0.8128752501667779,
          0.8162108072048032,
          0.8168779186124083,
          0.8182121414276184,
          0.8188792528352234,
          0.8218812541694464,
          0.8225483655770514,
          0.8345563709139426,
          0.8352234823215477,
          0.8368912608405604,
          0.8375583722481654,
          0.8395597064709807,
          0.8402268178785858,
          0.8635757171447632,
          0.8642428285523682,
          0.8645763842561708,
          0.8652434956637759,
          0.8655770513675783,
          0.8662441627751835,
          0.8712474983322215,
          0.8719146097398266,
          0.8785857238158773,
          0.8792528352234823,
          0.88025350233489,
          0.880920613742495,
          0.8835890593729153,
          0.8849232821881254,
          0.885256837891928,
          0.885923949299533,
          0.8925950633755837,
          0.8932621747831888,
          0.9012675116744496,
          0.9019346230820547,
          0.9036024016010674,
          0.9042695130086724,
          0.9052701801200801,
          0.9059372915276851,
          0.9172781854569713,
          0.9179452968645764,
          0.9266177451634423,
          0.9279519679786524,
          0.9426284189459639,
          0.9432955303535691,
          0.9482988659106071,
          0.9489659773182122,
          0.9523015343562375,
          0.9529686457638425,
          0.9543028685790527,
          0.9549699799866578,
          0.9603068712474984,
          0.9609739826551034,
          0.9693128752501667,
          0.9699799866577719,
          0.9716477651767845,
          0.9723148765843896,
          0.975650433622415,
          0.97631754503002,
          0.980653769179453,
          0.981320880587058,
          0.9829886591060707,
          0.9836557705136758,
          1
         ]
        },
        {
         "mode": "lines",
         "name": "test",
         "type": "scatter",
         "x": [
          0,
          0.030303030303030304,
          0.06060606060606061,
          0.06060606060606061,
          0.2727272727272727,
          0.2727272727272727,
          0.36363636363636365,
          0.36363636363636365,
          0.5757575757575758,
          0.5757575757575758,
          0.5757575757575758,
          0.6060606060606061,
          0.6060606060606061,
          0.6666666666666666,
          0.6666666666666666,
          0.6666666666666666,
          0.6666666666666666,
          0.696969696969697,
          0.696969696969697,
          0.7272727272727273,
          0.7272727272727273,
          0.7575757575757576,
          0.7575757575757576,
          0.7878787878787878,
          0.7878787878787878,
          0.8181818181818182,
          0.8181818181818182,
          0.8787878787878788,
          0.8787878787878788,
          0.9090909090909091,
          0.9090909090909091,
          0.9090909090909091,
          0.9090909090909091,
          0.9393939393939394,
          0.9393939393939394,
          0.9696969696969697,
          0.9696969696969697,
          0.9696969696969697,
          0.9696969696969697,
          0.9696969696969697,
          0.9696969696969697,
          0.9696969696969697,
          0.9696969696969697,
          0.9696969696969697,
          0.9696969696969697,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "y": [
          0,
          0,
          0,
          0.0013351134846461949,
          0.0013351134846461949,
          0.0026702269692923898,
          0.0026702269692923898,
          0.004005340453938585,
          0.004005340453938585,
          0.0053404539385847796,
          0.00801068090787717,
          0.00801068090787717,
          0.012016021361815754,
          0.012016021361815754,
          0.02403204272363151,
          0.0267022696929239,
          0.03337783711615487,
          0.03337783711615487,
          0.07476635514018691,
          0.07476635514018691,
          0.10146862483311081,
          0.10146862483311081,
          0.11481975967957277,
          0.11481975967957277,
          0.14686248331108145,
          0.14686248331108145,
          0.1588785046728972,
          0.1588785046728972,
          0.16555407209612816,
          0.16555407209612816,
          0.16955941255006676,
          0.17222963951935916,
          0.18958611481975968,
          0.18958611481975968,
          0.19492656875834447,
          0.19492656875834447,
          0.20293724966622162,
          0.205607476635514,
          0.20694259012016022,
          0.2096128170894526,
          0.21762349799732977,
          0.22029372496662217,
          0.2576769025367156,
          0.260347129506008,
          0.4779706275033378,
          0.4779706275033378,
          0.49666221628838453,
          0.4993324432576769,
          0.5580774365821095,
          0.5607476635514018,
          0.5901201602136181,
          0.5927903871829105,
          0.6154873164218959,
          0.6181575433911882,
          0.6381842456608812,
          0.6408544726301736,
          0.6675567423230975,
          0.6702269692923899,
          0.7596795727636849,
          0.7623497997329773,
          0.7783711615487316,
          0.7810413885180241,
          0.9586114819759679,
          0.9612817089452603,
          1
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "ROC curve",
         "x": 0.5
        },
        "xaxis": {
         "title": {
          "text": "False Positive Rate"
         }
        },
        "yaxis": {
         "title": {
          "text": "True Positive Rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize ROC curves\n",
    "probas_train = classifier.predict_proba(X_train)[:,1]\n",
    "precisions, recalls, thresholds = roc_curve(Y_train, probas_train)\n",
    "fig = go.Figure(\n",
    "    data = go.Scatter(\n",
    "        name = 'train',\n",
    "        x = recalls, \n",
    "        y = precisions, \n",
    "        mode = 'lines'\n",
    "    ),\n",
    "    layout = go.Layout(\n",
    "        title = go.layout.Title(text = \"ROC curve\", x = 0.5),\n",
    "        xaxis = go.layout.XAxis(title = 'False Positive Rate'),\n",
    "        yaxis = go.layout.YAxis(title = 'True Positive Rate')\n",
    "    )\n",
    ")\n",
    "\n",
    "probas_test = classifier.predict_proba(X_test)[:,1]\n",
    "precisions, recalls, thresholds = roc_curve(Y_test, probas_test)\n",
    "fig.add_trace(go.Scatter(\n",
    "    name = 'test',\n",
    "    x = recalls, \n",
    "    y = precisions, \n",
    "    mode = 'lines'\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de3c26e6ce99fdd1b8cf42170bfea1efe2e407789632cce096fb11ae10dab296"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
