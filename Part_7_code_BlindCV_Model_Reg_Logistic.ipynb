{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - Test modèle - Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U scikit-learn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\python39\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python39\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python39\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python39\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python39\\lib\\site-packages (from matplotlib) (1.23.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python39\\lib\\site-packages (from matplotlib) (4.34.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip \n",
    "pip.main([\"install\",\"matplotlib\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in c:\\python39\\lib\\site-packages (1.1.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.3 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.3.2 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des librairies nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # to avoid deprecation warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la dataset\n",
    "## Choix de 4 datasets\n",
    "\n",
    "#dataset = pd.read_csv(\"dataset_CV_labelise_features1_100.csv\", delimiter = \";\", encoding = \"utf-8\") ## 100CV + 4 features numériques\n",
    "\n",
    "#dataset = pd.read_csv(\"dataset_CV_labelise_features1_200.csv\", delimiter = \";\", encoding = \"utf-8\") ## 200CV + 4 features numériques\n",
    "\n",
    "#dataset = pd.read_csv(\"dataset_CV_labelise_features2_100.csv\", delimiter = \";\", encoding = \"utf-8\") ## 100CV + 12 features numériques\n",
    "\n",
    "dataset = pd.read_csv(\"dataset_CV_labelise_features2_200.csv\", delimiter = \";\", encoding = \"utf-8\") ## 200CV + 12 features numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5594, 19)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 5594\n",
      "\n",
      "Display of dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_Sentences</th>\n",
       "      <th>Sentences_CV_clean</th>\n",
       "      <th>CV_Number</th>\n",
       "      <th>Sentence_line</th>\n",
       "      <th>Nb_tokens</th>\n",
       "      <th>%texte_lu</th>\n",
       "      <th>%texte_lu_fin_ligne</th>\n",
       "      <th>Is_alpha</th>\n",
       "      <th>Grammar</th>\n",
       "      <th>Tokenization</th>\n",
       "      <th>Verb_count</th>\n",
       "      <th>Propn_count</th>\n",
       "      <th>Noun_count</th>\n",
       "      <th>Num_count</th>\n",
       "      <th>Pourcentage_verb_sentence</th>\n",
       "      <th>Pourcentage_propn_sentence</th>\n",
       "      <th>Pourcentage_noun_sentence</th>\n",
       "      <th>Pourcentage_num_sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE</td>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.80</td>\n",
       "      <td>97.20</td>\n",
       "      <td>[True, True, True, False, True, True]</td>\n",
       "      <td>['NOUN', 'PROPN', 'VERB', 'NUM', 'PROPN', 'PRO...</td>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PROFIL</td>\n",
       "      <td>PROFIL</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.27</td>\n",
       "      <td>96.73</td>\n",
       "      <td>[True]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td>PROFIL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PERSONNEL Je suis étudiante au lycée Condorcet.</td>\n",
       "      <td>PERSONNEL Je suis étudiante au lycée Condorcet.</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7.01</td>\n",
       "      <td>92.99</td>\n",
       "      <td>[True, True, True, True, True, True, True, False]</td>\n",
       "      <td>['NOUN', 'PRON', 'AUX', 'ADJ', 'ADP', 'NOUN', ...</td>\n",
       "      <td>PERSONNEL Je suis étudiante au lycée Condorcet.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je code depuis l'âge de 13 ans et j'aime créer...</td>\n",
       "      <td>Je code depuis l'âge de 13 ans et j'aime créer...</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>18.22</td>\n",
       "      <td>81.78</td>\n",
       "      <td>[True, True, True, False, True, True, False, T...</td>\n",
       "      <td>['PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', ...</td>\n",
       "      <td>Je code depuis l'âge de 13 ans et j'aime créer...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RÉALISATIONS</td>\n",
       "      <td>RÉALISATIONS</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>18.69</td>\n",
       "      <td>81.31</td>\n",
       "      <td>[True]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td>RÉALISATIONS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        CV_Sentences  \\\n",
       "0         SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE   \n",
       "1                                             PROFIL   \n",
       "2    PERSONNEL Je suis étudiante au lycée Condorcet.   \n",
       "3  Je code depuis l'âge de 13 ans et j'aime créer...   \n",
       "4                                       RÉALISATIONS   \n",
       "\n",
       "                                  Sentences_CV_clean CV_Number  Sentence_line  \\\n",
       "0         SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE      CV_1              0   \n",
       "1                                             PROFIL      CV_1              1   \n",
       "2    PERSONNEL Je suis étudiante au lycée Condorcet.      CV_1              2   \n",
       "3  Je code depuis l'âge de 13 ans et j'aime créer...      CV_1              3   \n",
       "4                                       RÉALISATIONS      CV_1              4   \n",
       "\n",
       "   Nb_tokens  %texte_lu  %texte_lu_fin_ligne  \\\n",
       "0          6       2.80                97.20   \n",
       "1          1       3.27                96.73   \n",
       "2          8       7.01                92.99   \n",
       "3         24      18.22                81.78   \n",
       "4          1      18.69                81.31   \n",
       "\n",
       "                                            Is_alpha  \\\n",
       "0              [True, True, True, False, True, True]   \n",
       "1                                             [True]   \n",
       "2  [True, True, True, True, True, True, True, False]   \n",
       "3  [True, True, True, False, True, True, False, T...   \n",
       "4                                             [True]   \n",
       "\n",
       "                                             Grammar  \\\n",
       "0  ['NOUN', 'PROPN', 'VERB', 'NUM', 'PROPN', 'PRO...   \n",
       "1                                           ['NOUN']   \n",
       "2  ['NOUN', 'PRON', 'AUX', 'ADJ', 'ADP', 'NOUN', ...   \n",
       "3  ['PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', ...   \n",
       "4                                           ['NOUN']   \n",
       "\n",
       "                                        Tokenization  Verb_count  Propn_count  \\\n",
       "0         SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE           1            3   \n",
       "1                                             PROFIL           0            0   \n",
       "2    PERSONNEL Je suis étudiante au lycée Condorcet.           0            1   \n",
       "3  Je code depuis l'âge de 13 ans et j'aime créer...           5            1   \n",
       "4                                       RÉALISATIONS           0            0   \n",
       "\n",
       "   Noun_count  Num_count  Pourcentage_verb_sentence  \\\n",
       "0           1          1                   0.166667   \n",
       "1           1          0                   0.000000   \n",
       "2           2          0                   0.000000   \n",
       "3           5          1                   0.208333   \n",
       "4           1          0                   0.000000   \n",
       "\n",
       "   Pourcentage_propn_sentence  Pourcentage_noun_sentence  \\\n",
       "0                    0.500000                   0.166667   \n",
       "1                    0.000000                   1.000000   \n",
       "2                    0.125000                   0.250000   \n",
       "3                    0.041667                   0.208333   \n",
       "4                    0.000000                   1.000000   \n",
       "\n",
       "   Pourcentage_num_sentence  Label  \n",
       "0                  0.166667      1  \n",
       "1                  0.000000      0  \n",
       "2                  0.000000      0  \n",
       "3                  0.041667      0  \n",
       "4                  0.000000      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basics statistics: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_Sentences</th>\n",
       "      <th>Sentences_CV_clean</th>\n",
       "      <th>CV_Number</th>\n",
       "      <th>Sentence_line</th>\n",
       "      <th>Nb_tokens</th>\n",
       "      <th>%texte_lu</th>\n",
       "      <th>%texte_lu_fin_ligne</th>\n",
       "      <th>Is_alpha</th>\n",
       "      <th>Grammar</th>\n",
       "      <th>Tokenization</th>\n",
       "      <th>Verb_count</th>\n",
       "      <th>Propn_count</th>\n",
       "      <th>Noun_count</th>\n",
       "      <th>Num_count</th>\n",
       "      <th>Pourcentage_verb_sentence</th>\n",
       "      <th>Pourcentage_propn_sentence</th>\n",
       "      <th>Pourcentage_noun_sentence</th>\n",
       "      <th>Pourcentage_num_sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5594</td>\n",
       "      <td>5592</td>\n",
       "      <td>5594</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594</td>\n",
       "      <td>5594</td>\n",
       "      <td>5592</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.00000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3805</td>\n",
       "      <td>3802</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1626</td>\n",
       "      <td>2748</td>\n",
       "      <td>3802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CV_10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>638</td>\n",
       "      <td>334</td>\n",
       "      <td>198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.673221</td>\n",
       "      <td>9.132285</td>\n",
       "      <td>50.470518</td>\n",
       "      <td>49.529483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.297283</td>\n",
       "      <td>0.611012</td>\n",
       "      <td>2.83822</td>\n",
       "      <td>0.320164</td>\n",
       "      <td>0.020117</td>\n",
       "      <td>0.105039</td>\n",
       "      <td>0.314979</td>\n",
       "      <td>0.042982</td>\n",
       "      <td>0.037898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.268076</td>\n",
       "      <td>10.914849</td>\n",
       "      <td>30.190027</td>\n",
       "      <td>30.190025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.784780</td>\n",
       "      <td>1.129198</td>\n",
       "      <td>3.56600</td>\n",
       "      <td>0.693458</td>\n",
       "      <td>0.059959</td>\n",
       "      <td>0.227969</td>\n",
       "      <td>0.251108</td>\n",
       "      <td>0.131906</td>\n",
       "      <td>0.190966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>24.190000</td>\n",
       "      <td>23.552500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>49.530000</td>\n",
       "      <td>50.470000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>76.447500</td>\n",
       "      <td>75.810000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.760000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>39.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CV_Sentences Sentences_CV_clean CV_Number  Sentence_line    Nb_tokens  \\\n",
       "count          5594               5592      5594    5594.000000  5594.000000   \n",
       "unique         3805               3802       200            NaN          NaN   \n",
       "top                                        CV_10            NaN          NaN   \n",
       "freq            198                198        66            NaN          NaN   \n",
       "mean            NaN                NaN       NaN      15.673221     9.132285   \n",
       "std             NaN                NaN       NaN      11.268076    10.914849   \n",
       "min             NaN                NaN       NaN       0.000000     1.000000   \n",
       "25%             NaN                NaN       NaN       7.000000     2.000000   \n",
       "50%             NaN                NaN       NaN      14.000000     5.000000   \n",
       "75%             NaN                NaN       NaN      23.000000    12.000000   \n",
       "max             NaN                NaN       NaN      65.000000   124.000000   \n",
       "\n",
       "          %texte_lu  %texte_lu_fin_ligne Is_alpha   Grammar Tokenization  \\\n",
       "count   5594.000000          5594.000000     5594      5594         5592   \n",
       "unique          NaN                  NaN     1626      2748         3802   \n",
       "top             NaN                  NaN  [False]  ['NOUN']                \n",
       "freq            NaN                  NaN      638       334          198   \n",
       "mean      50.470518            49.529483      NaN       NaN          NaN   \n",
       "std       30.190027            30.190025      NaN       NaN          NaN   \n",
       "min        0.240000             0.000000      NaN       NaN          NaN   \n",
       "25%       24.190000            23.552500      NaN       NaN          NaN   \n",
       "50%       49.530000            50.470000      NaN       NaN          NaN   \n",
       "75%       76.447500            75.810000      NaN       NaN          NaN   \n",
       "max      100.000000            99.760000      NaN       NaN          NaN   \n",
       "\n",
       "         Verb_count  Propn_count  Noun_count    Num_count  \\\n",
       "count   5594.000000  5594.000000  5594.00000  5594.000000   \n",
       "unique          NaN          NaN         NaN          NaN   \n",
       "top             NaN          NaN         NaN          NaN   \n",
       "freq            NaN          NaN         NaN          NaN   \n",
       "mean       0.297283     0.611012     2.83822     0.320164   \n",
       "std        0.784780     1.129198     3.56600     0.693458   \n",
       "min        0.000000     0.000000     0.00000     0.000000   \n",
       "25%        0.000000     0.000000     1.00000     0.000000   \n",
       "50%        0.000000     0.000000     2.00000     0.000000   \n",
       "75%        0.000000     1.000000     4.00000     0.000000   \n",
       "max        8.000000    14.000000    39.00000     8.000000   \n",
       "\n",
       "        Pourcentage_verb_sentence  Pourcentage_propn_sentence  \\\n",
       "count                 5594.000000                 5594.000000   \n",
       "unique                        NaN                         NaN   \n",
       "top                           NaN                         NaN   \n",
       "freq                          NaN                         NaN   \n",
       "mean                     0.020117                    0.105039   \n",
       "std                      0.059959                    0.227969   \n",
       "min                      0.000000                    0.000000   \n",
       "25%                      0.000000                    0.000000   \n",
       "50%                      0.000000                    0.000000   \n",
       "75%                      0.000000                    0.102564   \n",
       "max                      1.000000                    1.000000   \n",
       "\n",
       "        Pourcentage_noun_sentence  Pourcentage_num_sentence        Label  \n",
       "count                 5594.000000               5594.000000  5594.000000  \n",
       "unique                        NaN                       NaN          NaN  \n",
       "top                           NaN                       NaN          NaN  \n",
       "freq                          NaN                       NaN          NaN  \n",
       "mean                     0.314979                  0.042982     0.037898  \n",
       "std                      0.251108                  0.131906     0.190966  \n",
       "min                      0.000000                  0.000000     0.000000  \n",
       "25%                      0.157895                  0.000000     0.000000  \n",
       "50%                      0.307692                  0.000000     0.000000  \n",
       "75%                      0.416667                  0.000000     0.000000  \n",
       "max                      1.000000                  1.000000     1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CV_Sentences                  0.000000\n",
       "Sentences_CV_clean            0.035753\n",
       "CV_Number                     0.000000\n",
       "Sentence_line                 0.000000\n",
       "Nb_tokens                     0.000000\n",
       "%texte_lu                     0.000000\n",
       "%texte_lu_fin_ligne           0.000000\n",
       "Is_alpha                      0.000000\n",
       "Grammar                       0.000000\n",
       "Tokenization                  0.035753\n",
       "Verb_count                    0.000000\n",
       "Propn_count                   0.000000\n",
       "Noun_count                    0.000000\n",
       "Num_count                     0.000000\n",
       "Pourcentage_verb_sentence     0.000000\n",
       "Pourcentage_propn_sentence    0.000000\n",
       "Pourcentage_noun_sentence     0.000000\n",
       "Pourcentage_num_sentence      0.000000\n",
       "Label                         0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic stats\n",
    "print(\"Number of rows : {}\".format(dataset.shape[0]))\n",
    "print()\n",
    "\n",
    "print(\"Display of dataset: \")\n",
    "display(dataset.head())\n",
    "print()\n",
    "\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = dataset.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "print(\"Percentage of missing values: \")\n",
    "display(100*dataset.isnull().sum()/dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5416, 19)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Label\"].unique()\n",
    "dataset = dataset.loc[(dataset['CV_Sentences'] != \"#NOM?\"),:]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5412, 19)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.loc[(dataset['CV_Sentences'] != \":\"),:]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5410, 19)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna(axis =0, how = 'any')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV_Sentences                  0.0\n",
       "Sentences_CV_clean            0.0\n",
       "CV_Number                     0.0\n",
       "Sentence_line                 0.0\n",
       "Nb_tokens                     0.0\n",
       "%texte_lu                     0.0\n",
       "%texte_lu_fin_ligne           0.0\n",
       "Is_alpha                      0.0\n",
       "Grammar                       0.0\n",
       "Tokenization                  0.0\n",
       "Verb_count                    0.0\n",
       "Propn_count                   0.0\n",
       "Noun_count                    0.0\n",
       "Num_count                     0.0\n",
       "Pourcentage_verb_sentence     0.0\n",
       "Pourcentage_propn_sentence    0.0\n",
       "Pourcentage_noun_sentence     0.0\n",
       "Pourcentage_num_sentence      0.0\n",
       "Label                         0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(100*dataset.isnull().sum()/dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating labels from features...\n",
      "...Done.\n",
      "\n",
      "Y : \n",
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "X :\n",
      "   Sentence_line  Nb_tokens  %texte_lu  %texte_lu_fin_ligne  Verb_count  \\\n",
      "0              0          6       2.80                97.20           1   \n",
      "1              1          1       3.27                96.73           0   \n",
      "2              2          8       7.01                92.99           0   \n",
      "3              3         24      18.22                81.78           5   \n",
      "4              4          1      18.69                81.31           0   \n",
      "\n",
      "   Propn_count  Noun_count  Num_count  Pourcentage_verb_sentence  \\\n",
      "0            3           1          1                   0.166667   \n",
      "1            0           1          0                   0.000000   \n",
      "2            1           2          0                   0.000000   \n",
      "3            1           5          1                   0.208333   \n",
      "4            0           1          0                   0.000000   \n",
      "\n",
      "   Pourcentage_propn_sentence  Pourcentage_noun_sentence  \\\n",
      "0                    0.500000                   0.166667   \n",
      "1                    0.000000                   1.000000   \n",
      "2                    0.125000                   0.250000   \n",
      "3                    0.041667                   0.208333   \n",
      "4                    0.000000                   1.000000   \n",
      "\n",
      "   Pourcentage_num_sentence  \n",
      "0                  0.166667  \n",
      "1                  0.000000  \n",
      "2                  0.000000  \n",
      "3                  0.041667  \n",
      "4                  0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Separate target variable Y from features X\n",
    "print(\"Separating labels from features...\")\n",
    "\n",
    "## Choisir la features_list par rapport au dataset\n",
    "#features_list = [\"CV_Sentences\",\"Sentences_CV_clean\",\"CV_Number\", \"Is_alpha\", \"Grammar\", \"Label\"] ## Pour les datasets features1\n",
    "features_list = [\"CV_Sentences\",\"Sentences_CV_clean\",\"CV_Number\", \"Is_alpha\", \"Label\", \"Grammar\", \"Tokenization\"] ## Pour les datasets features2\n",
    "target_variable = \"Label\"\n",
    "\n",
    "X = dataset.drop(features_list, axis = 1)\n",
    "Y = dataset.loc[:,target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print('Y : ')\n",
    "print(Y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numeric features  ['Sentence_line', 'Nb_tokens', '%texte_lu', '%texte_lu_fin_ligne', 'Verb_count', 'Propn_count', 'Noun_count', 'Num_count', 'Pourcentage_verb_sentence', 'Pourcentage_propn_sentence', 'Pourcentage_noun_sentence', 'Pourcentage_num_sentence']  at positions  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Found categorical features  []  at positions  []\n"
     ]
    }
   ],
   "source": [
    "# Automatically detect positions of numeric/categorical features\n",
    "idx = 0\n",
    "numeric_features = []\n",
    "numeric_indices = []\n",
    "categorical_features = []\n",
    "categorical_indices = []\n",
    "for i,t in X.dtypes.iteritems():\n",
    "    if ('float' in str(t)) or ('int' in str(t)) :\n",
    "        numeric_features.append(i)\n",
    "        numeric_indices.append(idx)\n",
    "    else :\n",
    "        categorical_features.append(i)\n",
    "        categorical_indices.append(idx)\n",
    "\n",
    "    idx = idx + 1\n",
    "\n",
    "print('Found numeric features ', numeric_features,' at positions ', numeric_indices)\n",
    "print('Found categorical features ', categorical_features,' at positions ', categorical_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing into train and test sets...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "print(\"Dividing into train and test sets...\")\n",
    "# WARNING : don't forget stratify=Y for classification problems\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify = Y)\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert pandas DataFrames to numpy arrays...\n",
      "...Done\n",
      "[[25.          1.         45.85       54.15        0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [14.          6.         76.08       23.92        1.          0.\n",
      "   2.          0.          0.16666667  0.          0.33333333  0.        ]\n",
      " [17.          3.         47.06       52.94        0.          1.\n",
      "   1.          0.          0.          0.33333333  0.33333333  0.        ]\n",
      " [11.          1.         27.         73.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [ 6.          9.         20.78       79.22        1.          0.\n",
      "   1.          0.          0.11111111  0.          0.11111111  0.        ]]\n",
      "[[ 3.         15.         35.22       64.78        1.          0.\n",
      "   4.          0.          0.06666667  0.          0.26666667  0.        ]\n",
      " [13.          2.         36.42       63.58        0.          0.\n",
      "   0.          1.          0.          0.          0.          0.5       ]]\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrames to numpy arrays before using scikit-learn\n",
    "print(\"Convert pandas DataFrames to numpy arrays...\")\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "Y_train = Y_train.tolist()\n",
    "Y_test = Y_test.tolist()\n",
    "print(\"...Done\")\n",
    "\n",
    "print(X_train[0:5,:])\n",
    "print(X_test[0:2,:])\n",
    "print()\n",
    "print(Y_train[0:5])\n",
    "print(Y_test[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "[[25.          1.         45.85       54.15        0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [14.          6.         76.08       23.92        1.          0.\n",
      "   2.          0.          0.16666667  0.          0.33333333  0.        ]\n",
      " [17.          3.         47.06       52.94        0.          1.\n",
      "   1.          0.          0.          0.33333333  0.33333333  0.        ]\n",
      " [11.          1.         27.         73.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [ 6.          9.         20.78       79.22        1.          0.\n",
      "   1.          0.          0.11111111  0.          0.11111111  0.        ]]\n",
      "\n",
      "...Done!\n",
      "[[25.          1.         45.85       54.15        0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [14.          6.         76.08       23.92        1.          0.\n",
      "   2.          0.          0.16666667  0.          0.33333333  0.        ]\n",
      " [17.          3.         47.06       52.94        0.          1.\n",
      "   1.          0.          0.          0.33333333  0.33333333  0.        ]\n",
      " [11.          1.         27.         73.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [ 6.          9.         20.78       79.22        1.          0.\n",
      "   1.          0.          0.11111111  0.          0.11111111  0.        ]]\n",
      "\n",
      "Encoding categorical features and standardizing numerical features...\n",
      "\n",
      "[[25.          1.         45.85       54.15        0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [14.          6.         76.08       23.92        1.          0.\n",
      "   2.          0.          0.16666667  0.          0.33333333  0.        ]\n",
      " [17.          3.         47.06       52.94        0.          1.\n",
      "   1.          0.          0.          0.33333333  0.33333333  0.        ]\n",
      " [11.          1.         27.         73.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [ 6.          9.         20.78       79.22        1.          0.\n",
      "   1.          0.          0.11111111  0.          0.11111111  0.        ]]\n",
      "...Done\n",
      "[[ 0.84941168 -0.74490809 -0.14998016  0.14998009 -0.39287215  0.31994356\n",
      "  -0.80179486 -0.47323519 -0.35378762  3.88915581 -1.25327628 -0.33178113]\n",
      " [-0.13204091 -0.30377703  0.8477846  -0.84778473  0.81648109 -0.55369413\n",
      "  -0.25826472 -0.47323519  2.3625681  -0.46742016  0.07457067 -0.33178113]\n",
      " [ 0.13562797 -0.56845567 -0.11004316  0.1100431  -0.39287215  0.31994356\n",
      "  -0.53002979 -0.47323519 -0.35378762  0.98477183  0.07457067 -0.33178113]\n",
      " [-0.3997098  -0.74490809 -0.77213913  0.7721391  -0.39287215  0.31994356\n",
      "  -0.80179486 -0.47323519 -0.35378762  3.88915581 -1.25327628 -0.33178113]\n",
      " [-0.84582462 -0.0390984  -0.97743509  0.97743507  0.81648109 -0.55369413\n",
      "  -0.53002979 -0.47323519  1.45711619 -0.46742016 -0.81066063 -0.33178113]]\n",
      "Encoding labels...\n",
      "[0, 0, 0, 0, 0]\n",
      "...Done\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "print(\"Imputing missing values...\")\n",
    "print(X_train[0:5,:])\n",
    "print()\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train[:,numeric_indices] = imputer.fit_transform(X_train[:,numeric_indices])\n",
    "print(\"...Done!\")\n",
    "print(X_train[0:5,:]) \n",
    "print() \n",
    "\n",
    "# Encoding categorical features and standardizing numerical features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print()\n",
    "print(X_train[0:5,:])\n",
    "\n",
    "# Normalization\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# OHE / dummyfication\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "featureencoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_indices),    \n",
    "        ('num', numeric_transformer, numeric_indices)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "X_train = featureencoder.fit_transform(X_train)\n",
    "print(\"...Done\")\n",
    "print(X_train[0:5,:])\n",
    "\n",
    "# Label encoding\n",
    "print(\"Encoding labels...\")\n",
    "print(Y_train[0:5])\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"...Done\")\n",
    "print(Y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "...Done.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Train model...\")\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "print(\"...Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on training set...\n",
      "...Done.\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on training set\n",
    "print(\"Predictions on training set...\")\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print(Y_train_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "[[3.00000000e+00 1.50000000e+01 3.52200000e+01 6.47800000e+01\n",
      "  1.00000000e+00 0.00000000e+00 4.00000000e+00 0.00000000e+00\n",
      "  6.66666667e-02 0.00000000e+00 2.66666667e-01 0.00000000e+00]\n",
      " [1.30000000e+01 2.00000000e+00 3.64200000e+01 6.35800000e+01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.00000000e-01]\n",
      " [2.60000000e+01 3.00000000e+00 7.32900000e+01 2.67100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 2.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.33333333e-01 6.66666667e-01]\n",
      " [5.00000000e+00 3.00000000e+00 4.65900000e+01 5.34100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.66666667e-01 0.00000000e+00]\n",
      " [2.00000000e+01 9.00000000e+00 9.94600000e+01 5.40000000e-01\n",
      "  0.00000000e+00 1.00000000e+00 2.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 1.11111111e-01 2.22222222e-01 1.11111111e-01]]\n",
      "...Done!\n",
      "[[3.00000000e+00 1.50000000e+01 3.52200000e+01 6.47800000e+01\n",
      "  1.00000000e+00 0.00000000e+00 4.00000000e+00 0.00000000e+00\n",
      "  6.66666667e-02 0.00000000e+00 2.66666667e-01 0.00000000e+00]\n",
      " [1.30000000e+01 2.00000000e+00 3.64200000e+01 6.35800000e+01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.00000000e-01]\n",
      " [2.60000000e+01 3.00000000e+00 7.32900000e+01 2.67100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 2.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.33333333e-01 6.66666667e-01]\n",
      " [5.00000000e+00 3.00000000e+00 4.65900000e+01 5.34100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.66666667e-01 0.00000000e+00]\n",
      " [2.00000000e+01 9.00000000e+00 9.94600000e+01 5.40000000e-01\n",
      "  0.00000000e+00 1.00000000e+00 2.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 1.11111111e-01 2.22222222e-01 1.11111111e-01]]\n",
      "\n",
      "Encoding categorical features and standardizing numerical features...\n",
      "[[3.00000000e+00 1.50000000e+01 3.52200000e+01 6.47800000e+01\n",
      "  1.00000000e+00 0.00000000e+00 4.00000000e+00 0.00000000e+00\n",
      "  6.66666667e-02 0.00000000e+00 2.66666667e-01 0.00000000e+00]\n",
      " [1.30000000e+01 2.00000000e+00 3.64200000e+01 6.35800000e+01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.00000000e-01]\n",
      " [2.60000000e+01 3.00000000e+00 7.32900000e+01 2.67100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 2.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.33333333e-01 6.66666667e-01]\n",
      " [5.00000000e+00 3.00000000e+00 4.65900000e+01 5.34100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.66666667e-01 0.00000000e+00]\n",
      " [2.00000000e+01 9.00000000e+00 9.94600000e+01 5.40000000e-01\n",
      "  0.00000000e+00 1.00000000e+00 2.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 1.11111111e-01 2.22222222e-01 1.11111111e-01]]\n",
      "...Done\n",
      "[[-1.1134935   0.49025887 -0.50083161  0.50083157  0.81648109 -0.55369413\n",
      "   0.28526541 -0.47323519  0.73275467 -0.46742016 -0.19099872 -0.33178113]\n",
      " [-0.22126388 -0.65668188 -0.46122467  0.46122463 -0.39287215 -0.55369413\n",
      "  -0.80179486  0.92102069 -0.35378762 -0.46742016 -1.25327628  3.2859748 ]\n",
      " [ 0.93863464 -0.56845567  0.75569847 -0.75569859 -0.39287215 -0.55369413\n",
      "  -0.53002979  2.31527657 -0.35378762 -0.46742016  0.07457067  4.49189344]\n",
      " [-0.93504758 -0.56845567 -0.12555588  0.12555581 -0.39287215 -0.55369413\n",
      "  -0.25826472 -0.47323519 -0.35378762 -0.46742016  1.40241762 -0.33178113]\n",
      " [ 0.40329686 -0.0390984   1.61945976 -1.61945993 -0.39287215  0.31994356\n",
      "  -0.25826472  0.92102069 -0.35378762  0.01664384 -0.36804498  0.47216463]]\n",
      "Encoding labels...\n",
      "[0, 0, 0, 0, 0]\n",
      "...Done\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "print(\"Imputing missing values...\")\n",
    "print(X_test[0:5,:])\n",
    "X_test[:,numeric_indices] = imputer.transform(X_test[:,numeric_indices])\n",
    "print(\"...Done!\")\n",
    "print(X_test[0:5,:]) \n",
    "print() \n",
    "\n",
    "# Encoding categorical features and standardizing numerical features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print(X_test[0:5,:])\n",
    "X_test = featureencoder.transform(X_test)\n",
    "print(\"...Done\")\n",
    "print(X_test[0:5,:])\n",
    "\n",
    "# Label encoding\n",
    "print(\"Encoding labels...\")\n",
    "print(Y_test[0:5])\n",
    "Y_test = encoder.transform(Y_test)\n",
    "print(\"...Done\")\n",
    "print(Y_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set...\n",
      "...Done.\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on test set\n",
    "print(\"Predictions on test set...\")\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print(Y_test_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set :  0.9678835489833642\n",
      "accuracy on test set :  0.9658040665434381\n",
      "\n",
      "f1-score on training set :  0.39301310043668125\n",
      "f1-score on test set :  0.3018867924528302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print scores\n",
    "print(\"accuracy on training set : \", accuracy_score(Y_train, Y_train_pred))\n",
    "print(\"accuracy on test set : \", accuracy_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "print(\"f1-score on training set : \", f1_score(Y_train, Y_train_pred))                                            ## ATTENTION REVOIR F1-SCORE\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "train",
         "type": "scatter",
         "x": [
          0,
          0.011764705882352941,
          0.011764705882352941,
          0.052941176470588235,
          0.052941176470588235,
          0.06470588235294118,
          0.06470588235294118,
          0.08235294117647059,
          0.10588235294117647,
          0.13529411764705881,
          0.13529411764705881,
          0.14705882352941177,
          0.14705882352941177,
          0.15294117647058825,
          0.15294117647058825,
          0.17058823529411765,
          0.18235294117647058,
          0.18823529411764706,
          0.18823529411764706,
          0.19411764705882353,
          0.20588235294117646,
          0.20588235294117646,
          0.2411764705882353,
          0.2411764705882353,
          0.24705882352941178,
          0.2529411764705882,
          0.2529411764705882,
          0.25882352941176473,
          0.25882352941176473,
          0.27058823529411763,
          0.2823529411764706,
          0.2823529411764706,
          0.28823529411764703,
          0.28823529411764703,
          0.29411764705882354,
          0.29411764705882354,
          0.3058823529411765,
          0.3058823529411765,
          0.3176470588235294,
          0.3176470588235294,
          0.3352941176470588,
          0.3352941176470588,
          0.34705882352941175,
          0.34705882352941175,
          0.35294117647058826,
          0.35294117647058826,
          0.3588235294117647,
          0.3588235294117647,
          0.36470588235294116,
          0.36470588235294116,
          0.37058823529411766,
          0.37058823529411766,
          0.3764705882352941,
          0.3764705882352941,
          0.3941176470588235,
          0.3941176470588235,
          0.4235294117647059,
          0.4235294117647059,
          0.4235294117647059,
          0.4235294117647059,
          0.4294117647058823,
          0.4294117647058823,
          0.43529411764705883,
          0.43529411764705883,
          0.4470588235294118,
          0.4470588235294118,
          0.4470588235294118,
          0.4470588235294118,
          0.45294117647058824,
          0.45294117647058824,
          0.45294117647058824,
          0.45294117647058824,
          0.4588235294117647,
          0.4647058823529412,
          0.4647058823529412,
          0.4647058823529412,
          0.47058823529411764,
          0.47058823529411764,
          0.4823529411764706,
          0.4823529411764706,
          0.48823529411764705,
          0.48823529411764705,
          0.49411764705882355,
          0.49411764705882355,
          0.49411764705882355,
          0.5058823529411764,
          0.5058823529411764,
          0.5294117647058824,
          0.5294117647058824,
          0.5352941176470588,
          0.5352941176470588,
          0.5352941176470588,
          0.5352941176470588,
          0.5411764705882353,
          0.5411764705882353,
          0.5470588235294118,
          0.5470588235294118,
          0.5470588235294118,
          0.5588235294117647,
          0.5588235294117647,
          0.5705882352941176,
          0.5705882352941176,
          0.5764705882352941,
          0.5764705882352941,
          0.5823529411764706,
          0.5823529411764706,
          0.5941176470588235,
          0.5941176470588235,
          0.6058823529411764,
          0.6058823529411764,
          0.6058823529411764,
          0.611764705882353,
          0.611764705882353,
          0.611764705882353,
          0.611764705882353,
          0.6176470588235294,
          0.6176470588235294,
          0.6176470588235294,
          0.6235294117647059,
          0.6235294117647059,
          0.6235294117647059,
          0.6294117647058823,
          0.6294117647058823,
          0.6352941176470588,
          0.6352941176470588,
          0.6411764705882353,
          0.6411764705882353,
          0.6470588235294118,
          0.6470588235294118,
          0.6529411764705882,
          0.6529411764705882,
          0.6529411764705882,
          0.6529411764705882,
          0.6529411764705882,
          0.6529411764705882,
          0.6529411764705882,
          0.6529411764705882,
          0.6588235294117647,
          0.6588235294117647,
          0.6588235294117647,
          0.6588235294117647,
          0.6647058823529411,
          0.6647058823529411,
          0.6647058823529411,
          0.6647058823529411,
          0.6764705882352942,
          0.6764705882352942,
          0.6823529411764706,
          0.6823529411764706,
          0.6882352941176471,
          0.6882352941176471,
          0.6941176470588235,
          0.6941176470588235,
          0.6941176470588235,
          0.6941176470588235,
          0.7,
          0.7,
          0.7,
          0.7058823529411765,
          0.7058823529411765,
          0.7058823529411765,
          0.711764705882353,
          0.711764705882353,
          0.7235294117647059,
          0.7235294117647059,
          0.7235294117647059,
          0.7235294117647059,
          0.7294117647058823,
          0.7294117647058823,
          0.7352941176470589,
          0.7352941176470589,
          0.7411764705882353,
          0.7411764705882353,
          0.7411764705882353,
          0.7411764705882353,
          0.7411764705882353,
          0.7411764705882353,
          0.7470588235294118,
          0.7470588235294118,
          0.7470588235294118,
          0.7470588235294118,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7588235294117647,
          0.7647058823529411,
          0.7647058823529411,
          0.7647058823529411,
          0.7647058823529411,
          0.7705882352941177,
          0.7705882352941177,
          0.7705882352941177,
          0.7705882352941177,
          0.7705882352941177,
          0.7705882352941177,
          0.7705882352941177,
          0.7764705882352941,
          0.7764705882352941,
          0.7764705882352941,
          0.7764705882352941,
          0.7764705882352941,
          0.7764705882352941,
          0.7764705882352941,
          0.7764705882352941,
          0.7764705882352941,
          0.7764705882352941,
          0.788235294117647,
          0.788235294117647,
          0.788235294117647,
          0.788235294117647,
          0.7941176470588235,
          0.7941176470588235,
          0.8,
          0.8,
          0.8,
          0.8,
          0.8058823529411765,
          0.8058823529411765,
          0.8058823529411765,
          0.8058823529411765,
          0.8058823529411765,
          0.8058823529411765,
          0.8058823529411765,
          0.8058823529411765,
          0.8058823529411765,
          0.8058823529411765,
          0.8117647058823529,
          0.8117647058823529,
          0.8176470588235294,
          0.8176470588235294,
          0.8176470588235294,
          0.8176470588235294,
          0.8235294117647058,
          0.8235294117647058,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8294117647058824,
          0.8352941176470589,
          0.8352941176470589,
          0.8470588235294118,
          0.8470588235294118,
          0.8470588235294118,
          0.8470588235294118,
          0.8470588235294118,
          0.8470588235294118,
          0.8470588235294118,
          0.8470588235294118,
          0.8470588235294118,
          0.8529411764705882,
          0.8529411764705882,
          0.8529411764705882,
          0.8529411764705882,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8588235294117647,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8647058823529412,
          0.8705882352941177,
          0.8705882352941177,
          0.8705882352941177,
          0.8705882352941177,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8764705882352941,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.8823529411764706,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.888235294117647,
          0.8941176470588236,
          0.8941176470588236,
          0.8941176470588236,
          0.8941176470588236,
          0.8941176470588236,
          0.8941176470588236,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9,
          0.9058823529411765,
          0.9058823529411765,
          0.9117647058823529,
          0.9117647058823529,
          0.9117647058823529,
          0.9117647058823529,
          0.9176470588235294,
          0.9176470588235294,
          0.9176470588235294,
          0.9176470588235294,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9235294117647059,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9294117647058824,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9352941176470588,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9411764705882353,
          0.9470588235294117,
          0.9470588235294117,
          0.9470588235294117,
          0.9470588235294117,
          0.9529411764705882,
          0.9529411764705882,
          0.9529411764705882,
          0.9529411764705882,
          0.9529411764705882,
          0.9529411764705882,
          0.9529411764705882,
          0.9529411764705882,
          0.9588235294117647,
          0.9588235294117647,
          0.9588235294117647,
          0.9588235294117647,
          0.9588235294117647,
          0.9588235294117647,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9647058823529412,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9764705882352941,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9823529411764705,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9882352941176471,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          0.9941176470588236,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "y": [
          0,
          0,
          0.0002405002405002405,
          0.0002405002405002405,
          0.000481000481000481,
          0.000481000481000481,
          0.000962000962000962,
          0.000962000962000962,
          0.000962000962000962,
          0.000962000962000962,
          0.0012025012025012026,
          0.0012025012025012026,
          0.0016835016835016834,
          0.0016835016835016834,
          0.001924001924001924,
          0.001924001924001924,
          0.001924001924001924,
          0.001924001924001924,
          0.0021645021645021645,
          0.0021645021645021645,
          0.0021645021645021645,
          0.002405002405002405,
          0.002405002405002405,
          0.0026455026455026454,
          0.002886002886002886,
          0.002886002886002886,
          0.0031265031265031266,
          0.0031265031265031266,
          0.003367003367003367,
          0.003367003367003367,
          0.003367003367003367,
          0.003848003848003848,
          0.003848003848003848,
          0.004088504088504088,
          0.004088504088504088,
          0.00456950456950457,
          0.00456950456950457,
          0.005050505050505051,
          0.005050505050505051,
          0.005291005291005291,
          0.005291005291005291,
          0.005531505531505531,
          0.005531505531505531,
          0.005772005772005772,
          0.005772005772005772,
          0.006734006734006734,
          0.006734006734006734,
          0.007696007696007696,
          0.007696007696007696,
          0.008417508417508417,
          0.008417508417508417,
          0.008898508898508899,
          0.008898508898508899,
          0.00913900913900914,
          0.00913900913900914,
          0.010101010101010102,
          0.010101010101010102,
          0.010582010582010581,
          0.011063011063011063,
          0.012025012025012025,
          0.012025012025012025,
          0.012265512265512266,
          0.012265512265512266,
          0.012506012506012507,
          0.012506012506012507,
          0.012746512746512747,
          0.013227513227513227,
          0.013708513708513708,
          0.013708513708513708,
          0.015873015873015872,
          0.016354016354016353,
          0.016594516594516596,
          0.016835016835016835,
          0.016835016835016835,
          0.017556517556517555,
          0.018037518037518036,
          0.018037518037518036,
          0.01827801827801828,
          0.01827801827801828,
          0.01948051948051948,
          0.01948051948051948,
          0.02068302068302068,
          0.02068302068302068,
          0.020923520923520924,
          0.021404521404521405,
          0.021404521404521405,
          0.021885521885521887,
          0.021885521885521887,
          0.02284752284752285,
          0.02284752284752285,
          0.023088023088023088,
          0.02356902356902357,
          0.02429052429052429,
          0.02429052429052429,
          0.024531024531024532,
          0.024531024531024532,
          0.025012025012025013,
          0.025493025493025494,
          0.025493025493025494,
          0.025974025974025976,
          0.025974025974025976,
          0.026695526695526696,
          0.026695526695526696,
          0.026936026936026935,
          0.026936026936026935,
          0.02813852813852814,
          0.02813852813852814,
          0.02886002886002886,
          0.02886002886002886,
          0.030303030303030304,
          0.030784030784030785,
          0.030784030784030785,
          0.031024531024531024,
          0.031746031746031744,
          0.03222703222703223,
          0.03222703222703223,
          0.03270803270803271,
          0.03367003367003367,
          0.03367003367003367,
          0.034151034151034154,
          0.03583453583453584,
          0.03583453583453584,
          0.036315536315536315,
          0.036315536315536315,
          0.0367965367965368,
          0.0367965367965368,
          0.037037037037037035,
          0.037037037037037035,
          0.03751803751803752,
          0.03751803751803752,
          0.037999037999038,
          0.03848003848003848,
          0.03872053872053872,
          0.0392015392015392,
          0.04184704184704185,
          0.042328042328042326,
          0.04256854256854257,
          0.04256854256854257,
          0.04304954304954305,
          0.04353054353054353,
          0.04377104377104377,
          0.04377104377104377,
          0.04401154401154401,
          0.04449254449254449,
          0.045935545935545934,
          0.045935545935545934,
          0.046657046657046654,
          0.046897546897546896,
          0.047619047619047616,
          0.047619047619047616,
          0.048340548340548344,
          0.048340548340548344,
          0.05146705146705147,
          0.05194805194805195,
          0.0545935545935546,
          0.0545935545935546,
          0.055074555074555075,
          0.05555555555555555,
          0.05555555555555555,
          0.05603655603655604,
          0.05627705627705628,
          0.05627705627705628,
          0.056517556517556515,
          0.056517556517556515,
          0.05868205868205868,
          0.05916305916305916,
          0.0594035594035594,
          0.0594035594035594,
          0.06132756132756133,
          0.06132756132756133,
          0.06253006253006253,
          0.06253006253006253,
          0.06373256373256374,
          0.06421356421356421,
          0.0658970658970659,
          0.06637806637806638,
          0.06734006734006734,
          0.06734006734006734,
          0.06758056758056757,
          0.06830206830206831,
          0.06974506974506975,
          0.06974506974506975,
          0.06998556998556998,
          0.07046657046657047,
          0.07118807118807119,
          0.07166907166907167,
          0.07190957190957191,
          0.0723905723905724,
          0.07431457431457432,
          0.07479557479557479,
          0.075998075998076,
          0.07647907647907648,
          0.07960557960557961,
          0.08008658008658008,
          0.08080808080808081,
          0.08080808080808081,
          0.08128908128908129,
          0.08177008177008177,
          0.0848965848965849,
          0.0848965848965849,
          0.08585858585858586,
          0.08682058682058683,
          0.0873015873015873,
          0.08802308802308802,
          0.0885040885040885,
          0.08994708994708994,
          0.08994708994708994,
          0.09571909571909572,
          0.0962000962000962,
          0.09836459836459836,
          0.09884559884559885,
          0.10004810004810005,
          0.10052910052910052,
          0.10076960076960077,
          0.10125060125060124,
          0.10245310245310245,
          0.10245310245310245,
          0.10365560365560365,
          0.10413660413660414,
          0.10726310726310727,
          0.10726310726310727,
          0.10822510822510822,
          0.10822510822510822,
          0.11135161135161135,
          0.11231361231361231,
          0.1127946127946128,
          0.1127946127946128,
          0.11471861471861472,
          0.11568061568061568,
          0.11856661856661857,
          0.11952861952861953,
          0.12025012025012025,
          0.12073112073112073,
          0.12121212121212122,
          0.12169312169312169,
          0.12193362193362194,
          0.12193362193362194,
          0.12217412217412217,
          0.12241462241462242,
          0.12265512265512266,
          0.12313612313612314,
          0.12361712361712361,
          0.12361712361712361,
          0.12457912457912458,
          0.12457912457912458,
          0.12506012506012507,
          0.12578162578162577,
          0.12674362674362674,
          0.1272246272246272,
          0.13275613275613277,
          0.13323713323713324,
          0.1337181337181337,
          0.1341991341991342,
          0.13468013468013468,
          0.13564213564213565,
          0.13660413660413662,
          0.1370851370851371,
          0.13804713804713806,
          0.13852813852813853,
          0.1426166426166426,
          0.14309764309764308,
          0.14357864357864358,
          0.14405964405964405,
          0.1467051467051467,
          0.14766714766714767,
          0.15367965367965367,
          0.15416065416065416,
          0.15993265993265993,
          0.1608946608946609,
          0.16137566137566137,
          0.16185666185666187,
          0.1632996632996633,
          0.1632996632996633,
          0.16378066378066378,
          0.16378066378066378,
          0.16642616642616642,
          0.16690716690716692,
          0.16835016835016836,
          0.16883116883116883,
          0.17556517556517556,
          0.17604617604617603,
          0.17652717652717653,
          0.177008177008177,
          0.177008177008177,
          0.17724867724867724,
          0.17772967772967774,
          0.18013468013468015,
          0.18013468013468015,
          0.18422318422318423,
          0.1847041847041847,
          0.18662818662818662,
          0.18710918710918711,
          0.18999518999519,
          0.19047619047619047,
          0.19288119288119288,
          0.19336219336219337,
          0.1964886964886965,
          0.19696969696969696,
          0.1976911976911977,
          0.19817219817219817,
          0.2012987012987013,
          0.2012987012987013,
          0.20827320827320828,
          0.20875420875420875,
          0.20947570947570948,
          0.20995670995670995,
          0.21236171236171236,
          0.21284271284271283,
          0.21476671476671477,
          0.21524771524771524,
          0.2162097162097162,
          0.21669071669071668,
          0.22077922077922077,
          0.22126022126022127,
          0.22510822510822512,
          0.2255892255892256,
          0.22823472823472823,
          0.2287157287157287,
          0.23544973544973544,
          0.23593073593073594,
          0.23665223665223664,
          0.23713323713323714,
          0.24074074074074073,
          0.24122174122174123,
          0.24362674362674364,
          0.2441077441077441,
          0.24506974506974508,
          0.24579124579124578,
          0.24795574795574796,
          0.24795574795574796,
          0.24867724867724866,
          0.24915824915824916,
          0.2566137566137566,
          0.2566137566137566,
          0.2607022607022607,
          0.2611832611832612,
          0.26262626262626265,
          0.2631072631072631,
          0.2664742664742665,
          0.26695526695526695,
          0.2674362674362674,
          0.26815776815776815,
          0.27224627224627224,
          0.2727272727272727,
          0.2756132756132756,
          0.2760942760942761,
          0.2775372775372775,
          0.278018278018278,
          0.28066378066378067,
          0.28114478114478114,
          0.28619528619528617,
          0.2866762866762867,
          0.2878787878787879,
          0.2881192881192881,
          0.2934102934102934,
          0.29389129389129387,
          0.2950937950937951,
          0.2955747955747956,
          0.2958152958152958,
          0.2962962962962963,
          0.30543530543530545,
          0.3059163059163059,
          0.30784030784030786,
          0.30832130832130833,
          0.30976430976430974,
          0.31024531024531027,
          0.31337181337181336,
          0.31385281385281383,
          0.3140933140933141,
          0.31457431457431456,
          0.31505531505531503,
          0.31553631553631556,
          0.3181818181818182,
          0.3181818181818182,
          0.3191438191438191,
          0.31962481962481964,
          0.32106782106782106,
          0.32154882154882153,
          0.32756132756132755,
          0.328042328042328,
          0.32852332852332855,
          0.329004329004329,
          0.3294853294853295,
          0.32996632996632996,
          0.33261183261183264,
          0.33261183261183264,
          0.3436748436748437,
          0.34415584415584416,
          0.34463684463684463,
          0.3451178451178451,
          0.3516113516113516,
          0.3516113516113516,
          0.35714285714285715,
          0.3576238576238576,
          0.367003367003367,
          0.36748436748436747,
          0.3698893698893699,
          0.37037037037037035,
          0.3718133718133718,
          0.3722943722943723,
          0.37325637325637323,
          0.37373737373737376,
          0.37782587782587784,
          0.3783068783068783,
          0.37999037999038,
          0.38047138047138046,
          0.38167388167388167,
          0.38215488215488214,
          0.38263588263588266,
          0.38263588263588266,
          0.38311688311688313,
          0.38335738335738334,
          0.38672438672438675,
          0.3872053872053872,
          0.3917748917748918,
          0.3917748917748918,
          0.39874939874939874,
          0.3992303992303992,
          0.4009139009139009,
          0.4009139009139009,
          0.4045214045214045,
          0.405002405002405,
          0.40764790764790765,
          0.4081289081289081,
          0.411976911976912,
          0.41245791245791247,
          0.4139009139009139,
          0.4143819143819144,
          0.4155844155844156,
          0.4160654160654161,
          0.41798941798941797,
          0.4184704184704185,
          0.4215969215969216,
          0.42207792207792205,
          0.4302549302549303,
          0.43073593073593075,
          0.4353054353054353,
          0.4357864357864358,
          0.4367484367484368,
          0.4377104377104377,
          0.4389129389129389,
          0.4393939393939394,
          0.4432419432419432,
          0.44372294372294374,
          0.4494949494949495,
          0.4497354497354497,
          0.4518999518999519,
          0.4523809523809524,
          0.4535834535834536,
          0.45406445406445406,
          0.45695045695045694,
          0.4574314574314574,
          0.4665704665704666,
          0.46705146705146705,
          0.47186147186147187,
          0.4725829725829726,
          0.4737854737854738,
          0.4742664742664743,
          0.47883597883597884,
          0.4793169793169793,
          0.47955747955747957,
          0.48003848003848004,
          0.4805194805194805,
          0.481000481000481,
          0.48148148148148145,
          0.481962481962482,
          0.4829244829244829,
          0.4834054834054834,
          0.4874939874939875,
          0.48797498797498795,
          0.49206349206349204,
          0.49206349206349204,
          0.493987493987494,
          0.49446849446849445,
          0.4947089947089947,
          0.4951899951899952,
          0.49567099567099565,
          0.4961519961519962,
          0.4963924963924964,
          0.49687349687349686,
          0.49855699855699853,
          0.49927849927849927,
          0.5002405002405003,
          0.5007215007215007,
          0.5050505050505051,
          0.5055315055315055,
          0.5072150072150072,
          0.5076960076960076,
          0.5084175084175084,
          0.5093795093795094,
          0.5144300144300145,
          0.5149110149110149,
          0.5182780182780182,
          0.5182780182780182,
          0.5218855218855218,
          0.5223665223665224,
          0.5226070226070226,
          0.523088023088023,
          0.5305435305435305,
          0.5310245310245311,
          0.5324675324675324,
          0.532948532948533,
          0.5358345358345359,
          0.5363155363155363,
          0.5413660413660414,
          0.5418470418470418,
          0.5454545454545454,
          0.5459355459355459,
          0.5476190476190477,
          0.5481000481000481,
          0.5507455507455508,
          0.5512265512265512,
          0.5514670514670514,
          0.551948051948052,
          0.5543530543530544,
          0.5548340548340548,
          0.5582010582010583,
          0.5589225589225589,
          0.5668590668590668,
          0.5673400673400674,
          0.5678210678210678,
          0.5683020683020683,
          0.5692640692640693,
          0.5697450697450698,
          0.5704665704665705,
          0.570947570947571,
          0.5738335738335738,
          0.5743145743145743,
          0.5800865800865801,
          0.5805675805675806,
          0.5827320827320828,
          0.5832130832130832,
          0.5853775853775853,
          0.5858585858585859,
          0.5901875901875901,
          0.5906685906685907,
          0.5913900913900914,
          0.5918710918710919,
          0.5983645983645983,
          0.5988455988455988,
          0.6019721019721019,
          0.6024531024531025,
          0.6050986050986051,
          0.6060606060606061,
          0.6067821067821068,
          0.6072631072631073,
          0.6075036075036075,
          0.607984607984608,
          0.6183261183261183,
          0.6183261183261183,
          0.6204906204906205,
          0.620971620971621,
          0.6212121212121212,
          0.6212121212121212,
          0.6257816257816258,
          0.6262626262626263,
          0.6301106301106301,
          0.6305916305916306,
          0.6363636363636364,
          0.6368446368446369,
          0.6373256373256373,
          0.6373256373256373,
          0.6378066378066378,
          0.6382876382876382,
          0.6414141414141414,
          0.6418951418951419,
          0.6426166426166426,
          0.6426166426166426,
          0.6430976430976431,
          0.6443001443001443,
          0.6447811447811448,
          0.645021645021645,
          0.6455026455026455,
          0.6469456469456469,
          0.6474266474266475,
          0.6483886483886484,
          0.6488696488696488,
          0.6493506493506493,
          0.6498316498316499,
          0.6558441558441559,
          0.6563251563251563,
          0.6565656565656566,
          0.6570466570466571,
          0.6575276575276575,
          0.658008658008658,
          0.6611351611351611,
          0.6616161616161617,
          0.6717171717171717,
          0.6721981721981722,
          0.6736411736411736,
          0.6741221741221741,
          0.6750841750841751,
          0.6755651755651756,
          0.6782106782106783,
          0.6786916786916787,
          0.6808561808561808,
          0.6813371813371814,
          0.6907166907166907,
          0.6911976911976911,
          0.6952861952861953,
          0.6957671957671958,
          0.6962481962481962,
          0.696969696969697,
          0.6998556998556998,
          0.7003367003367004,
          0.702020202020202,
          0.702020202020202,
          0.7022607022607023,
          0.7027417027417028,
          0.708032708032708,
          0.7085137085137085,
          0.7101972101972102,
          0.7106782106782107,
          0.7198172198172198,
          0.7202982202982203,
          0.7241462241462241,
          0.7246272246272246,
          0.7267917267917268,
          0.7277537277537277,
          0.7323232323232324,
          0.7328042328042328,
          0.7349687349687349,
          0.7349687349687349,
          0.7354497354497355,
          0.7397787397787398,
          0.7402597402597403,
          0.7436267436267436,
          0.7441077441077442,
          0.7443482443482443,
          0.7448292448292448,
          0.7450697450697451,
          0.7455507455507455,
          0.7489177489177489,
          0.7493987493987494,
          0.7527657527657527,
          0.7532467532467533,
          0.7546897546897547,
          0.7551707551707552,
          0.7558922558922558,
          0.7563732563732564,
          0.757094757094757,
          0.7575757575757576,
          0.7609427609427609,
          0.7614237614237614,
          0.7619047619047619,
          0.7623857623857624,
          0.7640692640692641,
          0.7640692640692641,
          0.7671957671957672,
          0.7676767676767676,
          0.77008177008177,
          0.7705627705627706,
          0.7717652717652718,
          0.7722462722462723,
          0.7784992784992785,
          0.778980278980279,
          0.7792207792207793,
          0.7797017797017797,
          0.7818662818662818,
          0.7818662818662818,
          0.7840307840307841,
          0.7845117845117845,
          0.7852332852332853,
          0.7857142857142857,
          0.7931697931697932,
          0.7936507936507936,
          0.7991822991822992,
          0.7996632996632996,
          0.8015873015873016,
          0.803030303030303,
          0.8032708032708032,
          0.8037518037518038,
          0.8054353054353054,
          0.8063973063973064,
          0.815055315055315,
          0.8155363155363156,
          0.8172198172198172,
          0.8177008177008177,
          0.8229918229918229,
          0.8234728234728235,
          0.8270803270803271,
          0.8275613275613276,
          0.835016835016835,
          0.8354978354978355,
          0.8405483405483406,
          0.8415103415103415,
          0.8530543530543531,
          0.8535353535353535,
          0.8554593554593555,
          0.8559403559403559,
          0.8564213564213564,
          0.8569023569023569,
          0.8576238576238576,
          0.8581048581048581,
          0.8679653679653679,
          0.8684463684463685,
          0.8686868686868687,
          0.8691678691678691,
          0.8715728715728716,
          0.8720538720538721,
          0.8737373737373737,
          0.8756613756613757,
          0.8763828763828764,
          0.8768638768638769,
          0.8778258778258778,
          0.8783068783068783,
          0.8816738816738817,
          0.8821548821548821,
          0.8835978835978836,
          0.8840788840788841,
          0.8852813852813853,
          0.8857623857623858,
          0.8946608946608947,
          0.8951418951418951,
          0.9054834054834054,
          0.905964405964406,
          0.9122174122174123,
          0.9122174122174123,
          0.9129389129389129,
          0.9134199134199135,
          0.9177489177489178,
          0.9182299182299183,
          0.9199134199134199,
          0.9203944203944204,
          0.9208754208754208,
          0.9213564213564214,
          0.9225589225589226,
          0.923039923039923,
          0.9242424242424242,
          0.9247234247234247,
          0.9256854256854257,
          0.9261664261664262,
          0.9278499278499278,
          0.9283309283309283,
          0.929052429052429,
          0.9295334295334295,
          0.9331409331409332,
          0.9336219336219336,
          0.9492544492544492,
          0.9497354497354498,
          0.9531024531024531,
          0.9535834535834535,
          0.9557479557479558,
          0.9562289562289562,
          0.962000962000962,
          0.9624819624819625,
          0.9634439634439634,
          0.963924963924964,
          0.9658489658489658,
          0.9663299663299664,
          0.9665704665704665,
          0.967051467051467,
          0.9706589706589707,
          0.9711399711399712,
          0.9718614718614719,
          0.9728234728234728,
          0.9735449735449735,
          0.974025974025974,
          0.9757094757094757,
          0.9761904761904762,
          0.9781144781144782,
          0.9790764790764791,
          0.9836459836459837,
          0.9841269841269841,
          0.9855699855699855,
          0.9860509860509861,
          0.9862914862914863,
          0.9867724867724867,
          0.9882154882154882,
          0.9886964886964887,
          0.9891774891774892,
          0.9896584896584897,
          0.993025493025493,
          0.9935064935064936,
          1
         ]
        },
        {
         "mode": "lines",
         "name": "test",
         "type": "scatter",
         "x": [
          0,
          0,
          0.19047619047619047,
          0.19047619047619047,
          0.21428571428571427,
          0.21428571428571427,
          0.2619047619047619,
          0.2619047619047619,
          0.2857142857142857,
          0.2857142857142857,
          0.30952380952380953,
          0.30952380952380953,
          0.35714285714285715,
          0.35714285714285715,
          0.38095238095238093,
          0.38095238095238093,
          0.40476190476190477,
          0.40476190476190477,
          0.47619047619047616,
          0.47619047619047616,
          0.47619047619047616,
          0.5,
          0.5,
          0.5238095238095238,
          0.5238095238095238,
          0.5476190476190477,
          0.5476190476190477,
          0.5952380952380952,
          0.5952380952380952,
          0.5952380952380952,
          0.6190476190476191,
          0.6190476190476191,
          0.6428571428571429,
          0.6428571428571429,
          0.6904761904761905,
          0.6904761904761905,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7857142857142857,
          0.7857142857142857,
          0.7857142857142857,
          0.7857142857142857,
          0.8095238095238095,
          0.8095238095238095,
          0.8095238095238095,
          0.8095238095238095,
          0.8333333333333334,
          0.8333333333333334,
          0.8571428571428571,
          0.8571428571428571,
          0.8809523809523809,
          0.8809523809523809,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9285714285714286,
          0.9285714285714286,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9761904761904762,
          0.9761904761904762,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "y": [
          0,
          0.0009615384615384616,
          0.0009615384615384616,
          0.0028846153846153848,
          0.0028846153846153848,
          0.0057692307692307696,
          0.0057692307692307696,
          0.006730769230769231,
          0.006730769230769231,
          0.008653846153846154,
          0.008653846153846154,
          0.010576923076923078,
          0.010576923076923078,
          0.014423076923076924,
          0.014423076923076924,
          0.01826923076923077,
          0.01826923076923077,
          0.019230769230769232,
          0.019230769230769232,
          0.020192307692307693,
          0.022115384615384617,
          0.022115384615384617,
          0.03653846153846154,
          0.03653846153846154,
          0.04903846153846154,
          0.04903846153846154,
          0.058653846153846154,
          0.058653846153846154,
          0.0625,
          0.06538461538461539,
          0.06538461538461539,
          0.08557692307692308,
          0.08557692307692308,
          0.1,
          0.1,
          0.10480769230769231,
          0.10480769230769231,
          0.1375,
          0.13942307692307693,
          0.14038461538461539,
          0.1423076923076923,
          0.15865384615384615,
          0.1605769230769231,
          0.18653846153846154,
          0.18846153846153846,
          0.19230769230769232,
          0.19423076923076923,
          0.25384615384615383,
          0.25384615384615383,
          0.2692307692307692,
          0.27115384615384613,
          0.3125,
          0.3144230769230769,
          0.3269230769230769,
          0.32884615384615384,
          0.35096153846153844,
          0.3528846153846154,
          0.38269230769230766,
          0.38461538461538464,
          0.44326923076923075,
          0.44326923076923075,
          0.46057692307692305,
          0.4625,
          0.46634615384615385,
          0.46634615384615385,
          0.4826923076923077,
          0.4846153846153846,
          0.5057692307692307,
          0.5067307692307692,
          0.5269230769230769,
          0.5269230769230769,
          0.5528846153846154,
          0.5528846153846154,
          0.5596153846153846,
          0.5596153846153846,
          0.5701923076923077,
          0.5721153846153846,
          0.5865384615384616,
          0.5865384615384616,
          0.6134615384615385,
          0.6134615384615385,
          0.6211538461538462,
          0.6230769230769231,
          0.6259615384615385,
          0.6278846153846154,
          0.6490384615384616,
          0.6509615384615385,
          0.8423076923076923,
          0.8442307692307692,
          0.8509615384615384,
          0.8528846153846154,
          0.8711538461538462,
          0.8730769230769231,
          0.8769230769230769,
          0.8769230769230769,
          0.9346153846153846,
          0.9346153846153846,
          0.9403846153846154,
          0.9423076923076923,
          0.989423076923077,
          0.9913461538461539,
          1
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "ROC curve",
         "x": 0.5
        },
        "xaxis": {
         "title": {
          "text": "False Positive Rate"
         }
        },
        "yaxis": {
         "title": {
          "text": "True Positive Rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize ROC curves\n",
    "probas_train = classifier.predict_proba(X_train)[:,1]\n",
    "precisions, recalls, thresholds = roc_curve(Y_train, probas_train)\n",
    "fig = go.Figure(\n",
    "    data = go.Scatter(\n",
    "        name = 'train',\n",
    "        x = recalls, \n",
    "        y = precisions, \n",
    "        mode = 'lines'\n",
    "    ),\n",
    "    layout = go.Layout(\n",
    "        title = go.layout.Title(text = \"ROC curve\", x = 0.5),\n",
    "        xaxis = go.layout.XAxis(title = 'False Positive Rate'),\n",
    "        yaxis = go.layout.YAxis(title = 'True Positive Rate')\n",
    "    )\n",
    ")\n",
    "\n",
    "probas_test = classifier.predict_proba(X_test)[:,1]\n",
    "precisions, recalls, thresholds = roc_curve(Y_test, probas_test)\n",
    "fig.add_trace(go.Scatter(\n",
    "    name = 'test',\n",
    "    x = recalls, \n",
    "    y = precisions, \n",
    "    mode = 'lines'\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
