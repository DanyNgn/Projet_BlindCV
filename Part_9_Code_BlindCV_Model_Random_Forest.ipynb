{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\python39\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python39\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python39\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python39\\lib\\site-packages (from matplotlib) (4.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python39\\lib\\site-packages (from matplotlib) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python39\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip \n",
    "pip.main([\"install\",\"matplotlib\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in c:\\python39\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.3 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.3.2 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\python39\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\python39\\lib\\site-packages (from xgboost) (1.23.1)\n",
      "Requirement already satisfied: scipy in c:\\python39\\lib\\site-packages (from xgboost) (1.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des librairies nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # to avoid deprecation warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la dataset\n",
    "## Choix de 4 datasets, ne pas oublier de choisir la features_list adéquate\n",
    "\n",
    "#dataset = pd.read_csv(\"dataset_CV_labelise_features1_100.csv\", delimiter = \";\", encoding = \"utf-8\") ## 100CV + 4 features numériques\n",
    "\n",
    "#dataset = pd.read_csv(\"dataset_CV_labelise_features1_200.csv\", delimiter = \";\", encoding = \"utf-8\") ## 200CV + 4 features numériques\n",
    "\n",
    "#dataset = pd.read_csv(\"dataset_CV_labelise_features2_100.csv\", delimiter = \";\", encoding = \"utf-8\") ## 100CV + 12 features numériques\n",
    "\n",
    "dataset = pd.read_csv(\"dataset_CV_labelise_features2_200.csv\", delimiter = \";\", encoding = \"utf-8\") ## 200CV + 12 features numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5594, 19)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_Sentences</th>\n",
       "      <th>Sentences_CV_clean</th>\n",
       "      <th>CV_Number</th>\n",
       "      <th>Sentence_line</th>\n",
       "      <th>Nb_tokens</th>\n",
       "      <th>%texte_lu</th>\n",
       "      <th>%texte_lu_fin_ligne</th>\n",
       "      <th>Is_alpha</th>\n",
       "      <th>Grammar</th>\n",
       "      <th>Tokenization</th>\n",
       "      <th>Verb_count</th>\n",
       "      <th>Propn_count</th>\n",
       "      <th>Noun_count</th>\n",
       "      <th>Num_count</th>\n",
       "      <th>Pourcentage_verb_sentence</th>\n",
       "      <th>Pourcentage_propn_sentence</th>\n",
       "      <th>Pourcentage_noun_sentence</th>\n",
       "      <th>Pourcentage_num_sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5594</td>\n",
       "      <td>5592</td>\n",
       "      <td>5594</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594</td>\n",
       "      <td>5594</td>\n",
       "      <td>5592</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.00000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3805</td>\n",
       "      <td>3802</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1626</td>\n",
       "      <td>2748</td>\n",
       "      <td>3802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CV_10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>638</td>\n",
       "      <td>334</td>\n",
       "      <td>198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.673221</td>\n",
       "      <td>9.132285</td>\n",
       "      <td>50.470518</td>\n",
       "      <td>49.529483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.297283</td>\n",
       "      <td>0.611012</td>\n",
       "      <td>2.83822</td>\n",
       "      <td>0.320164</td>\n",
       "      <td>0.020117</td>\n",
       "      <td>0.105039</td>\n",
       "      <td>0.314979</td>\n",
       "      <td>0.042982</td>\n",
       "      <td>0.037898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.268076</td>\n",
       "      <td>10.914849</td>\n",
       "      <td>30.190027</td>\n",
       "      <td>30.190025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.784780</td>\n",
       "      <td>1.129198</td>\n",
       "      <td>3.56600</td>\n",
       "      <td>0.693458</td>\n",
       "      <td>0.059959</td>\n",
       "      <td>0.227969</td>\n",
       "      <td>0.251108</td>\n",
       "      <td>0.131906</td>\n",
       "      <td>0.190966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>24.190000</td>\n",
       "      <td>23.552500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>49.530000</td>\n",
       "      <td>50.470000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>76.447500</td>\n",
       "      <td>75.810000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.760000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>39.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CV_Sentences Sentences_CV_clean CV_Number  Sentence_line    Nb_tokens  \\\n",
       "count          5594               5592      5594    5594.000000  5594.000000   \n",
       "unique         3805               3802       200            NaN          NaN   \n",
       "top                                        CV_10            NaN          NaN   \n",
       "freq            198                198        66            NaN          NaN   \n",
       "mean            NaN                NaN       NaN      15.673221     9.132285   \n",
       "std             NaN                NaN       NaN      11.268076    10.914849   \n",
       "min             NaN                NaN       NaN       0.000000     1.000000   \n",
       "25%             NaN                NaN       NaN       7.000000     2.000000   \n",
       "50%             NaN                NaN       NaN      14.000000     5.000000   \n",
       "75%             NaN                NaN       NaN      23.000000    12.000000   \n",
       "max             NaN                NaN       NaN      65.000000   124.000000   \n",
       "\n",
       "          %texte_lu  %texte_lu_fin_ligne Is_alpha   Grammar Tokenization  \\\n",
       "count   5594.000000          5594.000000     5594      5594         5592   \n",
       "unique          NaN                  NaN     1626      2748         3802   \n",
       "top             NaN                  NaN  [False]  ['NOUN']                \n",
       "freq            NaN                  NaN      638       334          198   \n",
       "mean      50.470518            49.529483      NaN       NaN          NaN   \n",
       "std       30.190027            30.190025      NaN       NaN          NaN   \n",
       "min        0.240000             0.000000      NaN       NaN          NaN   \n",
       "25%       24.190000            23.552500      NaN       NaN          NaN   \n",
       "50%       49.530000            50.470000      NaN       NaN          NaN   \n",
       "75%       76.447500            75.810000      NaN       NaN          NaN   \n",
       "max      100.000000            99.760000      NaN       NaN          NaN   \n",
       "\n",
       "         Verb_count  Propn_count  Noun_count    Num_count  \\\n",
       "count   5594.000000  5594.000000  5594.00000  5594.000000   \n",
       "unique          NaN          NaN         NaN          NaN   \n",
       "top             NaN          NaN         NaN          NaN   \n",
       "freq            NaN          NaN         NaN          NaN   \n",
       "mean       0.297283     0.611012     2.83822     0.320164   \n",
       "std        0.784780     1.129198     3.56600     0.693458   \n",
       "min        0.000000     0.000000     0.00000     0.000000   \n",
       "25%        0.000000     0.000000     1.00000     0.000000   \n",
       "50%        0.000000     0.000000     2.00000     0.000000   \n",
       "75%        0.000000     1.000000     4.00000     0.000000   \n",
       "max        8.000000    14.000000    39.00000     8.000000   \n",
       "\n",
       "        Pourcentage_verb_sentence  Pourcentage_propn_sentence  \\\n",
       "count                 5594.000000                 5594.000000   \n",
       "unique                        NaN                         NaN   \n",
       "top                           NaN                         NaN   \n",
       "freq                          NaN                         NaN   \n",
       "mean                     0.020117                    0.105039   \n",
       "std                      0.059959                    0.227969   \n",
       "min                      0.000000                    0.000000   \n",
       "25%                      0.000000                    0.000000   \n",
       "50%                      0.000000                    0.000000   \n",
       "75%                      0.000000                    0.102564   \n",
       "max                      1.000000                    1.000000   \n",
       "\n",
       "        Pourcentage_noun_sentence  Pourcentage_num_sentence        Label  \n",
       "count                 5594.000000               5594.000000  5594.000000  \n",
       "unique                        NaN                       NaN          NaN  \n",
       "top                           NaN                       NaN          NaN  \n",
       "freq                          NaN                       NaN          NaN  \n",
       "mean                     0.314979                  0.042982     0.037898  \n",
       "std                      0.251108                  0.131906     0.190966  \n",
       "min                      0.000000                  0.000000     0.000000  \n",
       "25%                      0.157895                  0.000000     0.000000  \n",
       "50%                      0.307692                  0.000000     0.000000  \n",
       "75%                      0.416667                  0.000000     0.000000  \n",
       "max                      1.000000                  1.000000     1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_desc = dataset.describe(include='all')\n",
    "display(data_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV_Sentences                  0.000000\n",
       "Sentences_CV_clean            0.035753\n",
       "CV_Number                     0.000000\n",
       "Sentence_line                 0.000000\n",
       "Nb_tokens                     0.000000\n",
       "%texte_lu                     0.000000\n",
       "%texte_lu_fin_ligne           0.000000\n",
       "Is_alpha                      0.000000\n",
       "Grammar                       0.000000\n",
       "Tokenization                  0.035753\n",
       "Verb_count                    0.000000\n",
       "Propn_count                   0.000000\n",
       "Noun_count                    0.000000\n",
       "Num_count                     0.000000\n",
       "Pourcentage_verb_sentence     0.000000\n",
       "Pourcentage_propn_sentence    0.000000\n",
       "Pourcentage_noun_sentence     0.000000\n",
       "Pourcentage_num_sentence      0.000000\n",
       "Label                         0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(100*dataset.isnull().sum()/dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5416, 19)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Label\"].unique()\n",
    "dataset = dataset.loc[(dataset['CV_Sentences'] != \"#NOM?\"),:]\n",
    "dataset.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5412, 19)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.loc[(dataset['CV_Sentences'] != \":\"),:]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5410, 19)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna(axis =0, how = 'any')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV_Sentences                  0.0\n",
       "Sentences_CV_clean            0.0\n",
       "CV_Number                     0.0\n",
       "Sentence_line                 0.0\n",
       "Nb_tokens                     0.0\n",
       "%texte_lu                     0.0\n",
       "%texte_lu_fin_ligne           0.0\n",
       "Is_alpha                      0.0\n",
       "Grammar                       0.0\n",
       "Tokenization                  0.0\n",
       "Verb_count                    0.0\n",
       "Propn_count                   0.0\n",
       "Noun_count                    0.0\n",
       "Num_count                     0.0\n",
       "Pourcentage_verb_sentence     0.0\n",
       "Pourcentage_propn_sentence    0.0\n",
       "Pourcentage_noun_sentence     0.0\n",
       "Pourcentage_num_sentence      0.0\n",
       "Label                         0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(100*dataset.isnull().sum()/dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating labels from features...\n",
      "y : \n",
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "X :\n",
      "   Sentence_line  Nb_tokens  %texte_lu  %texte_lu_fin_ligne  Verb_count  \\\n",
      "0              0          6       2.80                97.20           1   \n",
      "1              1          1       3.27                96.73           0   \n",
      "2              2          8       7.01                92.99           0   \n",
      "3              3         24      18.22                81.78           5   \n",
      "4              4          1      18.69                81.31           0   \n",
      "\n",
      "   Propn_count  Noun_count  Num_count  Pourcentage_verb_sentence  \\\n",
      "0            3           1          1                   0.166667   \n",
      "1            0           1          0                   0.000000   \n",
      "2            1           2          0                   0.000000   \n",
      "3            1           5          1                   0.208333   \n",
      "4            0           1          0                   0.000000   \n",
      "\n",
      "   Pourcentage_propn_sentence  Pourcentage_noun_sentence  \\\n",
      "0                    0.500000                   0.166667   \n",
      "1                    0.000000                   1.000000   \n",
      "2                    0.125000                   0.250000   \n",
      "3                    0.041667                   0.208333   \n",
      "4                    0.000000                   1.000000   \n",
      "\n",
      "   Pourcentage_num_sentence  \n",
      "0                  0.166667  \n",
      "1                  0.000000  \n",
      "2                  0.000000  \n",
      "3                  0.041667  \n",
      "4                  0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"Separating labels from features...\")\n",
    "\n",
    "## Choisir la features_list par rapport au dataset\n",
    "#features_list = [\"CV_Sentences\",\"Sentences_CV_clean\",\"CV_Number\", \"Is_alpha\", \"Grammar\", \"Label\"] ## Pour les datasets features1\n",
    "features_list = [\"CV_Sentences\",\"Sentences_CV_clean\",\"CV_Number\", \"Is_alpha\", \"Label\", \"Grammar\", \"Tokenization\"] ## Pour les datasets features2\n",
    "target_variable = \"Label\"\n",
    "\n",
    "X = dataset.drop(features_list, axis = 1)\n",
    "y = dataset.loc[:,target_variable]\n",
    "\n",
    "print('y : ')\n",
    "print(y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numeric features  ['Sentence_line', 'Nb_tokens', '%texte_lu', '%texte_lu_fin_ligne', 'Verb_count', 'Propn_count', 'Noun_count', 'Num_count', 'Pourcentage_verb_sentence', 'Pourcentage_propn_sentence', 'Pourcentage_noun_sentence', 'Pourcentage_num_sentence']\n",
      "Found categorical features  []\n"
     ]
    }
   ],
   "source": [
    "# Automatically detect names of numeric/categorical columns\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "for i,t in X.dtypes.iteritems():\n",
    "    if ('float' in str(t)) or ('int' in str(t)) :\n",
    "        numeric_features.append(i)\n",
    "    else :\n",
    "        categorical_features.append(i)\n",
    "\n",
    "print('Found numeric features ', numeric_features)\n",
    "print('Found categorical features ', categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for numeric features\n",
    "numeric_transformer = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for categorical features\n",
    "categorical_transformer = OneHotEncoder(drop='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "...Done.\n",
      "Best hyperparameters :  {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100}\n",
      "Best validation accuracy :  0.9738907263179107\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'max_depth': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'n_estimators': [10, 20, 40, 60, 80, 100]\n",
    "}\n",
    "gridsearch = GridSearchCV(classifier, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on training set...\n",
      "...Done.\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on training set\n",
    "print(\"Predictions on training set...\")\n",
    "Y_train_pred = gridsearch.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print(Y_train_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set...\n",
      "...Done.\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on test set\n",
    "print(\"Predictions on test set...\")\n",
    "Y_test_pred = gridsearch.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print(Y_test_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set :  0.9847504621072088\n",
      "accuracy on test set :  0.9713493530499075\n",
      "\n",
      "f1-score on training set :  0.7692307692307692\n",
      "f1-score on test set :  0.523076923076923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print scores\n",
    "print(\"accuracy on training set : \", accuracy_score(y_train, Y_train_pred))\n",
    "print(\"accuracy on test set : \", accuracy_score(y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "print(\"f1-score on training set : \", f1_score(y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(y_test, Y_test_pred))                              \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(247,252,240)"
          ],
          [
           0.125,
           "rgb(224,243,219)"
          ],
          [
           0.25,
           "rgb(204,235,197)"
          ],
          [
           0.375,
           "rgb(168,221,181)"
          ],
          [
           0.5,
           "rgb(123,204,196)"
          ],
          [
           0.625,
           "rgb(78,179,211)"
          ],
          [
           0.75,
           "rgb(43,140,190)"
          ],
          [
           0.875,
           "rgb(8,104,172)"
          ],
          [
           1,
           "rgb(8,64,129)"
          ]
         ],
         "name": "train",
         "type": "heatmap",
         "x": [
          "0",
          "1"
         ],
         "xaxis": "x",
         "y": [
          "0",
          "1"
         ],
         "yaxis": "y",
         "z": [
          [
           4152,
           6
          ],
          [
           60,
           110
          ]
         ],
         "zmax": 4152,
         "zmin": 0
        },
        {
         "colorscale": [
          [
           0,
           "rgb(247,252,240)"
          ],
          [
           0.125,
           "rgb(224,243,219)"
          ],
          [
           0.25,
           "rgb(204,235,197)"
          ],
          [
           0.375,
           "rgb(168,221,181)"
          ],
          [
           0.5,
           "rgb(123,204,196)"
          ],
          [
           0.625,
           "rgb(78,179,211)"
          ],
          [
           0.75,
           "rgb(43,140,190)"
          ],
          [
           0.875,
           "rgb(8,104,172)"
          ],
          [
           1,
           "rgb(8,64,129)"
          ]
         ],
         "name": "test",
         "type": "heatmap",
         "x": [
          "0",
          "1"
         ],
         "xaxis": "x2",
         "y": [
          "0",
          "1"
         ],
         "yaxis": "y2",
         "z": [
          [
           1034,
           6
          ],
          [
           25,
           17
          ]
         ],
         "zmax": 4152,
         "zmin": 0
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "train",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "test",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Prediction",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0,
          "yanchor": "top",
          "yref": "paper",
          "yshift": -30
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "True label",
          "textangle": -90,
          "x": 0,
          "xanchor": "right",
          "xref": "paper",
          "xshift": -40,
          "y": 0.5,
          "yanchor": "middle",
          "yref": "paper"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Confusion matrices",
         "x": 0.5
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize confusion matrices\n",
    "from plotly.subplots import make_subplots\n",
    "cm_train = confusion_matrix(y_train, Y_train_pred)\n",
    "cm_test = confusion_matrix(y_test, Y_test_pred)\n",
    "\n",
    "fig = make_subplots(rows = 1, cols = 2, subplot_titles = (\"train\", \"test\"), \n",
    "                    x_title = 'Prediction', y_title = 'True label')\n",
    "fig.update_layout(\n",
    "        title = go.layout.Title(text = \"Confusion matrices\", x = 0.5))\n",
    "fig.update_yaxes(autorange='reversed')\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        name = 'train',\n",
    "        x = ['0', '1'], \n",
    "        y = ['0', '1'], \n",
    "        z = cm_train,\n",
    "        colorscale = 'gnbu',\n",
    "        zmin = 0,\n",
    "        zmax = max(cm_train.max(), cm_test.max())\n",
    "    ),\n",
    "    row = 1,\n",
    "    col = 1\n",
    ")  \n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        name = 'test',\n",
    "        x = ['0', '1'], \n",
    "        y = ['0', '1'], \n",
    "        z = cm_test,\n",
    "        colorscale = 'gnbu',\n",
    "        zmin = 0,\n",
    "        zmax = max(cm_train.max(), cm_test.max())\n",
    "    ),\n",
    "    row = 1,\n",
    "    col = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "train",
         "type": "scatter",
         "x": [
          0,
          0.023529411764705882,
          0.058823529411764705,
          0.08235294117647059,
          0.08823529411764706,
          0.10588235294117647,
          0.18823529411764706,
          0.2,
          0.2411764705882353,
          0.2529411764705882,
          0.2823529411764706,
          0.29411764705882354,
          0.3176470588235294,
          0.3411764705882353,
          0.3764705882352941,
          0.38823529411764707,
          0.4764705882352941,
          0.48823529411764705,
          0.5235294117647059,
          0.5352941176470588,
          0.5470588235294118,
          0.5470588235294118,
          0.611764705882353,
          0.611764705882353,
          0.6352941176470588,
          0.6411764705882353,
          0.6411764705882353,
          0.6470588235294118,
          0.6470588235294118,
          0.6764705882352942,
          0.6764705882352942,
          0.6823529411764706,
          0.6882352941176471,
          0.7,
          0.7,
          0.7058823529411765,
          0.7058823529411765,
          0.711764705882353,
          0.711764705882353,
          0.7176470588235294,
          0.7176470588235294,
          0.7294117647058823,
          0.7294117647058823,
          0.7411764705882353,
          0.7411764705882353,
          0.7647058823529411,
          0.7647058823529411,
          0.7823529411764706,
          0.7823529411764706,
          0.788235294117647,
          0.788235294117647,
          0.8058823529411765,
          0.8058823529411765,
          0.8176470588235294,
          0.8176470588235294,
          0.8352941176470589,
          0.8352941176470589,
          0.8647058823529412,
          0.8705882352941177,
          0.8764705882352941,
          0.8764705882352941,
          0.888235294117647,
          0.888235294117647,
          0.9,
          0.9058823529411765,
          0.9058823529411765,
          0.9058823529411765,
          0.9117647058823529,
          0.9117647058823529,
          0.9176470588235294,
          0.9235294117647059,
          0.9235294117647059,
          0.9294117647058824,
          0.9294117647058824,
          0.9411764705882353,
          0.9411764705882353,
          0.9470588235294117,
          0.9470588235294117,
          0.9529411764705882,
          0.9529411764705882,
          0.9529411764705882,
          0.9588235294117647,
          0.9588235294117647,
          0.9705882352941176,
          0.9764705882352941,
          0.9764705882352941,
          0.9823529411764705,
          0.9882352941176471,
          0.9882352941176471,
          0.9941176470588236,
          0.9941176470588236,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0002405002405002405,
          0.0002405002405002405,
          0.000481000481000481,
          0.000481000481000481,
          0.0007215007215007215,
          0.000962000962000962,
          0.000962000962000962,
          0.001443001443001443,
          0.001443001443001443,
          0.0016835016835016834,
          0.0016835016835016834,
          0.001924001924001924,
          0.001924001924001924,
          0.002405002405002405,
          0.002405002405002405,
          0.0026455026455026454,
          0.0026455026455026454,
          0.002886002886002886,
          0.002886002886002886,
          0.0036075036075036075,
          0.0036075036075036075,
          0.003848003848003848,
          0.003848003848003848,
          0.004088504088504088,
          0.004088504088504088,
          0.00456950456950457,
          0.00456950456950457,
          0.00481000481000481,
          0.00481000481000481,
          0.005050505050505051,
          0.005050505050505051,
          0.005291005291005291,
          0.005291005291005291,
          0.005531505531505531,
          0.005531505531505531,
          0.005772005772005772,
          0.005772005772005772,
          0.006012506012506013,
          0.006012506012506013,
          0.006253006253006253,
          0.006253006253006253,
          0.006974506974506974,
          0.006974506974506974,
          0.007215007215007215,
          0.007936507936507936,
          0.008177008177008177,
          0.008177008177008177,
          0.008417508417508417,
          0.008417508417508417,
          0.008658008658008658,
          0.00913900913900914,
          0.00913900913900914,
          0.00962000962000962,
          0.00962000962000962,
          0.009860509860509861,
          0.009860509860509861,
          0.010101010101010102,
          0.010101010101010102,
          0.011784511784511785,
          0.012265512265512266,
          0.012265512265512266,
          0.013949013949013949,
          0.013949013949013949,
          0.01418951418951419,
          0.017075517075517077,
          0.017075517075517077,
          0.017316017316017316,
          0.021404521404521405,
          0.021404521404521405,
          0.022607022607022607,
          0.022607022607022607,
          0.03006253006253006,
          0.030543530543530543,
          0.03150553150553151,
          0.03198653198653199,
          0.03439153439153439,
          0.034872534872534874,
          0.03535353535353535,
          0.03583453583453584,
          0.037037037037037035,
          0.03751803751803752,
          0.04256854256854257,
          0.04304954304954305,
          0.04641654641654642,
          0.046897546897546896,
          0.05603655603655604,
          0.056517556517556515,
          0.05772005772005772,
          0.0582010582010582,
          0.06084656084656084,
          0.06132756132756133,
          0.06445406445406446,
          0.06493506493506493,
          0.06517556517556518,
          0.06565656565656566,
          0.0670995670995671,
          0.06758056758056757,
          0.06806156806156806,
          0.06854256854256854,
          0.07864357864357864,
          0.07912457912457913,
          0.0848965848965849,
          0.08537758537758537,
          0.08561808561808562,
          0.0860990860990861,
          0.08706108706108706,
          0.08778258778258778,
          0.08802308802308802,
          0.0885040885040885,
          0.08874458874458875,
          0.08922558922558922,
          0.0897065897065897,
          0.09018759018759019,
          0.09163059163059163,
          0.09211159211159212,
          0.09283309283309284,
          0.09379509379509379,
          0.09475709475709476,
          0.09523809523809523,
          0.09547859547859548,
          0.09595959595959595,
          0.09956709956709957,
          0.10004810004810005,
          0.10245310245310245,
          0.10341510341510342,
          0.1050986050986051,
          0.10557960557960558,
          0.10798460798460799,
          0.10846560846560846,
          0.10870610870610871,
          0.1091871091871092,
          0.11014911014911015,
          0.11087061087061087,
          0.11255411255411256,
          0.11351611351611351,
          0.11447811447811448,
          0.11495911495911496,
          0.11832611832611832,
          0.1188071188071188,
          0.1204906204906205,
          0.12097162097162097,
          0.12121212121212122,
          0.12169312169312169,
          0.12481962481962482,
          0.12578162578162577,
          0.1272246272246272,
          0.12818662818662818,
          0.12962962962962962,
          0.13011063011063012,
          0.13347763347763347,
          0.1341991341991342,
          0.13876863876863876,
          0.13973063973063973,
          0.14045214045214044,
          0.1418951418951419,
          0.14237614237614238,
          0.14309764309764308,
          0.14357864357864358,
          0.15031265031265031,
          0.15079365079365079,
          0.15127465127465128,
          0.15175565175565175,
          0.15295815295815296,
          0.15343915343915343,
          0.15824915824915825,
          0.15873015873015872,
          0.16257816257816257,
          0.16305916305916307,
          0.164021164021164,
          0.16498316498316498,
          0.16714766714766716,
          0.16762866762866763,
          0.16835016835016836,
          0.16883116883116883,
          0.16907166907166907,
          0.16955266955266957,
          0.17027417027417027,
          0.17075517075517074,
          0.170995670995671,
          0.1717171717171717,
          0.1746031746031746,
          0.17532467532467533,
          0.18181818181818182,
          0.1822991822991823,
          0.18253968253968253,
          0.18302068302068303,
          0.18710918710918711,
          0.18759018759018758,
          0.18831168831168832,
          0.1887926887926888,
          0.18903318903318903,
          0.18951418951418952,
          0.19215969215969217,
          0.19264069264069264,
          0.19384319384319385,
          0.19456469456469455,
          0.19528619528619529,
          0.19576719576719576,
          0.19865319865319866,
          0.19913419913419914,
          0.19937469937469937,
          0.19985569985569984,
          0.20177970177970178,
          0.20226070226070225,
          0.20322270322270322,
          0.20466570466570466,
          0.2049062049062049,
          0.20562770562770563,
          0.2061087061087061,
          0.2065897065897066,
          0.20707070707070707,
          0.20803270803270804,
          0.2085137085137085,
          0.2113997113997114,
          0.2118807118807119,
          0.21236171236171236,
          0.21284271284271283,
          0.21332371332371333,
          0.2138047138047138,
          0.21572871572871574,
          0.2162097162097162,
          0.21645021645021645,
          0.21693121693121692,
          0.22101972101972103,
          0.2215007215007215,
          0.2227032227032227,
          0.22318422318422318,
          0.22510822510822512,
          0.22582972582972582,
          0.22631072631072632,
          0.2267917267917268,
          0.22727272727272727,
          0.227994227994228,
          0.23112073112073112,
          0.23208273208273208,
          0.2332852332852333,
          0.23376623376623376,
          0.23785473785473785,
          0.23833573833573835,
          0.24025974025974026,
          0.24074074074074073,
          0.24242424242424243,
          0.2429052429052429,
          0.24386724386724387,
          0.24434824434824434,
          0.24482924482924484,
          0.24579124579124578,
          0.24723424723424722,
          0.24771524771524772,
          0.25276575276575275,
          0.2532467532467532,
          0.25372775372775375,
          0.2542087542087542,
          0.25685425685425683,
          0.25757575757575757,
          0.25805675805675804,
          0.2582972582972583,
          0.2587782587782588,
          0.2597402597402597,
          0.26022126022126024,
          0.2607022607022607,
          0.2611832611832612,
          0.2619047619047619,
          0.26262626262626265,
          0.2631072631072631,
          0.26455026455026454,
          0.26527176527176527,
          0.2700817700817701,
          0.27056277056277056,
          0.2712842712842713,
          0.27176527176527177,
          0.2736892736892737,
          0.2741702741702742,
          0.2748917748917749,
          0.2753727753727754,
          0.27585377585377585,
          0.27705627705627706,
          0.2777777777777778,
          0.27825877825877826,
          0.2784992784992785,
          0.27946127946127947,
          0.27970177970177973,
          0.2801827801827802,
          0.2809042809042809,
          0.2813852813852814,
          0.2830687830687831,
          0.28354978354978355,
          0.2837902837902838,
          0.28475228475228476,
          0.2857142857142857,
          0.28619528619528617,
          0.28643578643578643,
          0.2873977873977874,
          0.2881192881192881,
          0.2890812890812891,
          0.2902837902837903,
          0.29124579124579125,
          0.291967291967292,
          0.29244829244829246,
          0.2934102934102934,
          0.29389129389129387,
          0.29485329485329487,
          0.29533429533429534,
          0.29605579605579607,
          0.29653679653679654,
          0.29677729677729675,
          0.2972582972582973,
          0.29773929773929775,
          0.2987012987012987,
          0.30014430014430016,
          0.3008658008658009,
          0.30206830206830204,
          0.30254930254930257,
          0.30423280423280424,
          0.3047138047138047,
          0.30543530543530545,
          0.3059163059163059,
          0.30663780663780665,
          0.3071188071188071,
          0.30784030784030786,
          0.30808080808080807,
          0.30856180856180854,
          0.3088023088023088,
          0.30928330928330927,
          0.31024531024531027,
          0.31096681096681095,
          0.3112073112073112,
          0.31216931216931215,
          0.3124098124098124,
          0.3128908128908129,
          0.31433381433381435,
          0.3152958152958153,
          0.31577681577681577,
          0.3177008177008177,
          0.31866281866281865,
          0.3201058201058201,
          0.3205868205868206,
          0.32323232323232326,
          0.3256373256373256,
          0.3265993265993266,
          0.3270803270803271,
          0.32924482924482923,
          0.3297258297258297,
          0.3321308321308321,
          0.33261183261183264,
          0.3333333333333333,
          0.33381433381433384,
          0.3345358345358345,
          0.3354978354978355,
          0.33645983645983646,
          0.33814333814333813,
          0.3386243386243386,
          0.3403078403078403,
          0.3407888407888408,
          0.341029341029341,
          0.3422318422318422,
          0.3431938431938432,
          0.3436748436748437,
          0.3443963443963444,
          0.3451178451178451,
          0.34559884559884557,
          0.34583934583934584,
          0.3463203463203463,
          0.34704184704184704,
          0.3475228475228475,
          0.3492063492063492,
          0.3496873496873497,
          0.3501683501683502,
          0.35064935064935066,
          0.35209235209235207,
          0.3525733525733526,
          0.3537758537758538,
          0.35425685425685427,
          0.35714285714285715,
          0.3578643578643579,
          0.3581048581048581,
          0.35858585858585856,
          0.3593073593073593,
          0.35978835978835977,
          0.36002886002886003,
          0.3605098605098605,
          0.360990860990861,
          0.36147186147186144,
          0.3617123617123617,
          0.3621933621933622,
          0.36267436267436265,
          0.3633958633958634,
          0.3665223665223665,
          0.367003367003367,
          0.36964886964886967,
          0.37012987012987014,
          0.37133237133237135,
          0.3718133718133718,
          0.37325637325637323,
          0.37373737373737376,
          0.37421837421837423,
          0.3746993746993747,
          0.37566137566137564,
          0.3759018759018759,
          0.37734487734487737,
          0.3775853775853776,
          0.3785473785473785,
          0.38167388167388167,
          0.38215488215488214,
          0.38311688311688313,
          0.3835978835978836,
          0.38431938431938434,
          0.3862433862433862,
          0.38672438672438675,
          0.38864838864838863,
          0.38961038961038963,
          0.38985088985088984,
          0.3903318903318903,
          0.3927368927368927,
          0.39321789321789324,
          0.3939393939393939,
          0.3944203944203944,
          0.3961038961038961,
          0.3965848965848966,
          0.39874939874939874,
          0.3994708994708995,
          0.4021164021164021,
          0.4025974025974026,
          0.4042809042809043,
          0.40476190476190477,
          0.405002405002405,
          0.4054834054834055,
          0.40644540644540644,
          0.4069264069264069,
          0.4074074074074074,
          0.4078884078884079,
          0.4081289081289081,
          0.4086099086099086,
          0.4093314093314093,
          0.4098124098124098,
          0.41053391053391053,
          0.41125541125541126,
          0.41173641173641173,
          0.4122174122174122,
          0.4126984126984127,
          0.4131794131794132,
          0.4136604136604137,
          0.4139009139009139,
          0.4143819143819144,
          0.41678691678691676,
          0.4172679172679173,
          0.41798941798941797,
          0.41895141895141896,
          0.41919191919191917,
          0.4196729196729197,
          0.4211159211159211,
          0.42207792207792205,
          0.42448292448292446,
          0.424963924963925,
          0.42568542568542567,
          0.4261664261664262,
          0.42688792688792687,
          0.42736892736892734,
          0.42784992784992787,
          0.4288119288119288,
          0.4292929292929293,
          0.4314574314574315,
          0.43193843193843195,
          0.4341029341029341,
          0.43506493506493504,
          0.43554593554593557,
          0.43626743626743625,
          0.4367484367484368,
          0.4379509379509379,
          0.4393939393939394,
          0.43963443963443966,
          0.4405964405964406,
          0.44155844155844154,
          0.44252044252044254,
          0.44612794612794615,
          0.4470899470899471,
          0.4482924482924483,
          0.44901394901394903,
          0.45021645021645024,
          0.4506974506974507,
          0.4509379509379509,
          0.45165945165945165,
          0.4523809523809524,
          0.45286195286195285,
          0.4543049543049543,
          0.455026455026455,
          0.455988455988456,
          0.4574314574314574,
          0.45815295815295815,
          0.4583934583934584,
          0.45911495911495914,
          0.46320346320346323,
          0.4639249639249639,
          0.4653679653679654,
          0.46584896584896585,
          0.4692159692159692,
          0.4704184704184704,
          0.4708994708994709,
          0.47113997113997114,
          0.4716209716209716,
          0.4721019721019721,
          0.4725829725829726,
          0.4730639730639731,
          0.4737854737854738,
          0.474025974025974,
          0.474987974987975,
          0.4766714766714767,
          0.47715247715247716,
          0.47763347763347763,
          0.4781144781144781,
          0.47883597883597884,
          0.4797979797979798,
          0.48124098124098125,
          0.4817219817219817,
          0.481962481962482,
          0.4829244829244829,
          0.48412698412698413,
          0.48484848484848486,
          0.48532948532948533,
          0.4858104858104858,
          0.48677248677248675,
          0.48773448773448774,
          0.4894179894179894,
          0.49013949013949015,
          0.4906204906204906,
          0.4911014911014911,
          0.49134199134199136,
          0.49278499278499277,
          0.49663299663299665,
          0.4971139971139971,
          0.49783549783549785,
          0.49807599807599806,
          0.49855699855699853,
          0.5004810004810005,
          0.5012025012025012,
          0.5016835016835017,
          0.5036075036075036,
          0.5040885040885041,
          0.5050505050505051,
          0.506012506012506,
          0.5064935064935064,
          0.5088985088985089,
          0.5108225108225108,
          0.5113035113035113,
          0.5117845117845118,
          0.5122655122655123,
          0.5132275132275133,
          0.5137085137085137,
          0.5139490139490139,
          0.5144300144300145,
          0.5158730158730159,
          0.5163540163540163,
          0.5168350168350169,
          0.5173160173160173,
          0.5175565175565175,
          0.5180375180375181,
          0.5194805194805194,
          0.51996151996152,
          0.5216450216450217,
          0.5221260221260221,
          0.5240500240500241,
          0.5245310245310245,
          0.525012025012025,
          0.5254930254930255,
          0.525974025974026,
          0.5264550264550265,
          0.5269360269360269,
          0.5274170274170275,
          0.5278980278980279,
          0.5283790283790284,
          0.532948532948533,
          0.5336700336700336,
          0.5346320346320347,
          0.5358345358345359,
          0.5396825396825397,
          0.5404040404040404,
          0.5411255411255411,
          0.5423280423280423,
          0.5428090428090429,
          0.5437710437710438,
          0.544973544973545,
          0.5459355459355459,
          0.5461760461760462,
          0.5476190476190477,
          0.5495430495430496,
          0.5505050505050505,
          0.550986050986051,
          0.5514670514670514,
          0.5517075517075517,
          0.5521885521885522,
          0.5529100529100529,
          0.5538720538720538,
          0.5565175565175565,
          0.5572390572390572,
          0.5574795574795575,
          0.557960557960558,
          0.5606060606060606,
          0.5615680615680616,
          0.5618085618085618,
          0.5622895622895623,
          0.563011063011063,
          0.5634920634920635,
          0.5642135642135642,
          0.5646945646945647,
          0.5651755651755652,
          0.5656565656565656,
          0.5658970658970659,
          0.5675805675805676,
          0.568061568061568,
          0.5685425685425686,
          0.569023569023569,
          0.5695045695045695,
          0.56998556998557,
          0.5721500721500722,
          0.5735930735930735,
          0.5743145743145743,
          0.575036075036075,
          0.5767195767195767,
          0.5772005772005772,
          0.5776815776815777,
          0.5781625781625782,
          0.5791245791245792,
          0.5796055796055796,
          0.5805675805675806,
          0.5815295815295816,
          0.5829725829725829,
          0.5834535834535834,
          0.5846560846560847,
          0.5851370851370852,
          0.5856180856180856,
          0.5877825877825877,
          0.588023088023088,
          0.588985088985089,
          0.5892255892255892,
          0.5897065897065897,
          0.5916305916305916,
          0.5921115921115921,
          0.5928330928330928,
          0.5933140933140933,
          0.5942760942760943,
          0.5952380952380952,
          0.5954785954785955,
          0.5971620971620971,
          0.5974025974025974,
          0.5981240981240982,
          0.5990860990860991,
          0.5993265993265994,
          0.5998075998075998,
          0.602934102934103,
          0.6038961038961039,
          0.6046176046176046,
          0.6053391053391053,
          0.6058201058201058,
          0.6067821067821068,
          0.6075036075036075,
          0.607984607984608,
          0.6089466089466089,
          0.6094276094276094,
          0.6101491101491101,
          0.6106301106301106,
          0.6113516113516113,
          0.6118326118326118,
          0.6123136123136124,
          0.6125541125541125,
          0.613035113035113,
          0.613997113997114,
          0.6144781144781145,
          0.6156806156806157,
          0.6161616161616161,
          0.6180856180856181,
          0.6195286195286195,
          0.62000962000962,
          0.6204906204906205,
          0.622895622895623,
          0.6233766233766234,
          0.6236171236171236,
          0.6240981240981242,
          0.6272246272246272,
          0.6277056277056277,
          0.6281866281866282,
          0.6293891293891294,
          0.6325156325156325,
          0.6332371332371333,
          0.6334776334776335,
          0.6339586339586339,
          0.6358826358826358,
          0.6363636363636364,
          0.6368446368446369,
          0.6373256373256373,
          0.6375661375661376,
          0.6380471380471381,
          0.639009139009139,
          0.6394901394901394,
          0.6418951418951419,
          0.6423761423761424,
          0.6430976430976431,
          0.6440596440596441,
          0.6443001443001443,
          0.645021645021645,
          0.6457431457431457,
          0.6467051467051467,
          0.6474266474266475,
          0.6479076479076479,
          0.6503126503126503,
          0.6507936507936508,
          0.6517556517556518,
          0.6522366522366523,
          0.6524771524771524,
          0.652958152958153,
          0.6536796536796536,
          0.6541606541606542,
          0.6544011544011544,
          0.6548821548821548,
          0.656084656084656,
          0.6565656565656566,
          0.6584896584896585,
          0.6594516594516594,
          0.6620971620971621,
          0.6628186628186629,
          0.6637806637806638,
          0.6654641654641654,
          0.665945165945166,
          0.6666666666666666,
          0.6673881673881674,
          0.6676286676286677,
          0.6681096681096681,
          0.6748436748436748,
          0.6750841750841751,
          0.6755651755651756,
          0.6789321789321789,
          0.6794131794131794,
          0.6796536796536796,
          0.6803751803751804,
          0.6810966810966811,
          0.6815776815776816,
          0.6822991822991823,
          0.6827801827801828,
          0.6859066859066859,
          0.6868686868686869,
          0.6885521885521886,
          0.68999518999519,
          0.690957190957191,
          0.6911976911976911,
          0.6921596921596922,
          0.6928811928811929,
          0.6931216931216931,
          0.6952861952861953,
          0.6962481962481962,
          0.6967291967291968,
          0.6972101972101972,
          0.6976911976911977,
          0.6984126984126984,
          0.6988936988936989,
          0.6996151996151996,
          0.6998556998556998,
          0.7003367003367004,
          0.701058201058201,
          0.702020202020202,
          0.7025012025012025,
          0.7034632034632035,
          0.7046657046657047,
          0.7056277056277056,
          0.7073112073112073,
          0.7077922077922078,
          0.708032708032708,
          0.7094757094757095,
          0.7097162097162097,
          0.7101972101972102,
          0.7111592111592112,
          0.7113997113997114,
          0.7128427128427128,
          0.714045214045214,
          0.7145262145262146,
          0.7147667147667147,
          0.7154882154882155,
          0.7157287157287158,
          0.7162097162097162,
          0.7178932178932179,
          0.7186147186147186,
          0.7190957190957191,
          0.7195767195767195,
          0.7198172198172198,
          0.7207792207792207,
          0.7212602212602213,
          0.7222222222222222,
          0.7231842231842232,
          0.7241462241462241,
          0.7246272246272246,
          0.7260702260702261,
          0.7265512265512265,
          0.7267917267917268,
          0.7272727272727273,
          0.7275132275132276,
          0.7289562289562289,
          0.72991822991823,
          0.7313612313612313,
          0.7323232323232324,
          0.7325637325637325,
          0.733044733044733,
          0.7335257335257336,
          0.7344877344877345,
          0.7347282347282347,
          0.7352092352092352,
          0.7364117364117364,
          0.7373737373737373,
          0.7376142376142376,
          0.7385762385762386,
          0.7390572390572391,
          0.7395382395382395,
          0.7397787397787398,
          0.7402597402597403,
          0.7424242424242424,
          0.742905242905243,
          0.7436267436267436,
          0.7448292448292448,
          0.7457912457912458,
          0.7465127465127465,
          0.7477152477152477,
          0.7481962481962482,
          0.7491582491582491,
          0.7498797498797499,
          0.7513227513227513,
          0.7522847522847523,
          0.7525252525252525,
          0.7534872534872535,
          0.7544492544492545,
          0.7546897546897547,
          0.7551707551707552,
          0.7554112554112554,
          0.7558922558922558,
          0.7566137566137566,
          0.757094757094757,
          0.7575757575757576,
          0.7587782587782588,
          0.759018759018759,
          0.7602212602212602,
          0.7609427609427609,
          0.7614237614237614,
          0.7623857623857624,
          0.7633477633477633,
          0.7645502645502645,
          0.7655122655122655,
          0.7662337662337663,
          0.7667147667147667,
          0.766955266955267,
          0.7674362674362675,
          0.7693602693602694,
          0.7703222703222703,
          0.7710437710437711,
          0.7715247715247715,
          0.7741702741702742,
          0.7746512746512747,
          0.7753727753727754,
          0.7760942760942761,
          0.778018278018278,
          0.7784992784992785,
          0.7801827801827802,
          0.7809042809042809,
          0.7816257816257817,
          0.7852332852332853,
          0.7866762866762866,
          0.789081289081289,
          0.7895622895622896,
          0.7902837902837903,
          0.7905242905242905,
          0.7914862914862915,
          0.7917267917267917,
          0.7922077922077922,
          0.7941317941317941,
          0.7948532948532948,
          0.7955747955747956,
          0.796055796055796,
          0.7965367965367965,
          0.7977392977392977,
          0.7987012987012987,
          0.7999037999037999,
          0.8006253006253006,
          0.8015873015873016,
          0.8025493025493026,
          0.803030303030303,
          0.8037518037518038,
          0.8047138047138047,
          0.804954304954305,
          0.8054353054353054,
          0.8068783068783069,
          0.8073593073593074,
          0.8080808080808081,
          0.8085618085618086,
          0.8095238095238095,
          0.81000481000481,
          0.8128908128908129,
          0.8136123136123136,
          0.8143338143338144,
          0.8148148148148148,
          0.8164983164983165,
          0.817941317941318,
          0.8193843193843194,
          0.8196248196248196,
          0.8201058201058201,
          0.8203463203463204,
          0.8208273208273208,
          0.8210678210678211,
          0.8215488215488216,
          0.8217893217893217,
          0.8244348244348244,
          0.8246753246753247,
          0.8251563251563252,
          0.8253968253968254,
          0.8261183261183261,
          0.8263588263588264,
          0.8268398268398268,
          0.8285233285233286,
          0.8318903318903319,
          0.8321308321308322,
          0.8330928330928331,
          0.8335738335738335,
          0.8345358345358346,
          0.8354978354978355,
          0.8359788359788359,
          0.8367003367003367,
          0.8379028379028379,
          0.8388648388648389,
          0.8405483405483406,
          0.841029341029341,
          0.8415103415103415,
          0.8458393458393458,
          0.8468013468013468,
          0.8484848484848485,
          0.8489658489658489,
          0.8494468494468495,
          0.8504088504088504,
          0.8513708513708513,
          0.8516113516113516,
          0.8520923520923521,
          0.8523328523328524,
          0.8537758537758537,
          0.8556998556998557,
          0.8576238576238576,
          0.8581048581048581,
          0.8590668590668591,
          0.8602693602693603,
          0.8607503607503607,
          0.8619528619528619,
          0.8624338624338624,
          0.8631553631553631,
          0.8636363636363636,
          0.8641173641173642,
          0.8645983645983646,
          0.8653198653198653,
          0.8658008658008658,
          0.8667628667628667,
          0.8672438672438673,
          0.8674843674843675,
          0.8679653679653679,
          0.8682058682058682,
          0.8691678691678691,
          0.8696488696488697,
          0.8701298701298701,
          0.8708513708513709,
          0.8713323713323713,
          0.8722943722943723,
          0.873015873015873,
          0.873977873977874,
          0.8742183742183742,
          0.8746993746993748,
          0.8749398749398749,
          0.8759018759018758,
          0.8768638768638769,
          0.8775853775853776,
          0.8785473785473785,
          0.8792688792688793,
          0.8795093795093795,
          0.8804713804713805,
          0.8807118807118807,
          0.8811928811928812,
          0.8821548821548821,
          0.8828763828763829,
          0.8833573833573833,
          0.8840788840788841,
          0.8843193843193843,
          0.8850408850408851,
          0.8855218855218855,
          0.8862433862433863,
          0.8867243867243867,
          0.8872053872053872,
          0.8876863876863876,
          0.8881673881673882,
          0.8888888888888888,
          0.8908128908128908,
          0.8912938912938912,
          0.8915343915343915,
          0.892015392015392,
          0.892977392977393,
          0.8932178932178932,
          0.8936988936988937,
          0.8939393939393939,
          0.8953823953823954,
          0.8965848965848966,
          0.8968253968253969,
          0.8973063973063973,
          0.8985088985088985,
          0.8994708994708994,
          0.8997113997113997,
          0.9001924001924002,
          0.9009139009139009,
          0.9013949013949014,
          0.9016354016354017,
          0.9023569023569024,
          0.9025974025974026,
          0.9033189033189033,
          0.9037999037999038,
          0.9042809042809042,
          0.905002405002405,
          0.9052429052429053,
          0.9057239057239057,
          0.9064454064454065,
          0.9086099086099086,
          0.9090909090909091,
          0.9136604136604136,
          0.9139009139009139,
          0.9143819143819144,
          0.9146224146224147,
          0.9151034151034151,
          0.9165464165464166,
          0.918951418951419,
          0.9191919191919192,
          0.9196729196729196,
          0.9199134199134199,
          0.9203944203944204,
          0.9211159211159211,
          0.9213564213564214,
          0.9218374218374218,
          0.922077922077922,
          0.9237614237614238,
          0.924963924963925,
          0.9254449254449254,
          0.9261664261664262,
          0.9268879268879269,
          0.9273689273689274,
          0.9276094276094277,
          0.9280904280904281,
          0.929052429052429,
          0.9295334295334295,
          0.936988936988937,
          0.9377104377104377,
          0.9381914381914382,
          0.9396344396344396,
          0.9405964405964405,
          0.9451659451659452,
          0.9456469456469456,
          0.948051948051948,
          0.9485329485329486,
          0.9492544492544492,
          0.9494949494949495,
          0.9504569504569504,
          0.9511784511784511,
          0.9523809523809523,
          0.9528619528619529,
          0.9531024531024531,
          0.9538239538239538,
          0.9547859547859547,
          0.9552669552669553,
          0.9555074555074555,
          0.9559884559884559,
          0.9562289562289562,
          0.9605579605579606,
          0.9615199615199616,
          0.9634439634439634,
          0.9641654641654641,
          0.9646464646464646,
          0.9665704665704665,
          0.967051467051467,
          0.9675324675324676,
          0.9684944684944685,
          0.9689754689754689,
          0.9694564694564695,
          0.9696969696969697,
          0.9701779701779701,
          0.9706589706589707,
          0.9711399711399712,
          0.9721019721019721,
          0.9725829725829725,
          0.9735449735449735,
          0.9745069745069745,
          0.976911976911977,
          0.9773929773929774,
          0.9793169793169794,
          0.9795574795574795,
          0.98003848003848,
          0.9824434824434825,
          0.982924482924483,
          0.9834054834054834,
          0.9841269841269841,
          0.9846079846079846,
          0.9860509860509861,
          0.9874939874939875,
          0.987974987974988,
          0.9882154882154882,
          0.9886964886964887,
          0.9894179894179894,
          0.9903799903799904,
          0.9920634920634921,
          0.993025493025493,
          0.9932659932659933,
          0.9937469937469937,
          0.993987493987494,
          0.9951899951899952,
          0.9959114959114959,
          0.9966329966329966,
          0.9975949975949976,
          0.9980759980759981,
          0.9985569985569985,
          0.9987974987974988,
          0.9997594997594997,
          1
         ]
        },
        {
         "mode": "lines",
         "name": "test",
         "type": "scatter",
         "x": [
          0,
          0.023809523809523808,
          0.047619047619047616,
          0.047619047619047616,
          0.16666666666666666,
          0.16666666666666666,
          0.19047619047619047,
          0.19047619047619047,
          0.30952380952380953,
          0.30952380952380953,
          0.3333333333333333,
          0.3333333333333333,
          0.38095238095238093,
          0.38095238095238093,
          0.4523809523809524,
          0.4523809523809524,
          0.4523809523809524,
          0.4523809523809524,
          0.5,
          0.5,
          0.5238095238095238,
          0.5238095238095238,
          0.5476190476190477,
          0.5476190476190477,
          0.5714285714285714,
          0.5714285714285714,
          0.5952380952380952,
          0.5952380952380952,
          0.5952380952380952,
          0.5952380952380952,
          0.5952380952380952,
          0.5952380952380952,
          0.6190476190476191,
          0.6190476190476191,
          0.6428571428571429,
          0.6428571428571429,
          0.6666666666666666,
          0.6904761904761905,
          0.6904761904761905,
          0.7142857142857143,
          0.7142857142857143,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7380952380952381,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7619047619047619,
          0.7857142857142857,
          0.8333333333333334,
          0.8333333333333334,
          0.8333333333333334,
          0.8333333333333334,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8571428571428571,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.8809523809523809,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9047619047619048,
          0.9285714285714286,
          0.9285714285714286,
          0.9285714285714286,
          0.9285714285714286,
          0.9285714285714286,
          0.9285714285714286,
          0.9285714285714286,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9523809523809523,
          0.9761904761904762,
          0.9761904761904762,
          0.9761904761904762,
          0.9761904761904762,
          0.9761904761904762,
          0.9761904761904762,
          0.9761904761904762,
          0.9761904761904762,
          0.9761904761904762,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "y": [
          0,
          0,
          0,
          0.0009615384615384616,
          0.0009615384615384616,
          0.0019230769230769232,
          0.0019230769230769232,
          0.0028846153846153848,
          0.0028846153846153848,
          0.0038461538461538464,
          0.0038461538461538464,
          0.004807692307692308,
          0.004807692307692308,
          0.0057692307692307696,
          0.0057692307692307696,
          0.007692307692307693,
          0.009615384615384616,
          0.015384615384615385,
          0.015384615384615385,
          0.023076923076923078,
          0.023076923076923078,
          0.026923076923076925,
          0.026923076923076925,
          0.027884615384615386,
          0.027884615384615386,
          0.04519230769230769,
          0.04519230769230769,
          0.06826923076923076,
          0.07019230769230769,
          0.08557692307692308,
          0.08942307692307692,
          0.09423076923076923,
          0.09423076923076923,
          0.125,
          0.125,
          0.12692307692307692,
          0.12788461538461537,
          0.12788461538461537,
          0.16153846153846155,
          0.16153846153846155,
          0.175,
          0.175,
          0.17884615384615385,
          0.18173076923076922,
          0.19807692307692307,
          0.2,
          0.20288461538461539,
          0.2048076923076923,
          0.2605769230769231,
          0.2605769230769231,
          0.30673076923076925,
          0.3096153846153846,
          0.31153846153846154,
          0.33653846153846156,
          0.33653846153846156,
          0.33653846153846156,
          0.3375,
          0.33942307692307694,
          0.3490384615384615,
          0.3490384615384615,
          0.35096153846153844,
          0.35384615384615387,
          0.3596153846153846,
          0.36153846153846153,
          0.37403846153846154,
          0.37596153846153846,
          0.4,
          0.40192307692307694,
          0.4173076923076923,
          0.41923076923076924,
          0.4269230769230769,
          0.4288461538461538,
          0.42980769230769234,
          0.43173076923076925,
          0.4423076923076923,
          0.4442307692307692,
          0.4461538461538462,
          0.4480769230769231,
          0.46153846153846156,
          0.4653846153846154,
          0.4721153846153846,
          0.4740384615384615,
          0.49038461538461536,
          0.49230769230769234,
          0.5009615384615385,
          0.5009615384615385,
          0.5028846153846154,
          0.5048076923076923,
          0.5192307692307693,
          0.5230769230769231,
          0.525,
          0.5269230769230769,
          0.5365384615384615,
          0.5384615384615384,
          0.5788461538461539,
          0.5807692307692308,
          0.5836538461538462,
          0.5855769230769231,
          0.5903846153846154,
          0.5923076923076923,
          0.5951923076923077,
          0.5971153846153846,
          0.6,
          0.6048076923076923,
          0.6086538461538461,
          0.6105769230769231,
          0.6182692307692308,
          0.6211538461538462,
          0.625,
          0.6269230769230769,
          0.6278846153846154,
          0.6288461538461538,
          0.6413461538461539,
          0.6432692307692308,
          0.6576923076923077,
          0.6596153846153846,
          0.6605769230769231,
          0.6625,
          0.6634615384615384,
          0.6653846153846154,
          0.6673076923076923,
          0.675,
          0.6817307692307693,
          0.6836538461538462,
          0.6836538461538462,
          0.6855769230769231,
          0.6884615384615385,
          0.6903846153846154,
          0.6942307692307692,
          0.6961538461538461,
          0.7019230769230769,
          0.7019230769230769,
          0.7057692307692308,
          0.7096153846153846,
          0.7115384615384616,
          0.7134615384615385,
          0.7269230769230769,
          0.7298076923076923,
          0.7346153846153847,
          0.7365384615384616,
          0.7423076923076923,
          0.7451923076923077,
          0.7480769230769231,
          0.7519230769230769,
          0.7567307692307692,
          0.7586538461538461,
          0.760576923076923,
          0.7625,
          0.7673076923076924,
          0.7692307692307693,
          0.775,
          0.7778846153846154,
          0.7884615384615384,
          0.7903846153846154,
          0.7932692307692307,
          0.7951923076923076,
          0.7971153846153847,
          0.7990384615384616,
          0.8038461538461539,
          0.8086538461538462,
          0.8115384615384615,
          0.8144230769230769,
          0.8163461538461538,
          0.8192307692307692,
          0.8221153846153846,
          0.8240384615384615,
          0.8326923076923077,
          0.8346153846153846,
          0.8355769230769231,
          0.8375,
          0.8413461538461539,
          0.8461538461538461,
          0.8490384615384615,
          0.8557692307692307,
          0.8576923076923076,
          0.8615384615384616,
          0.8634615384615385,
          0.8653846153846154,
          0.8673076923076923,
          0.8701923076923077,
          0.8740384615384615,
          0.8923076923076924,
          0.8951923076923077,
          0.8961538461538462,
          0.9,
          0.9009615384615385,
          0.9048076923076923,
          0.9076923076923077,
          0.9115384615384615,
          0.9163461538461538,
          0.925,
          0.9278846153846154,
          0.9298076923076923,
          0.9384615384615385,
          0.9394230769230769,
          0.9423076923076923,
          0.9432692307692307,
          0.948076923076923,
          0.9509615384615384,
          0.9538461538461539,
          0.9596153846153846,
          0.9615384615384616,
          0.9625,
          0.9644230769230769,
          0.9653846153846154,
          0.9711538461538461,
          0.9788461538461538,
          0.9807692307692307,
          0.9817307692307692,
          0.9855769230769231,
          0.9875,
          0.989423076923077,
          0.9913461538461539,
          0.9942307692307693,
          0.9961538461538462,
          0.9971153846153846,
          0.9990384615384615,
          1
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "ROC curve",
         "x": 0.5
        },
        "xaxis": {
         "title": {
          "text": "False Positive Rate"
         }
        },
        "yaxis": {
         "title": {
          "text": "True Positive Rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize ROC curves\n",
    "probas_train = gridsearch.predict_proba(X_train)[:,1]\n",
    "precisions, recalls, thresholds = roc_curve(y_train, probas_train)\n",
    "fig = go.Figure(\n",
    "    data = go.Scatter(\n",
    "        name = 'train',\n",
    "        x = recalls, \n",
    "        y = precisions, \n",
    "        mode = 'lines'\n",
    "    ),\n",
    "    layout = go.Layout(\n",
    "        title = go.layout.Title(text = \"ROC curve\", x = 0.5),\n",
    "        xaxis = go.layout.XAxis(title = 'False Positive Rate'),\n",
    "        yaxis = go.layout.YAxis(title = 'True Positive Rate')\n",
    "    )\n",
    ")\n",
    "\n",
    "probas_test = gridsearch.predict_proba(X_test)[:,1]\n",
    "precisions, recalls, thresholds = roc_curve(y_test, probas_test)\n",
    "fig.add_trace(go.Scatter(\n",
    "    name = 'test',\n",
    "    x = recalls, \n",
    "    y = precisions, \n",
    "    mode = 'lines'\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test métrique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "knn = KNeighborsClassifier()\n",
    "nb = GaussianNB()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "classifiers.append(tree)\n",
    "classifiers.append(knn)\n",
    "classifiers.append(nb)\n",
    "classifiers.append(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_results = []\n",
    "for classifier in classifiers:\n",
    "  cv_results.append(cross_val_score(classifier, X_train, y_train, cv=10 ,scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.97459584, 0.97459584, 0.97228637, 0.97228637, 0.9630485 ,\n",
       "        0.96766744, 0.96997691, 0.97228637, 0.97222222, 0.97685185]),\n",
       " array([0.96997691, 0.96766744, 0.96535797, 0.96997691, 0.9630485 ,\n",
       "        0.9630485 , 0.96766744, 0.96997691, 0.96990741, 0.96759259]),\n",
       " array([0.91224018, 0.93533487, 0.91685912, 0.90300231, 0.88452656,\n",
       "        0.86143187, 0.89376443, 0.89145497, 0.90740741, 0.90509259]),\n",
       " array([0.96766744, 0.9630485 , 0.97228637, 0.9630485 , 0.95612009,\n",
       "        0.96535797, 0.97459584, 0.96766744, 0.97222222, 0.96990741])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy_mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.971582</td>\n",
       "      <td>0.003722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.967422</td>\n",
       "      <td>0.002616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg</td>\n",
       "      <td>0.967192</td>\n",
       "      <td>0.005236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb</td>\n",
       "      <td>0.901111</td>\n",
       "      <td>0.018971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithm  Accuracy_mean       Std\n",
       "0      tree       0.971582  0.003722\n",
       "1       knn       0.967422  0.002616\n",
       "3    logreg       0.967192  0.005236\n",
       "2        nb       0.901111  0.018971"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Algorithm\": [\"tree\", \"knn\", \"nb\", \"logreg\"],\n",
    "             \"Accuracy_mean\": [cv_result.mean() for cv_result in cv_results],\n",
    "             \"Std\": [cv_result.std() for cv_result in cv_results]})\n",
    "\n",
    "results = results.sort_values(by=\"Accuracy_mean\", ascending=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "best Tree score on the train set : \n",
      " 0.975739371534196 \n",
      " best Tree score on the test set : \n",
      " 0.9685767097966729\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier() \n",
    "tree_param_grid = {\"max_depth\" : np.arange(1,10,1)}\n",
    "\n",
    "gsTree = GridSearchCV(tree,tree_param_grid, cv=10, scoring=\"accuracy\", verbose = 1)\n",
    "\n",
    "gsTree.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "tree_best = gsTree.best_estimator_\n",
    "\n",
    "# Best score\n",
    "print(\"best {0} score on the train set : \\n {1} \\n best {0} score on the test set : \\n {2}\".format(\"Tree\", tree_best.score(X_train,y_train), tree_best.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 19 candidates, totalling 190 fits\n",
      "best Tree score on the train set : \n",
      " 0.9893715341959335 \n",
      " best Tree score on the test set : \n",
      " 0.9713493530499075\n"
     ]
    }
   ],
   "source": [
    "tree = RandomForestClassifier() \n",
    "tree_param_grid = {\"max_depth\" : np.arange(1,20,1)}\n",
    "\n",
    "gsTree = GridSearchCV(tree,tree_param_grid, cv=10, scoring=\"accuracy\", verbose = 1)\n",
    "\n",
    "gsTree.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "tree_best = gsTree.best_estimator_\n",
    "\n",
    "# Best score\n",
    "print(\"best {0} score on the train set : \\n {1} \\n best {0} score on the test set : \\n {2}\".format(\"Tree\", tree_best.score(X_train,y_train), tree_best.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "best Knn score on the train set : \n",
      " 0.9715804066543438 \n",
      " best Knn score on the test set : \n",
      " 0.9685767097966729\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn_param_grid = {\"n_neighbors\": np.arange(1,10,1)}\n",
    "\n",
    "gsknn = GridSearchCV(knn,knn_param_grid, cv=10, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n",
    "gsknn.fit(X_train, y_train)\n",
    "\n",
    "gsknn_best = gsknn.best_estimator_\n",
    "print(\"best {0} score on the train set : \\n {1} \\n best {0} score on the test set : \\n {2}\".format(\"Knn\", gsknn_best.score(X_train,y_train), gsknn_best.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "best Logistic regression score on the train set : \n",
      " 0.967652495378928 \n",
      " best Logistic regression score on the test set : \n",
      " 0.9658040665434381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Log = LogisticRegression()\n",
    "log_param_grid = {\"class_weight\": [None, \"balanced\"],\n",
    "                 \"C\": [0.5, 0.6,0.7,0.8,0.9,1.0]}\n",
    "gsLog = GridSearchCV(Log, log_param_grid, cv =10, scoring ='accuracy', n_jobs=-1, verbose=1)\n",
    "gsLog.fit(X_train, y_train)\n",
    "gsLog_best = gsLog.best_estimator_\n",
    "\n",
    "print(\"best {0} score on the train set : \\n {1} \\n best {0} score on the test set : \\n {2}\".format(\"Logistic regression\", gsLog_best.score(X_train,y_train), gsLog_best.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no grid search needed on naive bayes\n",
    "Naive_Bayes = GaussianNB()\n",
    "gsNaive_Bayes_best = Naive_Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "votingC = VotingClassifier(estimators=[(\"tree\", tree_best),(\"knn\",gsknn_best),(\"LogisticRegression\",gsLog_best),(\"Naive Bayes\",gsNaive_Bayes_best)], \n",
    "                           voting='soft')\n",
    "\n",
    "votingC = votingC.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Voting Classifier score on the train set : \n",
      " 0.9766635859519408 \n",
      " best Voting Classifier score on the test set : \n",
      " 0.967652495378928\n"
     ]
    }
   ],
   "source": [
    "print(\" {0} score on the train set : \\n {1} \\n best {0} score on the test set : \\n {2}\".format(\"Voting Classifier\", votingC.score(X_train,y_train), votingC.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average score is : 0.9725039560345564 \n",
      " the standard deviation of the score is : 0.004193648280645329 \n",
      " the list of score : [0.97459584 0.97459584 0.97690531 0.97459584 0.9630485  0.96766744\n",
      " 0.97690531 0.97459584 0.97222222 0.96990741]\n"
     ]
    }
   ],
   "source": [
    "Xvalscore = cross_val_score(votingC, X_train, y_train, scoring = \"accuracy\", cv = 10, n_jobs=4)\n",
    "print(\"the average score is : {0} \\n the standard deviation of the score is : {1} \\n the list of score : {2}\".format(Xvalscore.mean(), Xvalscore.std(), Xvalscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score on test set :  0.9236641221374046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"f1-score on test set : \", f1_score(y_test, predictions))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=12,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    random_state=0,\n",
    "    shuffle=False,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feature_names = list_columns.columns\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time to compute the importances: 0.012 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence_line', 'Nb_tokens', '%texte_lu', '%texte_lu_fin_ligne',\n",
       "       'Verb_count', 'Propn_count', 'Noun_count', 'Num_count',\n",
       "       'Pourcentage_verb_sentence', 'Pourcentage_propn_sentence',\n",
       "       'Pourcentage_noun_sentence', 'Pourcentage_num_sentence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_columns = dataset.drop(features_list, axis = 1)\n",
    "list_columns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
