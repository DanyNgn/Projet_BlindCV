{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6 - Concatenation des CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des librairies nécessaires\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Chargement de la dataset\n",
    "dataset = pd.read_csv(\"dataset_text_brut_200.csv\", delimiter = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation de la bibliothèque fr_core_news_lg de Spacy\n",
    "nlp = spacy.load(\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization des textes de CV\n",
    "dataset[\"Tokenization_preparation\"] = dataset[\"CV_text_brut\"].apply(lambda x : str(x).replace(\"\\n\", \" \"))\n",
    "dataset[\"Tokenization\"] = dataset[\"Tokenization_preparation\"].apply(lambda x : nlp(x))\n",
    "dataset[\"Number_of_tokens\"] = dataset[\"Tokenization\"].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_number</th>\n",
       "      <th>CV_text_brut</th>\n",
       "      <th>Tokenization_preparation</th>\n",
       "      <th>Tokenization</th>\n",
       "      <th>Number_of_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CV_1</td>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE PRO...</td>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE PRO...</td>\n",
       "      <td>(SELMA, LAFKIR, CORDE, 80, CODEUSE, ENTHOUSIAS...</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CV_2</td>\n",
       "      <td>Ananya Singh \\nProfesseure de Lycée\\nPROFIL PE...</td>\n",
       "      <td>Ananya Singh  Professeure de Lycée PROFIL PERS...</td>\n",
       "      <td>(Ananya, Singh,  , Professeure, de, Lycée, PRO...</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CV_3</td>\n",
       "      <td>Samira Hadid \\n16 place Urbain II, 63000 Clerm...</td>\n",
       "      <td>Samira Hadid  16 place Urbain II, 63000 Clermo...</td>\n",
       "      <td>(Samira, Hadid,  , 16, place, Urbain, II, ,, 6...</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CV_4</td>\n",
       "      <td>Carine Maurice \\nIngénieure logiciel avec 10 a...</td>\n",
       "      <td>Carine Maurice  Ingénieure logiciel avec 10 an...</td>\n",
       "      <td>(Carine, Maurice,  , Ingénieure, logiciel, ave...</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CV_5</td>\n",
       "      <td>MARCELLE ANDRÉ EXPÉRIENCE PROFESSIONNELLE \\nAS...</td>\n",
       "      <td>MARCELLE ANDRÉ EXPÉRIENCE PROFESSIONNELLE  ASS...</td>\n",
       "      <td>(MARCELLE, ANDRÉ, EXPÉRIENCE, PROFESSIONNELLE,...</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CV_number                                       CV_text_brut  \\\n",
       "0      CV_1  SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE PRO...   \n",
       "1      CV_2  Ananya Singh \\nProfesseure de Lycée\\nPROFIL PE...   \n",
       "2      CV_3  Samira Hadid \\n16 place Urbain II, 63000 Clerm...   \n",
       "3      CV_4  Carine Maurice \\nIngénieure logiciel avec 10 a...   \n",
       "4      CV_5  MARCELLE ANDRÉ EXPÉRIENCE PROFESSIONNELLE \\nAS...   \n",
       "\n",
       "                            Tokenization_preparation  \\\n",
       "0  SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE PRO...   \n",
       "1  Ananya Singh  Professeure de Lycée PROFIL PERS...   \n",
       "2  Samira Hadid  16 place Urbain II, 63000 Clermo...   \n",
       "3  Carine Maurice  Ingénieure logiciel avec 10 an...   \n",
       "4  MARCELLE ANDRÉ EXPÉRIENCE PROFESSIONNELLE  ASS...   \n",
       "\n",
       "                                        Tokenization  Number_of_tokens  \n",
       "0  (SELMA, LAFKIR, CORDE, 80, CODEUSE, ENTHOUSIAS...               214  \n",
       "1  (Ananya, Singh,  , Professeure, de, Lycée, PRO...               291  \n",
       "2  (Samira, Hadid,  , 16, place, Urbain, II, ,, 6...               315  \n",
       "3  (Carine, Maurice,  , Ingénieure, logiciel, ave...               292  \n",
       "4  (MARCELLE, ANDRÉ, EXPÉRIENCE, PROFESSIONNELLE,...               284  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-67-5ba8c267383a>:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['%texte_lu'][i] = (f\"{pourcentage_debut:.2f}\")\n",
      "<ipython-input-67-5ba8c267383a>:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"%texte_lu_fin_ligne\"][k] = (f\"{resultat:.2f}\")\n",
      "<ipython-input-67-5ba8c267383a>:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Is_alpha\"][l] = list_token_is_alpha\n",
      "<ipython-input-67-5ba8c267383a>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Grammar\"][m] = list_token_pos\n"
     ]
    }
   ],
   "source": [
    "dataset_concat = pd.DataFrame()\n",
    "\n",
    "for cv in range(len(dataset)): \n",
    "    ## Création d'une dataframe pour accueillir les features des CV\n",
    "    df = pd.DataFrame(columns = ['CV_Sentences'])\n",
    "    df.head()\n",
    "    \n",
    "    ## Extraction du texte du CV pour appliquer la sentencisation des CV\n",
    "    index = dataset.columns.get_loc(\"Tokenization_preparation\")\n",
    "\n",
    "    doc = dataset.iloc[cv,index]                                                             \n",
    "    doc = nlp(doc)  \n",
    "    \n",
    "    ## Création d'une liste de phrases qui va accueillir le texte découpé en phrases grâce à la fonction doc.sents\n",
    "    list_sentence = []\n",
    "    for sent in doc.sents :\n",
    "        list_sentence.append(sent)\n",
    "\n",
    "    ## Affectation des phrases dans la colonne CV_Sentences et suppression des parenthèses\n",
    "    df['CV_Sentences'] = list_sentence\n",
    "    df['Sentences_CV_clean'] = df['CV_Sentences'].apply(lambda x : str(x).replace(\"(\",\"\").replace(\")\",\"\"))## Création de la colonne CV_number pour identifier les phrases de chaque CV\n",
    "    index2 = dataset.columns.get_loc(\"CV_number\")\n",
    "    df['CV_Number'] = dataset.iloc[cv,index2]                                              \n",
    "\n",
    "    ## Création d'une colonne qui compte le nombre de phrase par CV\n",
    "    df['Sentence_line'] = df.index     \n",
    "\n",
    "    ## Création d'une colonne qui compte le nombre de tokens dans la phrase\n",
    "    df['Nb_tokens'] = df['CV_Sentences'].apply(lambda x : len(x))\n",
    "\n",
    "    ## Création d'une colonne pour connaitre la position en pourcentage de la phrase par rapport au texte entier\n",
    "    df['%texte_lu'] = 0\n",
    "    token_count = 0\n",
    "    for i in range(len(df)):\n",
    "        token_count += df['Nb_tokens'][i]\n",
    "        pourcentage_debut = ((token_count / df['Nb_tokens'].sum()) * 100)\n",
    "        df['%texte_lu'][i] = (f\"{pourcentage_debut:.2f}\")\n",
    "\n",
    "    ## Création pour connaître le % de texte lu à partir de la fin de ligne\n",
    "    df[\"%texte_lu_fin_ligne\"] = 0\n",
    "    nb_tokens_total = df[\"Nb_tokens\"].sum()\n",
    "    sum_token = 0\n",
    "\n",
    "    for k in range(len(df)):\n",
    "        sum_token += df['Nb_tokens'][k]\n",
    "        resultat = ((nb_tokens_total - sum_token) / nb_tokens_total) * 100\n",
    "        df[\"%texte_lu_fin_ligne\"][k] = (f\"{resultat:.2f}\")\n",
    "\n",
    "    ## Création d'une colonne pour savoir si le mot est alphanumerique ou pas\n",
    "    df[\"Is_alpha\"] = 0\n",
    "    index3 = df.columns.get_loc(\"Sentences_CV_clean\")\n",
    "    for l in range(len(df)):\n",
    "        doc = df.iloc[l,index3]\n",
    "        doc = nlp(doc)\n",
    "        list_token_is_alpha = []\n",
    "        for token in doc:\n",
    "            list_token_is_alpha.append(token.is_alpha)\n",
    "        df[\"Is_alpha\"][l] = list_token_is_alpha\n",
    "\n",
    "    ## Création de la colonne Grammaire pour connaitre le type de mots de la phrase\n",
    "    df[\"Grammar\"] = 0\n",
    "    index = df.columns.get_loc(\"Sentences_CV_clean\")\n",
    "    for m in range(len(df)):\n",
    "        doc = df.iloc[m,index]\n",
    "        doc = nlp(doc)\n",
    "        list_token_pos = []\n",
    "        for token in doc:\n",
    "            list_token_pos.append(token.pos_)\n",
    "            df[\"Grammar\"][m] = list_token_pos\n",
    "\n",
    "    ## Création de la colonne Label qui sera notre target. Soit 1 il faut supprimer soit 0 on doit garder\n",
    "    df['Label'] = 0\n",
    "\n",
    "    ## Création de la dataset avec les CV concatenés\n",
    "    dataset_concat = dataset_concat.append(df)\n",
    "\n",
    "## Création d'un fichier Excel avec toutes les phrases de tous les CV\n",
    "PATH = \"/Users/kinn/Desktop/Projet_BlindCV/dataset_CV_200.csv\"\n",
    "dataset_concat.to_csv(path_or_buf = PATH, index = False, sep = \";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de3c26e6ce99fdd1b8cf42170bfea1efe2e407789632cce096fb11ae10dab296"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
