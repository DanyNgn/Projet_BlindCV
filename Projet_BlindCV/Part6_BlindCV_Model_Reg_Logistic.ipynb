{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U scikit-learn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in c:\\python39\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.3 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.3.2 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\python39\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python39\\lib\\site-packages (from matplotlib) (4.34.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python39\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python39\\lib\\site-packages (from matplotlib) (1.23.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python39\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python39\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dany\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Chargement des librairies nécessaires\n",
    "import pip \n",
    "pip.main([\"install\",\"matplotlib\"]) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # to avoid deprecation warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la dataset\n",
    "## Choix de 4 datasets, ne pas oublier de choisir la features_list adéquate\n",
    "\n",
    "#dataset = pd.read_csv(\"dataset_CV_labelise_features1_100.csv\", delimiter = \";\", encoding = \"utf-8\") ## 100CV + 4 features numériques\n",
    "\n",
    "#dataset = pd.read_csv(\"dataset_CV_labelise_features1_200.csv\", delimiter = \";\", encoding = \"utf-8\") ## 200CV + 4 features numériques\n",
    "\n",
    "#dataset = pd.read_csv(\"dataset_CV_labelise_features2_100.csv\", delimiter = \";\", encoding = \"utf-8\") ## 100CV + 12 features numériques\n",
    "\n",
    "dataset = pd.read_csv(\"dataset_CV_labelise_features2_200.csv\", delimiter = \";\", encoding = \"utf-8\") ## 200CV + 12 features numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5594, 19)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 5594\n",
      "\n",
      "Display of dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_Sentences</th>\n",
       "      <th>Sentences_CV_clean</th>\n",
       "      <th>CV_Number</th>\n",
       "      <th>Sentence_line</th>\n",
       "      <th>Nb_tokens</th>\n",
       "      <th>%texte_lu</th>\n",
       "      <th>%texte_lu_fin_ligne</th>\n",
       "      <th>Is_alpha</th>\n",
       "      <th>Grammar</th>\n",
       "      <th>Tokenization</th>\n",
       "      <th>Verb_count</th>\n",
       "      <th>Propn_count</th>\n",
       "      <th>Noun_count</th>\n",
       "      <th>Num_count</th>\n",
       "      <th>Pourcentage_verb_sentence</th>\n",
       "      <th>Pourcentage_propn_sentence</th>\n",
       "      <th>Pourcentage_noun_sentence</th>\n",
       "      <th>Pourcentage_num_sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE</td>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.80</td>\n",
       "      <td>97.20</td>\n",
       "      <td>[True, True, True, False, True, True]</td>\n",
       "      <td>['NOUN', 'PROPN', 'VERB', 'NUM', 'PROPN', 'PRO...</td>\n",
       "      <td>SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PROFIL</td>\n",
       "      <td>PROFIL</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.27</td>\n",
       "      <td>96.73</td>\n",
       "      <td>[True]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td>PROFIL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PERSONNEL Je suis étudiante au lycée Condorcet.</td>\n",
       "      <td>PERSONNEL Je suis étudiante au lycée Condorcet.</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7.01</td>\n",
       "      <td>92.99</td>\n",
       "      <td>[True, True, True, True, True, True, True, False]</td>\n",
       "      <td>['NOUN', 'PRON', 'AUX', 'ADJ', 'ADP', 'NOUN', ...</td>\n",
       "      <td>PERSONNEL Je suis étudiante au lycée Condorcet.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je code depuis l'âge de 13 ans et j'aime créer...</td>\n",
       "      <td>Je code depuis l'âge de 13 ans et j'aime créer...</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>18.22</td>\n",
       "      <td>81.78</td>\n",
       "      <td>[True, True, True, False, True, True, False, T...</td>\n",
       "      <td>['PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', ...</td>\n",
       "      <td>Je code depuis l'âge de 13 ans et j'aime créer...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RÉALISATIONS</td>\n",
       "      <td>RÉALISATIONS</td>\n",
       "      <td>CV_1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>18.69</td>\n",
       "      <td>81.31</td>\n",
       "      <td>[True]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td>RÉALISATIONS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        CV_Sentences  \\\n",
       "0         SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE   \n",
       "1                                             PROFIL   \n",
       "2    PERSONNEL Je suis étudiante au lycée Condorcet.   \n",
       "3  Je code depuis l'âge de 13 ans et j'aime créer...   \n",
       "4                                       RÉALISATIONS   \n",
       "\n",
       "                                  Sentences_CV_clean CV_Number  Sentence_line  \\\n",
       "0         SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE      CV_1              0   \n",
       "1                                             PROFIL      CV_1              1   \n",
       "2    PERSONNEL Je suis étudiante au lycée Condorcet.      CV_1              2   \n",
       "3  Je code depuis l'âge de 13 ans et j'aime créer...      CV_1              3   \n",
       "4                                       RÉALISATIONS      CV_1              4   \n",
       "\n",
       "   Nb_tokens  %texte_lu  %texte_lu_fin_ligne  \\\n",
       "0          6       2.80                97.20   \n",
       "1          1       3.27                96.73   \n",
       "2          8       7.01                92.99   \n",
       "3         24      18.22                81.78   \n",
       "4          1      18.69                81.31   \n",
       "\n",
       "                                            Is_alpha  \\\n",
       "0              [True, True, True, False, True, True]   \n",
       "1                                             [True]   \n",
       "2  [True, True, True, True, True, True, True, False]   \n",
       "3  [True, True, True, False, True, True, False, T...   \n",
       "4                                             [True]   \n",
       "\n",
       "                                             Grammar  \\\n",
       "0  ['NOUN', 'PROPN', 'VERB', 'NUM', 'PROPN', 'PRO...   \n",
       "1                                           ['NOUN']   \n",
       "2  ['NOUN', 'PRON', 'AUX', 'ADJ', 'ADP', 'NOUN', ...   \n",
       "3  ['PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', ...   \n",
       "4                                           ['NOUN']   \n",
       "\n",
       "                                        Tokenization  Verb_count  Propn_count  \\\n",
       "0         SELMA LAFKIR CORDE 80 CODEUSE ENTHOUSIASTE           1            3   \n",
       "1                                             PROFIL           0            0   \n",
       "2    PERSONNEL Je suis étudiante au lycée Condorcet.           0            1   \n",
       "3  Je code depuis l'âge de 13 ans et j'aime créer...           5            1   \n",
       "4                                       RÉALISATIONS           0            0   \n",
       "\n",
       "   Noun_count  Num_count  Pourcentage_verb_sentence  \\\n",
       "0           1          1                   0.166667   \n",
       "1           1          0                   0.000000   \n",
       "2           2          0                   0.000000   \n",
       "3           5          1                   0.208333   \n",
       "4           1          0                   0.000000   \n",
       "\n",
       "   Pourcentage_propn_sentence  Pourcentage_noun_sentence  \\\n",
       "0                    0.500000                   0.166667   \n",
       "1                    0.000000                   1.000000   \n",
       "2                    0.125000                   0.250000   \n",
       "3                    0.041667                   0.208333   \n",
       "4                    0.000000                   1.000000   \n",
       "\n",
       "   Pourcentage_num_sentence  Label  \n",
       "0                  0.166667      1  \n",
       "1                  0.000000      0  \n",
       "2                  0.000000      0  \n",
       "3                  0.041667      0  \n",
       "4                  0.000000      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basics statistics: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_Sentences</th>\n",
       "      <th>Sentences_CV_clean</th>\n",
       "      <th>CV_Number</th>\n",
       "      <th>Sentence_line</th>\n",
       "      <th>Nb_tokens</th>\n",
       "      <th>%texte_lu</th>\n",
       "      <th>%texte_lu_fin_ligne</th>\n",
       "      <th>Is_alpha</th>\n",
       "      <th>Grammar</th>\n",
       "      <th>Tokenization</th>\n",
       "      <th>Verb_count</th>\n",
       "      <th>Propn_count</th>\n",
       "      <th>Noun_count</th>\n",
       "      <th>Num_count</th>\n",
       "      <th>Pourcentage_verb_sentence</th>\n",
       "      <th>Pourcentage_propn_sentence</th>\n",
       "      <th>Pourcentage_noun_sentence</th>\n",
       "      <th>Pourcentage_num_sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5594</td>\n",
       "      <td>5592</td>\n",
       "      <td>5594</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594</td>\n",
       "      <td>5594</td>\n",
       "      <td>5592</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.00000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "      <td>5594.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3805</td>\n",
       "      <td>3802</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1626</td>\n",
       "      <td>2748</td>\n",
       "      <td>3802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CV_10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False]</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>638</td>\n",
       "      <td>334</td>\n",
       "      <td>198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.673221</td>\n",
       "      <td>9.132285</td>\n",
       "      <td>50.470518</td>\n",
       "      <td>49.529483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.297283</td>\n",
       "      <td>0.611012</td>\n",
       "      <td>2.83822</td>\n",
       "      <td>0.320164</td>\n",
       "      <td>0.020117</td>\n",
       "      <td>0.105039</td>\n",
       "      <td>0.314979</td>\n",
       "      <td>0.042982</td>\n",
       "      <td>0.037898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.268076</td>\n",
       "      <td>10.914849</td>\n",
       "      <td>30.190027</td>\n",
       "      <td>30.190025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.784780</td>\n",
       "      <td>1.129198</td>\n",
       "      <td>3.56600</td>\n",
       "      <td>0.693458</td>\n",
       "      <td>0.059959</td>\n",
       "      <td>0.227969</td>\n",
       "      <td>0.251108</td>\n",
       "      <td>0.131906</td>\n",
       "      <td>0.190966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>24.190000</td>\n",
       "      <td>23.552500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>49.530000</td>\n",
       "      <td>50.470000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>76.447500</td>\n",
       "      <td>75.810000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.760000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>39.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CV_Sentences Sentences_CV_clean CV_Number  Sentence_line    Nb_tokens  \\\n",
       "count          5594               5592      5594    5594.000000  5594.000000   \n",
       "unique         3805               3802       200            NaN          NaN   \n",
       "top                                        CV_10            NaN          NaN   \n",
       "freq            198                198        66            NaN          NaN   \n",
       "mean            NaN                NaN       NaN      15.673221     9.132285   \n",
       "std             NaN                NaN       NaN      11.268076    10.914849   \n",
       "min             NaN                NaN       NaN       0.000000     1.000000   \n",
       "25%             NaN                NaN       NaN       7.000000     2.000000   \n",
       "50%             NaN                NaN       NaN      14.000000     5.000000   \n",
       "75%             NaN                NaN       NaN      23.000000    12.000000   \n",
       "max             NaN                NaN       NaN      65.000000   124.000000   \n",
       "\n",
       "          %texte_lu  %texte_lu_fin_ligne Is_alpha   Grammar Tokenization  \\\n",
       "count   5594.000000          5594.000000     5594      5594         5592   \n",
       "unique          NaN                  NaN     1626      2748         3802   \n",
       "top             NaN                  NaN  [False]  ['NOUN']                \n",
       "freq            NaN                  NaN      638       334          198   \n",
       "mean      50.470518            49.529483      NaN       NaN          NaN   \n",
       "std       30.190027            30.190025      NaN       NaN          NaN   \n",
       "min        0.240000             0.000000      NaN       NaN          NaN   \n",
       "25%       24.190000            23.552500      NaN       NaN          NaN   \n",
       "50%       49.530000            50.470000      NaN       NaN          NaN   \n",
       "75%       76.447500            75.810000      NaN       NaN          NaN   \n",
       "max      100.000000            99.760000      NaN       NaN          NaN   \n",
       "\n",
       "         Verb_count  Propn_count  Noun_count    Num_count  \\\n",
       "count   5594.000000  5594.000000  5594.00000  5594.000000   \n",
       "unique          NaN          NaN         NaN          NaN   \n",
       "top             NaN          NaN         NaN          NaN   \n",
       "freq            NaN          NaN         NaN          NaN   \n",
       "mean       0.297283     0.611012     2.83822     0.320164   \n",
       "std        0.784780     1.129198     3.56600     0.693458   \n",
       "min        0.000000     0.000000     0.00000     0.000000   \n",
       "25%        0.000000     0.000000     1.00000     0.000000   \n",
       "50%        0.000000     0.000000     2.00000     0.000000   \n",
       "75%        0.000000     1.000000     4.00000     0.000000   \n",
       "max        8.000000    14.000000    39.00000     8.000000   \n",
       "\n",
       "        Pourcentage_verb_sentence  Pourcentage_propn_sentence  \\\n",
       "count                 5594.000000                 5594.000000   \n",
       "unique                        NaN                         NaN   \n",
       "top                           NaN                         NaN   \n",
       "freq                          NaN                         NaN   \n",
       "mean                     0.020117                    0.105039   \n",
       "std                      0.059959                    0.227969   \n",
       "min                      0.000000                    0.000000   \n",
       "25%                      0.000000                    0.000000   \n",
       "50%                      0.000000                    0.000000   \n",
       "75%                      0.000000                    0.102564   \n",
       "max                      1.000000                    1.000000   \n",
       "\n",
       "        Pourcentage_noun_sentence  Pourcentage_num_sentence        Label  \n",
       "count                 5594.000000               5594.000000  5594.000000  \n",
       "unique                        NaN                       NaN          NaN  \n",
       "top                           NaN                       NaN          NaN  \n",
       "freq                          NaN                       NaN          NaN  \n",
       "mean                     0.314979                  0.042982     0.037898  \n",
       "std                      0.251108                  0.131906     0.190966  \n",
       "min                      0.000000                  0.000000     0.000000  \n",
       "25%                      0.157895                  0.000000     0.000000  \n",
       "50%                      0.307692                  0.000000     0.000000  \n",
       "75%                      0.416667                  0.000000     0.000000  \n",
       "max                      1.000000                  1.000000     1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CV_Sentences                  0.000000\n",
       "Sentences_CV_clean            0.035753\n",
       "CV_Number                     0.000000\n",
       "Sentence_line                 0.000000\n",
       "Nb_tokens                     0.000000\n",
       "%texte_lu                     0.000000\n",
       "%texte_lu_fin_ligne           0.000000\n",
       "Is_alpha                      0.000000\n",
       "Grammar                       0.000000\n",
       "Tokenization                  0.035753\n",
       "Verb_count                    0.000000\n",
       "Propn_count                   0.000000\n",
       "Noun_count                    0.000000\n",
       "Num_count                     0.000000\n",
       "Pourcentage_verb_sentence     0.000000\n",
       "Pourcentage_propn_sentence    0.000000\n",
       "Pourcentage_noun_sentence     0.000000\n",
       "Pourcentage_num_sentence      0.000000\n",
       "Label                         0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic stats\n",
    "print(\"Number of rows : {}\".format(dataset.shape[0]))\n",
    "print()\n",
    "\n",
    "print(\"Display of dataset: \")\n",
    "display(dataset.head())\n",
    "print()\n",
    "\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = dataset.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "print(\"Percentage of missing values: \")\n",
    "display(100*dataset.isnull().sum()/dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vérifier des valeurs de la colonne label\n",
    "dataset[\"Label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5416, 19)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppression des lignes de CV_Sentences avec les '#NOM?'\n",
    "dataset = dataset.loc[(dataset['CV_Sentences'] != \"#NOM?\"),:]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5412, 19)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppression des lignes de CV_Sentences avec ':'\n",
    "dataset = dataset.loc[(dataset['CV_Sentences'] != \":\"),:]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5410, 19)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppresion des lignes sans valeur (avec NaN)\n",
    "dataset = dataset.dropna(axis =0, how = 'any')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV_Sentences                  0.0\n",
       "Sentences_CV_clean            0.0\n",
       "CV_Number                     0.0\n",
       "Sentence_line                 0.0\n",
       "Nb_tokens                     0.0\n",
       "%texte_lu                     0.0\n",
       "%texte_lu_fin_ligne           0.0\n",
       "Is_alpha                      0.0\n",
       "Grammar                       0.0\n",
       "Tokenization                  0.0\n",
       "Verb_count                    0.0\n",
       "Propn_count                   0.0\n",
       "Noun_count                    0.0\n",
       "Num_count                     0.0\n",
       "Pourcentage_verb_sentence     0.0\n",
       "Pourcentage_propn_sentence    0.0\n",
       "Pourcentage_noun_sentence     0.0\n",
       "Pourcentage_num_sentence      0.0\n",
       "Label                         0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vérification de la présence des valeurs 'Null' dans la dataset\n",
    "display(100*dataset.isnull().sum()/dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating labels from features...\n",
      "...Done.\n",
      "\n",
      "Y : \n",
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "X :\n",
      "   Sentence_line  Nb_tokens  %texte_lu  %texte_lu_fin_ligne  Verb_count  \\\n",
      "0              0          6       2.80                97.20           1   \n",
      "1              1          1       3.27                96.73           0   \n",
      "2              2          8       7.01                92.99           0   \n",
      "3              3         24      18.22                81.78           5   \n",
      "4              4          1      18.69                81.31           0   \n",
      "\n",
      "   Propn_count  Noun_count  Num_count  Pourcentage_verb_sentence  \\\n",
      "0            3           1          1                   0.166667   \n",
      "1            0           1          0                   0.000000   \n",
      "2            1           2          0                   0.000000   \n",
      "3            1           5          1                   0.208333   \n",
      "4            0           1          0                   0.000000   \n",
      "\n",
      "   Pourcentage_propn_sentence  Pourcentage_noun_sentence  \\\n",
      "0                    0.500000                   0.166667   \n",
      "1                    0.000000                   1.000000   \n",
      "2                    0.125000                   0.250000   \n",
      "3                    0.041667                   0.208333   \n",
      "4                    0.000000                   1.000000   \n",
      "\n",
      "   Pourcentage_num_sentence  \n",
      "0                  0.166667  \n",
      "1                  0.000000  \n",
      "2                  0.000000  \n",
      "3                  0.041667  \n",
      "4                  0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Separate target variable Y from features X\n",
    "print(\"Separating labels from features...\")\n",
    "\n",
    "## Choisir la features_list par rapport au dataset\n",
    "#features_list = [\"CV_Sentences\",\"Sentences_CV_clean\",\"CV_Number\", \"Is_alpha\", \"Grammar\", \"Label\"] ## Pour les datasets features1\n",
    "features_list = [\"CV_Sentences\",\"Sentences_CV_clean\",\"CV_Number\", \"Is_alpha\", \"Label\", \"Grammar\", \"Tokenization\"] ## Pour les datasets features2\n",
    "target_variable = \"Label\"\n",
    "\n",
    "X = dataset.drop(features_list, axis = 1)\n",
    "Y = dataset.loc[:,target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print('Y : ')\n",
    "print(Y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numeric features  ['Sentence_line', 'Nb_tokens', '%texte_lu', '%texte_lu_fin_ligne', 'Verb_count', 'Propn_count', 'Noun_count', 'Num_count', 'Pourcentage_verb_sentence', 'Pourcentage_propn_sentence', 'Pourcentage_noun_sentence', 'Pourcentage_num_sentence']  at positions  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Found categorical features  []  at positions  []\n"
     ]
    }
   ],
   "source": [
    "# Automatically detect positions of numeric/categorical features\n",
    "idx = 0\n",
    "numeric_features = []\n",
    "numeric_indices = []\n",
    "categorical_features = []\n",
    "categorical_indices = []\n",
    "for i,t in X.dtypes.iteritems():\n",
    "    if ('float' in str(t)) or ('int' in str(t)) :\n",
    "        numeric_features.append(i)\n",
    "        numeric_indices.append(idx)\n",
    "    else :\n",
    "        categorical_features.append(i)\n",
    "        categorical_indices.append(idx)\n",
    "\n",
    "    idx = idx + 1\n",
    "\n",
    "print('Found numeric features ', numeric_features,' at positions ', numeric_indices)\n",
    "print('Found categorical features ', categorical_features,' at positions ', categorical_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing into train and test sets...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "print(\"Dividing into train and test sets...\")\n",
    "# WARNING : don't forget stratify=Y for classification problems\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify = Y)\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert pandas DataFrames to numpy arrays...\n",
      "...Done\n",
      "[[25.          1.         45.85       54.15        0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [14.          6.         76.08       23.92        1.          0.\n",
      "   2.          0.          0.16666667  0.          0.33333333  0.        ]\n",
      " [17.          3.         47.06       52.94        0.          1.\n",
      "   1.          0.          0.          0.33333333  0.33333333  0.        ]\n",
      " [11.          1.         27.         73.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [ 6.          9.         20.78       79.22        1.          0.\n",
      "   1.          0.          0.11111111  0.          0.11111111  0.        ]]\n",
      "[[ 3.         15.         35.22       64.78        1.          0.\n",
      "   4.          0.          0.06666667  0.          0.26666667  0.        ]\n",
      " [13.          2.         36.42       63.58        0.          0.\n",
      "   0.          1.          0.          0.          0.          0.5       ]]\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrames to numpy arrays before using scikit-learn\n",
    "print(\"Convert pandas DataFrames to numpy arrays...\")\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "Y_train = Y_train.tolist()\n",
    "Y_test = Y_test.tolist()\n",
    "print(\"...Done\")\n",
    "\n",
    "print(X_train[0:5,:])\n",
    "print(X_test[0:2,:])\n",
    "print()\n",
    "print(Y_train[0:5])\n",
    "print(Y_test[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "[[25.          1.         45.85       54.15        0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [14.          6.         76.08       23.92        1.          0.\n",
      "   2.          0.          0.16666667  0.          0.33333333  0.        ]\n",
      " [17.          3.         47.06       52.94        0.          1.\n",
      "   1.          0.          0.          0.33333333  0.33333333  0.        ]\n",
      " [11.          1.         27.         73.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [ 6.          9.         20.78       79.22        1.          0.\n",
      "   1.          0.          0.11111111  0.          0.11111111  0.        ]]\n",
      "\n",
      "...Done!\n",
      "[[25.          1.         45.85       54.15        0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [14.          6.         76.08       23.92        1.          0.\n",
      "   2.          0.          0.16666667  0.          0.33333333  0.        ]\n",
      " [17.          3.         47.06       52.94        0.          1.\n",
      "   1.          0.          0.          0.33333333  0.33333333  0.        ]\n",
      " [11.          1.         27.         73.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [ 6.          9.         20.78       79.22        1.          0.\n",
      "   1.          0.          0.11111111  0.          0.11111111  0.        ]]\n",
      "\n",
      "Encoding categorical features and standardizing numerical features...\n",
      "\n",
      "[[25.          1.         45.85       54.15        0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [14.          6.         76.08       23.92        1.          0.\n",
      "   2.          0.          0.16666667  0.          0.33333333  0.        ]\n",
      " [17.          3.         47.06       52.94        0.          1.\n",
      "   1.          0.          0.          0.33333333  0.33333333  0.        ]\n",
      " [11.          1.         27.         73.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.        ]\n",
      " [ 6.          9.         20.78       79.22        1.          0.\n",
      "   1.          0.          0.11111111  0.          0.11111111  0.        ]]\n",
      "...Done\n",
      "[[ 0.84941168 -0.74490809 -0.14998016  0.14998009 -0.39287215  0.31994356\n",
      "  -0.80179486 -0.47323519 -0.35378762  3.88915581 -1.25327628 -0.33178113]\n",
      " [-0.13204091 -0.30377703  0.8477846  -0.84778473  0.81648109 -0.55369413\n",
      "  -0.25826472 -0.47323519  2.3625681  -0.46742016  0.07457067 -0.33178113]\n",
      " [ 0.13562797 -0.56845567 -0.11004316  0.1100431  -0.39287215  0.31994356\n",
      "  -0.53002979 -0.47323519 -0.35378762  0.98477183  0.07457067 -0.33178113]\n",
      " [-0.3997098  -0.74490809 -0.77213913  0.7721391  -0.39287215  0.31994356\n",
      "  -0.80179486 -0.47323519 -0.35378762  3.88915581 -1.25327628 -0.33178113]\n",
      " [-0.84582462 -0.0390984  -0.97743509  0.97743507  0.81648109 -0.55369413\n",
      "  -0.53002979 -0.47323519  1.45711619 -0.46742016 -0.81066063 -0.33178113]]\n",
      "Encoding labels...\n",
      "[0, 0, 0, 0, 0]\n",
      "...Done\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "print(\"Imputing missing values...\")\n",
    "print(X_train[0:5,:])\n",
    "print()\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train[:,numeric_indices] = imputer.fit_transform(X_train[:,numeric_indices])\n",
    "print(\"...Done!\")\n",
    "print(X_train[0:5,:]) \n",
    "print() \n",
    "\n",
    "# Encoding categorical features and standardizing numerical features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print()\n",
    "print(X_train[0:5,:])\n",
    "\n",
    "# Normalization\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# OHE / dummyfication\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "featureencoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_indices),    \n",
    "        ('num', numeric_transformer, numeric_indices)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "X_train = featureencoder.fit_transform(X_train)\n",
    "print(\"...Done\")\n",
    "print(X_train[0:5,:])\n",
    "\n",
    "# Label encoding\n",
    "print(\"Encoding labels...\")\n",
    "print(Y_train[0:5])\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"...Done\")\n",
    "print(Y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "...Done.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Train model...\")\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "print(\"...Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on training set...\n",
      "...Done.\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on training set\n",
    "print(\"Predictions on training set...\")\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print(Y_train_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "[[3.00000000e+00 1.50000000e+01 3.52200000e+01 6.47800000e+01\n",
      "  1.00000000e+00 0.00000000e+00 4.00000000e+00 0.00000000e+00\n",
      "  6.66666667e-02 0.00000000e+00 2.66666667e-01 0.00000000e+00]\n",
      " [1.30000000e+01 2.00000000e+00 3.64200000e+01 6.35800000e+01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.00000000e-01]\n",
      " [2.60000000e+01 3.00000000e+00 7.32900000e+01 2.67100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 2.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.33333333e-01 6.66666667e-01]\n",
      " [5.00000000e+00 3.00000000e+00 4.65900000e+01 5.34100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.66666667e-01 0.00000000e+00]\n",
      " [2.00000000e+01 9.00000000e+00 9.94600000e+01 5.40000000e-01\n",
      "  0.00000000e+00 1.00000000e+00 2.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 1.11111111e-01 2.22222222e-01 1.11111111e-01]]\n",
      "...Done!\n",
      "[[3.00000000e+00 1.50000000e+01 3.52200000e+01 6.47800000e+01\n",
      "  1.00000000e+00 0.00000000e+00 4.00000000e+00 0.00000000e+00\n",
      "  6.66666667e-02 0.00000000e+00 2.66666667e-01 0.00000000e+00]\n",
      " [1.30000000e+01 2.00000000e+00 3.64200000e+01 6.35800000e+01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.00000000e-01]\n",
      " [2.60000000e+01 3.00000000e+00 7.32900000e+01 2.67100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 2.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.33333333e-01 6.66666667e-01]\n",
      " [5.00000000e+00 3.00000000e+00 4.65900000e+01 5.34100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.66666667e-01 0.00000000e+00]\n",
      " [2.00000000e+01 9.00000000e+00 9.94600000e+01 5.40000000e-01\n",
      "  0.00000000e+00 1.00000000e+00 2.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 1.11111111e-01 2.22222222e-01 1.11111111e-01]]\n",
      "\n",
      "Encoding categorical features and standardizing numerical features...\n",
      "[[3.00000000e+00 1.50000000e+01 3.52200000e+01 6.47800000e+01\n",
      "  1.00000000e+00 0.00000000e+00 4.00000000e+00 0.00000000e+00\n",
      "  6.66666667e-02 0.00000000e+00 2.66666667e-01 0.00000000e+00]\n",
      " [1.30000000e+01 2.00000000e+00 3.64200000e+01 6.35800000e+01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.00000000e-01]\n",
      " [2.60000000e+01 3.00000000e+00 7.32900000e+01 2.67100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 2.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.33333333e-01 6.66666667e-01]\n",
      " [5.00000000e+00 3.00000000e+00 4.65900000e+01 5.34100000e+01\n",
      "  0.00000000e+00 0.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.66666667e-01 0.00000000e+00]\n",
      " [2.00000000e+01 9.00000000e+00 9.94600000e+01 5.40000000e-01\n",
      "  0.00000000e+00 1.00000000e+00 2.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 1.11111111e-01 2.22222222e-01 1.11111111e-01]]\n",
      "...Done\n",
      "[[-1.1134935   0.49025887 -0.50083161  0.50083157  0.81648109 -0.55369413\n",
      "   0.28526541 -0.47323519  0.73275467 -0.46742016 -0.19099872 -0.33178113]\n",
      " [-0.22126388 -0.65668188 -0.46122467  0.46122463 -0.39287215 -0.55369413\n",
      "  -0.80179486  0.92102069 -0.35378762 -0.46742016 -1.25327628  3.2859748 ]\n",
      " [ 0.93863464 -0.56845567  0.75569847 -0.75569859 -0.39287215 -0.55369413\n",
      "  -0.53002979  2.31527657 -0.35378762 -0.46742016  0.07457067  4.49189344]\n",
      " [-0.93504758 -0.56845567 -0.12555588  0.12555581 -0.39287215 -0.55369413\n",
      "  -0.25826472 -0.47323519 -0.35378762 -0.46742016  1.40241762 -0.33178113]\n",
      " [ 0.40329686 -0.0390984   1.61945976 -1.61945993 -0.39287215  0.31994356\n",
      "  -0.25826472  0.92102069 -0.35378762  0.01664384 -0.36804498  0.47216463]]\n",
      "Encoding labels...\n",
      "[0, 0, 0, 0, 0]\n",
      "...Done\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "print(\"Imputing missing values...\")\n",
    "print(X_test[0:5,:])\n",
    "X_test[:,numeric_indices] = imputer.transform(X_test[:,numeric_indices])\n",
    "print(\"...Done!\")\n",
    "print(X_test[0:5,:]) \n",
    "print() \n",
    "\n",
    "# Encoding categorical features and standardizing numerical features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print(X_test[0:5,:])\n",
    "X_test = featureencoder.transform(X_test)\n",
    "print(\"...Done\")\n",
    "print(X_test[0:5,:])\n",
    "\n",
    "# Label encoding\n",
    "print(\"Encoding labels...\")\n",
    "print(Y_test[0:5])\n",
    "Y_test = encoder.transform(Y_test)\n",
    "print(\"...Done\")\n",
    "print(Y_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set...\n",
      "...Done.\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on test set\n",
    "print(\"Predictions on test set...\")\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print(Y_test_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 10.0, 'penalty': 'l2'}\n",
      "accuracy : 0.9674231246257804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "70 fits failed out of a total of 140.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.96072085        nan 0.9621076         nan 0.96534407\n",
      "        nan 0.96719218        nan 0.96742312        nan 0.96742312\n",
      "        nan 0.96742312]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Grid search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(X_train,Y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9658040665434381\n"
     ]
    }
   ],
   "source": [
    "logreg2=LogisticRegression(C=1,penalty=\"l2\")\n",
    "logreg2.fit(X_train,Y_train)\n",
    "print(\"score\",logreg2.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set :  0.9678835489833642\n",
      "accuracy on test set :  0.9658040665434381\n",
      "\n",
      "f1-score on training set :  0.39301310043668125\n",
      "f1-score on test set :  0.3018867924528302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print scores\n",
    "print(\"accuracy on training set : \", accuracy_score(Y_train, Y_train_pred))\n",
    "print(\"accuracy on test set : \", accuracy_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "print(\"f1-score on training set : \", f1_score(Y_train, Y_train_pred))                                            \n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fin Part 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
